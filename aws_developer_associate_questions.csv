QUESTION,ANSWER_CHOICES,ANSWERS,EXPLANATIONS
Which of the following are good use cases for how Amazon ElastiCache can help an application? (Select TWO),"[""Improve the performance of S3 PUT operations."", ""Improve the latency of deployments performed by AWS CodeDeploy."", ""Improve latency and throughput for read-heavy application workloads."", ""Reduce the time required to merge AWS CodeCommit branches."", ""Improve performance of compute-intensive applications.""]","[""Improve latency and throughput for read-heavy application workloads."", ""Improve performance of compute-intensive applications.""]","```json
{
    ""Improve the performance of S3 PUT operations."": ""This choice is incorrect. Amazon ElastiCache is primarily used to improve the performance of read-heavy application workloads by caching frequently accessed data. It does not directly interact with Amazon S3 operations like PUT requests, which are related to object storage rather than data caching."",
    ""Improve the latency of deployments performed by AWS CodeDeploy."": ""This choice is incorrect. AWS CodeDeploy is a service that automates code deployments to any instance, including Amazon EC2 instances and on-premises servers. ElastiCache does not play a role in deployment processes or improve the latency of deployments, as it is not involved in code deployment workflows."",
    ""Improve latency and throughput for read-heavy application workloads."": ""This choice is correct. Amazon ElastiCache is designed to improve the performance of applications by caching frequently accessed data, thereby reducing the load on databases and improving latency and throughput for read-heavy workloads. This is a common use case for ElastiCache, particularly with applications that experience high read traffic."",
    ""Reduce the time required to merge AWS CodeCommit branches."": ""This choice is incorrect. AWS CodeCommit is a source control service that hosts Git repositories. The process of merging branches is a function of the version control system and is not related to caching or data retrieval, which is what ElastiCache is used for."",
    ""Improve performance of compute-intensive applications."": ""This choice is incorrect. While ElastiCache can help reduce database load and improve data retrieval times, it is not specifically designed to enhance the performance of compute-intensive applications. Compute-intensive applications typically benefit more from optimized compute resources, such as those provided by Amazon EC2 with appropriate instance types.""
}
```"
Which of the following services are key/value stores? (Choose 3 answers),"[""Amazon ElastiCache."", ""Simple Notification Service."", ""DynamoDB."", ""Simple Workflow Service."", ""Simple Storage Service.""]","[""Amazon ElastiCache."", ""DynamoDB."", ""Simple Storage Service.""]","```json
{
    ""Amazon ElastiCache."": ""Amazon ElastiCache is correct because it supports key/value storage through its Redis and Memcached engines. Both Redis and Memcached are in-memory data stores that use a key/value model, making them suitable for caching and session storage. This service is often used to improve application performance by reducing database load."",
    ""Simple Notification Service."": ""Simple Notification Service (SNS) is incorrect because it is a fully managed messaging service for sending notifications, not a key/value store. SNS is used for pub/sub messaging and sending messages to multiple subscribers, which does not involve storing data in a key/value format."",
    ""DynamoDB."": ""DynamoDB is correct because it is a fully managed NoSQL database service that provides fast and predictable performance with seamless scalability. It uses a key/value and document data model, making it ideal for applications that require low-latency data access at any scale."",
    ""Simple Workflow Service."": ""Simple Workflow Service (SWF) is incorrect because it is a service for building distributed applications as workflows. It is not a data storage service and does not use a key/value model. SWF is used to coordinate tasks across distributed components, not for storing data."",
    ""Simple Storage Service."": ""Simple Storage Service (S3) is correct because it is an object storage service that stores data as objects within buckets. Each object is identified by a unique key, which makes it a key/value store. S3 is often used for storing and retrieving any amount of data, such as backup and archival, big data analytics, and content storage.""
}
```"
A developer wants to send multi-value headers to an AWS Lambda function that is registered as a target with an Application Load Balancer (ALB). What should the developer do to achieve this?,"[""Place the Lambda function and target group in the same account."", ""Send the request body to the Lambda function with a size less than 1 MB 0."", ""Include the Base64 encoding status status code, status description, and headers in the Lambda function."", ""Enable the multi-value headers on the ALB.""]","[""Enable the multi-value headers on the ALB.""]","```json
{
    ""Place the Lambda function and target group in the same account."": ""This choice is incorrect. While it's common to have the Lambda function and target group in the same AWS account for simplicity and access management, it is not a requirement for enabling multi-value headers on an ALB. Multi-value headers are a feature of the ALB itself and do not depend on the account configuration of the Lambda function."",
    ""Send the request body to the Lambda function with a size less than 1 MB 0."": ""This choice is incorrect. The size of the request body is related to the payload size limits when invoking a Lambda function, but it has no direct impact on the ability to send multi-value headers. Multi-value headers are a feature that needs to be explicitly enabled on the ALB, and they allow the ALB to handle multiple headers with the same name, which is unrelated to the request body size."",
    ""Include the Base64 encoding status status code, status description, and headers in the Lambda function."": ""This choice is incorrect. While Base64 encoding might be relevant for handling binary data in Lambda responses, it does not relate to enabling multi-value headers. Multi-value headers are a feature of the ALB that must be enabled on the ALB itself, not configured within the Lambda function."",
    ""Enable the multi-value headers on the ALB."": ""This choice is correct. To send multi-value headers to a Lambda function through an ALB, you must enable the multi-value headers feature on the ALB. This feature allows the ALB to pass multiple headers with the same name to the Lambda function, which is necessary for applications that rely on such headers. This is supported by AWS documentation and is a best practice for handling such scenarios.""
}
```"
"A company's ecommerce website is experiencing massive traffic spikes, which are causing performance problems in the company database. Users are reporting that accessing the website takes a long time. A developer wants to implement a caching layer using Amazon ElastiCache. The website is required to be responsive no matter which product a user views, and the updates to product information and prices must be strongly consistent. Which cache writing policy will satisfy these requirements?","[""Write to the cache directly and sync the backend at a later time."", ""Write to the backend first and wait for the cache to expire."", ""Write to the cache and the backend at the same time."", ""Write to the backend first and invalidate the cache.""]","[""Write to the backend first and invalidate the cache.""]","```json
{
    ""Write to the cache directly and sync the backend at a later time."": ""This approach is incorrect because it does not ensure strong consistency. Writing to the cache first and syncing the backend later can lead to stale data being served to users if the backend update fails or is delayed. This violates the requirement for updates to product information and prices to be strongly consistent."",
    ""Write to the backend first and wait for the cache to expire."": ""This approach is incorrect because it can lead to serving stale data until the cache expires. Although writing to the backend first ensures data consistency, waiting for the cache to expire does not provide immediate consistency in the cache, which is necessary for the requirement of strong consistency."",
    ""Write to the cache and the backend at the same time."": ""This approach is incorrect because it can lead to race conditions and inconsistencies between the cache and backend. If the cache update succeeds but the backend update fails, or vice versa, users might receive inconsistent data. This does not satisfy the requirement for strong consistency."",
    ""Write to the backend first and invalidate the cache."": ""This approach is correct because it ensures strong consistency. By writing to the backend first, the source of truth is updated, and then invalidating the cache ensures that subsequent reads will fetch the updated data from the backend, thus maintaining consistency. This is a common pattern in caching strategies known as 'cache-aside' or 'lazy loading', which is recommended for scenarios where strong consistency is required.""
}
```"
A Developer wants to upload data to Amazon S3 and must encrypt the data in transit. Which of the following solutions will accomplish this task? (Choose TWO),"[""Set up hardware VPN tunnels to a VPC and access S3 through a VPC endpoint."", ""Set up Client-Side Encryption with an AWS KMS-Managed Customer Master Key."", ""Set up Server-Side Encryption with AWS KMS-Managed Keys."", ""Transfer the data over an SSL connection."", ""Set up Server-Side Encryption with S3-Managed Keys.""]","[""Set up Client-Side Encryption with an AWS KMS-Managed Customer Master Key."", ""Transfer the data over an SSL connection.""]","{""Set up hardware VPN tunnels to a VPC and access S3 through a VPC endpoint."": ""Incorrect. While setting up a VPN tunnel and accessing S3 through a VPC endpoint can secure data in transit within a VPC, it is not necessary for encrypting data in transit to S3. S3 can be accessed directly over the internet using SSL/TLS for encryption in transit, which is simpler and more cost-effective."", ""Set up Client-Side Encryption with an AWS KMS-Managed Customer Master Key."": ""Correct. Client-side encryption involves encrypting data before sending it to S3, ensuring that data is encrypted in transit. Using AWS KMS-Managed Customer Master Keys provides a secure way to manage encryption keys, aligning with AWS best practices for data protection."", ""Set up Server-Side Encryption with AWS KMS-Managed Keys."": ""Incorrect. Server-side encryption with AWS KMS-Managed Keys encrypts data at rest, not in transit. While it is a good practice for data protection, it does not address the requirement to encrypt data in transit."", ""Transfer the data over an SSL connection."": ""Correct. SSL/TLS is the standard protocol for encrypting data in transit over the internet. By using HTTPS, data is encrypted between the client and S3, fulfilling the requirement to encrypt data in transit."", ""Set up Server-Side Encryption with S3-Managed Keys."": ""Incorrect. Similar to server-side encryption with KMS-Managed Keys, this method encrypts data at rest, not in transit. It does not meet the requirement to encrypt data while it is being transmitted to S3.""}"
A Developer wants to encrypt new objects that are being uploaded to an Amazon S3 bucket by an application. There must be an audit trail of who has used the key during this process. There should be no change to the performance of the application. Which type of encryption meets these requirements?,"[""Server-side encryption using S3-managed keys."", ""Server-side encryption with AWS KMS-managed keys."", ""Client-side encryption with a client-side symmetric master key."", ""Client-side encryption with AWS KMS-managed keys.""]","[""Server-side encryption with AWS KMS-managed keys.""]","```json
{
    ""Server-side encryption using S3-managed keys."": ""This option is incorrect because while server-side encryption with S3-managed keys (SSE-S3) provides encryption at rest, it does not provide an audit trail of who used the key. SSE-S3 uses keys managed by Amazon S3 and does not integrate with AWS CloudTrail for auditing key usage."",
    ""Server-side encryption with AWS KMS-managed keys."": ""This option is correct because server-side encryption with AWS KMS-managed keys (SSE-KMS) provides encryption at rest and integrates with AWS CloudTrail to provide an audit trail of key usage. This allows tracking of who accessed the keys, which meets the requirement for an audit trail. Additionally, SSE-KMS is designed to have minimal impact on performance, aligning with the requirement of no performance change."",
    ""Client-side encryption with a client-side symmetric master key."": ""This option is incorrect because while client-side encryption provides control over encryption keys, it does not inherently provide an audit trail of key usage unless additional logging mechanisms are implemented by the client. Moreover, managing client-side encryption keys can add complexity and potential performance overhead, which does not align with the requirement of no performance change."",
    ""Client-side encryption with AWS KMS-managed keys."": ""This option is incorrect because although AWS KMS can be used for client-side encryption, it requires additional implementation effort to manage encryption and decryption processes on the client side. This can introduce performance overhead and complexity, which contradicts the requirement for no change in application performance. Additionally, while KMS can provide an audit trail, the client-side nature of this option makes it less straightforward compared to server-side solutions.""
}
```"
An application is being developed to audit several AWS accounts. The application will run in Account A and must access AWS services in Accounts B and C. What is the MOST secure way to allow the application to call AWS services in each audited account?,"[""Configure cross-account roles in each audited account. Write code in Account A that assumes those roles."", ""Use S3 cross-region replication to communicate among accounts, with Amazon S3 event notifications to trigger Lambda functions."", ""Deploy an application in each audited account with its own role. Have Account A authenticate with the application."", ""Create an IAM user with an access key in each audited account. Write code in Account A that uses those access keys.""]","[""Configure cross-account roles in each audited account. Write code in Account A that assumes those roles.""]","```json
{
    ""Configure cross-account roles in each audited account. Write code in Account A that assumes those roles."": ""This is the correct choice because using IAM roles for cross-account access is a best practice in AWS. It allows Account A to assume roles in Accounts B and C, providing temporary security credentials with the necessary permissions to access resources. This method is secure as it avoids the need to manage long-term credentials and allows for fine-grained access control. AWS documentation recommends using IAM roles for cross-account access as it minimizes the risk of credential leakage and simplifies permissions management."",
    ""Use S3 cross-region replication to communicate among accounts, with Amazon S3 event notifications to trigger Lambda functions."": ""This choice is incorrect because S3 cross-region replication is designed for replicating objects across different AWS regions and not for enabling cross-account access. While S3 event notifications can trigger Lambda functions, this setup does not inherently provide the necessary permissions or access control needed for auditing multiple accounts. This approach is not aligned with the requirement to securely access AWS services across accounts."",
    ""Deploy an application in each audited account with its own role. Have Account A authenticate with the application."": ""This choice is incorrect because it involves deploying separate applications in each account, which increases complexity and maintenance overhead. Additionally, having Account A authenticate with each application could require managing multiple authentication mechanisms, which is less secure and efficient compared to using cross-account roles. This approach does not leverage AWS's built-in cross-account access capabilities."",
    ""Create an IAM user with an access key in each audited account. Write code in Account A that uses those access keys."": ""This choice is incorrect because creating IAM users with access keys in each account is not a secure practice. It involves managing long-term credentials, which increases the risk of credential leakage and requires additional effort to rotate and manage these keys. AWS best practices recommend avoiding the use of long-term credentials and instead using IAM roles with temporary credentials for cross-account access.""
}
```"
"A company uses a third-party tool to build, bundle, and package rts applications on-premises and store them locally. The company uses Amazon EC2 instances to run its front-end applications. How can an application be deployed from the source control system onto the EC2 instances?","[""Use AWS CodeDeploy and point it to the local storage to directly deploy a bundle m a zip. tar. or tar.gz format."", ""Upload the bundle to an Amazon S3 bucket and specify the S3 location when doing a deployment using AWS CodeDeploy."", ""Create a repository using AWS CodeCommit to automatically trigger a deployment to the EC2 instances."", ""Use AWS CodeBuild to automatically deploy the latest build to the latest EC2 instances.""]","[""Upload the bundle to an Amazon S3 bucket and specify the S3 location when doing a deployment using AWS CodeDeploy.""]","```json
{
    ""Use AWS CodeDeploy and point it to the local storage to directly deploy a bundle m a zip. tar. or tar.gz format."": ""This choice is incorrect because AWS CodeDeploy does not support deploying directly from local storage. CodeDeploy requires the application bundle to be stored in an Amazon S3 bucket, a GitHub repository, or an AWS CodeCommit repository. This ensures that the deployment package is accessible to the CodeDeploy service in a scalable and reliable manner."",
    ""Upload the bundle to an Amazon S3 bucket and specify the S3 location when doing a deployment using AWS CodeDeploy."": ""This choice is correct because AWS CodeDeploy can deploy applications from an Amazon S3 bucket. By uploading the application bundle to S3, you ensure that it is stored in a highly available and durable manner. During deployment, you specify the S3 location, and CodeDeploy retrieves the bundle from there to deploy it onto the EC2 instances. This method aligns with AWS best practices for application deployment."",
    ""Create a repository using AWS CodeCommit to automatically trigger a deployment to the EC2 instances."": ""This choice is incorrect because simply creating a repository in AWS CodeCommit does not automatically trigger a deployment. While CodeCommit can be used as a source for CodeDeploy, additional configuration is required to set up triggers or pipelines that initiate deployments. Furthermore, the scenario describes using a third-party tool for building and bundling, which implies that the source is not natively in CodeCommit."",
    ""Use AWS CodeBuild to automatically deploy the latest build to the latest EC2 instances."": ""This choice is incorrect because AWS CodeBuild is primarily used for building and testing code, not for deployment. While CodeBuild can be part of a CI/CD pipeline that includes deployment, it does not directly deploy applications to EC2 instances. CodeDeploy or other deployment services are typically used for the deployment phase.""
}
```"
A company is building a compute-intensive application that will run on a fleet of Amazon EC2 instances. The application uses attached Amazon EBS disks for storing data. The application will process sensitive information and all the data must be encrypted. What should a developer do to ensure the data is encrypted on disk without impacting performance?,"[""Configure the Amazon EC2 instance fleet to use encrypted EBS volumes for storing data."", ""Add logic to write all data to an encrypted Amazon S3 bucket."", ""Add a custom encryption algorithm to the application that will encrypt and decrypt all data."", ""Create a new Amazon Machine Image (AMI) with an encrypted root volume and store the data to ephemeral disks.""]","[""Configure the Amazon EC2 instance fleet to use encrypted EBS volumes for storing data.""]","```json
{
    ""Configure the Amazon EC2 instance fleet to use encrypted EBS volumes for storing data."": ""This choice is correct because Amazon EBS provides the ability to create encrypted volumes that protect data at rest using AWS Key Management Service (KMS). Encrypted EBS volumes offer seamless encryption of data without impacting performance, as the encryption and decryption processes are handled by AWS. This is a best practice for handling sensitive information on EC2 instances, ensuring data is encrypted on disk with minimal impact on application performance."",
    ""Add logic to write all data to an encrypted Amazon S3 bucket."": ""This choice is incorrect because while Amazon S3 provides server-side encryption for data at rest, it is not suitable for applications that require high-performance local storage like EBS volumes. Writing data to S3 would introduce additional latency and complexity, as the application would need to handle data transfer between EBS and S3, which is not optimal for compute-intensive applications requiring fast disk access."",
    ""Add a custom encryption algorithm to the application that will encrypt and decrypt all data."": ""This choice is incorrect because implementing a custom encryption algorithm can be error-prone and may lead to security vulnerabilities if not done correctly. Additionally, it would increase the complexity of the application and could negatively impact performance. AWS provides built-in encryption mechanisms like EBS encryption that are optimized for performance and security, making custom encryption unnecessary."",
    ""Create a new Amazon Machine Image (AMI) with an encrypted root volume and store the data to ephemeral disks."": ""This choice is incorrect because ephemeral disks, also known as instance store volumes, are not persistent and data is lost when the instance is stopped or terminated. While you can create an AMI with an encrypted root volume, using ephemeral storage for sensitive data is not recommended due to its non-persistent nature. EBS volumes are a better choice for storing sensitive data as they are persistent and can be encrypted easily.""
}
```"
"A global company has an application running on Amazon EC2 instances that serves image files from Amazon S3. User requests from the browser are causing high traffic, which results in degraded performance. Which optimization solution should a Developer implement to increase application performance?","[""Create multiple prefix in the S3 bucket to increase the request rate."", ""Create an Amazon ElastiCache cluster to cache and serve frequently accessed items."", ""Use Amazon CloudFront to serve the content of images stored in Amazon S3."", ""Submit a ticket to AWS support to request a rate limit increase for the S3 bucket.""]","[""Use Amazon CloudFront to serve the content of images stored in Amazon S3.""]","{""Create multiple prefix in the S3 bucket to increase the request rate."": ""This choice is incorrect because while using multiple prefixes in an S3 bucket can help distribute the load and increase the request rate, it does not address the issue of serving content efficiently to users globally. The problem described involves high traffic and degraded performance, which is better addressed by a content delivery network like CloudFront."", ""Create an Amazon ElastiCache cluster to cache and serve frequently accessed items."": ""This choice is incorrect because ElastiCache is primarily used for caching database queries or computational results to reduce load on databases and improve application performance. It is not typically used for serving static content like images from S3. The issue here is about serving static content efficiently, which is better solved by using a CDN like CloudFront."", ""Use Amazon CloudFront to serve the content of images stored in Amazon S3."": ""This choice is correct because Amazon CloudFront is a content delivery network (CDN) that caches copies of your content close to users at edge locations around the world. This reduces latency and improves performance for users accessing static content like images stored in S3. CloudFront is designed to handle high traffic and provide faster content delivery, which directly addresses the problem of degraded performance due to high traffic."", ""Submit a ticket to AWS support to request a rate limit increase for the S3 bucket."": ""This choice is incorrect because while increasing the rate limit might temporarily alleviate some issues, it does not fundamentally solve the problem of efficiently serving content to a global audience. The use of a CDN like CloudFront is a more scalable and effective solution for improving performance and reducing latency for users worldwide.""}"
"An AWS Lambda function generates a 3MB JSON file and then uploads it to an Amazon S3 bucket daily. The file contains sensitive information, so the Developer must ensure that it is encrypted before uploading to the bucket. Which of the following modifications should the Developer make to ensure that the data is encrypted before uploading it to the bucket?","[""Use the default AWS KMS customer master key for S3 in the Lambda function code."", ""Use the S3 managed key and call the GenerateDataKey API to encrypt the file."", ""Use the GenerateDataKey API, then use that data key to encrypt the file in the Lambda function code."", ""Use a custom KMS customer master key created for S3 in the Lambda function code.""]","[""Use the GenerateDataKey API, then use that data key to encrypt the file in the Lambda function code.""]","```json
{
    ""Use the default AWS KMS customer master key for S3 in the Lambda function code."": ""This option is incorrect because the default AWS KMS customer master key (CMK) for S3 is used for server-side encryption at rest, not for client-side encryption before uploading. The requirement is to encrypt the data before uploading it to S3, which requires client-side encryption using a data key generated by KMS."",
    ""Use the S3 managed key and call the GenerateDataKey API to encrypt the file."": ""This option is incorrect because S3 managed keys (SSE-S3) are used for server-side encryption and do not involve the GenerateDataKey API, which is used for client-side encryption. The GenerateDataKey API is part of AWS KMS and is used to generate a data key for encrypting data before it is uploaded to S3."",
    ""Use the GenerateDataKey API, then use that data key to encrypt the file in the Lambda function code."": ""This option is correct because it involves using AWS KMS to generate a data key specifically for encrypting the file before uploading it to S3. This approach ensures that the data is encrypted client-side, which is necessary for protecting sensitive information before it is transferred to S3. The GenerateDataKey API provides a data key that can be used to encrypt the file within the Lambda function."",
    ""Use a custom KMS customer master key created for S3 in the Lambda function code."": ""This option is incorrect because simply using a custom KMS CMK does not automatically encrypt the data before uploading. While a custom CMK can be used to generate data keys for encryption, the option does not specify the use of the GenerateDataKey API to perform client-side encryption, which is necessary to meet the requirement of encrypting data before upload.""
}
```"
"Company D is running their corporate website on Amazon S3 accessed from `http://www.companyd.com`. Their marketing team has published new web fonts to a separate S3 bucket accessed by the S3 endpoint `https://s3-us-west-1.amazonaws.com/cdfonts`. While testing the new web fonts, Company D recognized the web fonts are being blocked by the browser. What should Company D do to prevent the web fonts from being blocked by the browser?","[""Enable versioning on the cdfonts bucket for each web font."", ""Create a policy on the cdfonts bucket to enable access to everyone."", ""Add the Content-MD5 header to the request for webfonts in the cdfonts bucket from the website."", ""Configure the cdfonts bucket to allow cross-origin requests by creating a CORS configuration.""]","[""Configure the cdfonts bucket to allow cross-origin requests by creating a CORS configuration.""]","```json
{
    ""Enable versioning on the cdfonts bucket for each web font."": ""This choice is incorrect because versioning in S3 is used to keep multiple versions of an object in the same bucket, which helps in recovering from unintended user actions and application failures. It does not address the issue of web fonts being blocked by the browser due to cross-origin resource sharing (CORS) policies."",
    ""Create a policy on the cdfonts bucket to enable access to everyone."": ""This choice is incorrect because creating a bucket policy to allow access to everyone would make the bucket publicly accessible, which is a security risk and does not solve the issue of CORS. CORS is a browser security feature that restricts web pages from making requests to a different domain than the one that served the web page, and it requires a CORS configuration, not a bucket policy."",
    ""Add the Content-MD5 header to the request for webfonts in the cdfonts bucket from the website."": ""This choice is incorrect because the Content-MD5 header is used to provide a message integrity check for the data being uploaded to S3. It does not relate to or resolve issues with CORS, which is the cause of the web fonts being blocked by the browser."",
    ""Configure the cdfonts bucket to allow cross-origin requests by creating a CORS configuration."": ""This choice is correct because configuring CORS on the S3 bucket allows the browser to make cross-origin requests to the bucket. CORS is a mechanism that allows restricted resources on a web page to be requested from another domain outside the domain from which the first resource was served. By setting up a CORS configuration, Company D can specify which domains are allowed to access the resources in the cdfonts bucket, thus preventing the web fonts from being blocked.""
}
```"
A developer must extend an existing application that is based on the AWS Serverless Application Model (AWS SAM). The developer has used the AWS SAM CLI to create the project. The project contains different AWS Lambda functions. Which combination of commands must the developer use to redeploy the AWS SAM application? (Select TWO),"[""`sam init`."", ""`sam validate`."", ""`sam build`."", ""`sam deploy`."", ""`sam publish`.""]","[""`sam build`."", ""`sam deploy`.""]","```json
{
    ""`sam init`."": ""This command is used to initialize a new AWS SAM application. It sets up the initial project structure and is not used for redeploying an existing application. Therefore, it is incorrect in the context of redeploying an application."",
    ""`sam validate`."": ""This command is used to validate an AWS SAM template to ensure it is syntactically correct. While it is a good practice to validate templates before deployment, it is not directly involved in the redeployment process itself. Thus, it is incorrect for the specific task of redeploying the application."",
    ""`sam build`."": ""This command is correct because it compiles the application source code and prepares the deployment artifacts. It is a necessary step before deploying the application, as it ensures that the latest code changes are included in the deployment package."",
    ""`sam deploy`."": ""This command is correct because it is used to deploy the built application to AWS. It uploads the deployment package to AWS and creates or updates the resources defined in the AWS SAM template. This is a critical step in the redeployment process."",
    ""`sam publish`."": ""This command is used to publish a packaged AWS SAM application to the AWS Serverless Application Repository. It is not involved in the redeployment of an application to AWS infrastructure, making it incorrect for this scenario.""
}
```"
"An application deployed on AWS Elastic Beanstalk experiences increased error rates during deployments of new application versions, resulting in service degradation for users. The Development team believes that this is because of the reduction in capacity during the deployment steps. The team would like to change the deployment policy configuration of the environment to an option that maintains full capacity during deployment while using the existing instances. Which deployment policy will meet these requirements while using the existing instances?","[""All at once."", ""Rolling."", ""Rolling with additional batch."", ""Immutable.""]","[""Rolling with additional batch.""]","```json
{
    ""All at once."": ""This deployment policy replaces all instances at once with the new version, leading to a temporary reduction in capacity and potential downtime. This does not meet the requirement of maintaining full capacity during deployment, hence it is incorrect."",
    ""Rolling."": ""Rolling deployments update instances in batches, which can reduce capacity temporarily as each batch is replaced. This does not maintain full capacity during deployment, which is why it is incorrect for the given scenario."",
    ""Rolling with additional batch."": ""This deployment policy adds an extra batch of instances temporarily during the deployment process, ensuring that the full capacity is maintained. This meets the requirement of maintaining full capacity while using existing instances, making it the correct choice."",
    ""Immutable."": ""Immutable deployments create a separate set of instances with the new application version before switching traffic to them. While this maintains full capacity, it does not use the existing instances, which does not align with the requirement to use existing instances, making it incorrect.""
}
```"
A Developer is creating an application that needs to locate the public IPv4 address of the Amazon EC2 instance on which it runs. How can the application locate this information?,"[""Get the instance metadata by retrieving http://169.254.169.254/latest/metadata/."", ""Get the instance user data by retrieving http://169.254.169.254/latest/userdata/."", ""Get the application to run IFCONFIG to get the public IP address."", ""Get the application to run IPCONFIG to get the public IP address.""]","[""Get the instance metadata by retrieving http://169.254.169.254/latest/metadata/.""]","{""Get the instance metadata by retrieving http://169.254.169.254/latest/metadata/."": ""This choice is correct because the instance metadata service provides information about the instance, including its public IPv4 address. The metadata can be accessed from within the instance using the special IP address 169.254.169.254. This is a common practice for applications running on EC2 instances to obtain instance-specific data, such as public IP addresses, without needing external configuration."", ""Get the instance user data by retrieving http://169.254.169.254/latest/userdata/."": ""This choice is incorrect because user data is intended for instance initialization scripts and does not contain information about the instance's public IP address. User data is typically used for bootstrapping applications or configurations at instance launch, not for retrieving metadata like IP addresses."", ""Get the application to run IFCONFIG to get the public IP address."": ""This choice is incorrect because IFCONFIG is a command used to display network configuration on Unix/Linux systems, but it will only show the private IP address assigned to the instance, not the public IP address. The public IP is managed by AWS and not directly visible through system network configuration commands."", ""Get the application to run IPCONFIG to get the public IP address."": ""This choice is incorrect because IPCONFIG is a command used on Windows systems to display network configuration, similar to IFCONFIG on Unix/Linux. Like IFCONFIG, it will only show the private IP address of the instance and not the public IP address. The public IP is not directly accessible through these system commands.""}"
"The development team is working on an API that will be served from Amazon API gateway. The API will be served from three environments: development, test, and production. The API Gateway is configured to use 237 GB of cache in all three stages. Which is the MOST cost-efficient deployment strategy?","[""Create a single API Gateway with all three stages."", ""Create three API Gateways, one for each stage in a single AWS account."", ""Create an API Gateway in three separate AWS accounts."", ""Enable the cache for development and test environments only when needed.""]","[""Enable the cache for development and test environments only when needed.""]","```json
{
    ""Create a single API Gateway with all three stages."": ""This option is incorrect because while it allows for easy management of different stages (development, test, production) within a single API Gateway, it does not address the cost-efficiency of caching. Caching is charged based on the size and duration of the cache, and having a large cache size (237 GB) always enabled for all stages can be costly, especially for development and test stages that may not require constant caching."",
    ""Create three API Gateways, one for each stage in a single AWS account."": ""This option is incorrect because creating separate API Gateways for each stage increases management overhead and does not inherently solve the issue of cache cost efficiency. Each API Gateway would still incur caching costs if caching is enabled, and managing multiple gateways can complicate deployment and maintenance processes."",
    ""Create an API Gateway in three separate AWS accounts."": ""This option is incorrect as it unnecessarily complicates the architecture by requiring separate AWS accounts for each environment. This increases complexity in terms of account management and does not provide any cost benefits related to caching. The cost of caching remains the same regardless of the number of accounts."",
    ""Enable the cache for development and test environments only when needed."": ""This option is correct because it addresses the cost-efficiency aspect by reducing unnecessary caching costs. Caching in API Gateway is charged based on the size and duration, so enabling it only when needed for development and test environments minimizes costs. This approach aligns with AWS best practices of optimizing resource usage and cost management by only using resources when necessary.""
}
```"
"A company is migrating its on-premises database to Amazon RDS for MySQL. The company has read-heavy workloads, and wants to make sure it re-factors its code to achieve optimum read performance for its queries. How can this objective be met?","[""Add database retries to effectively use RDS with vertical scaling."", ""Use RDS with multi-AZ deployment."", ""Add a connection string to use an RDS read replica for read queries."", ""Add a connection string to use a read replica on an EC2 instance.""]","[""Add a connection string to use an RDS read replica for read queries.""]","```json
{
    ""Add database retries to effectively use RDS with vertical scaling."": ""This choice is incorrect because adding database retries and vertical scaling do not specifically address optimizing read performance for read-heavy workloads. Vertical scaling involves increasing the instance size, which can improve performance but does not specifically optimize read operations. Database retries are more about handling transient errors rather than optimizing read performance."",
    ""Use RDS with multi-AZ deployment."": ""This choice is incorrect because Multi-AZ deployments are primarily used for high availability and failover support rather than optimizing read performance. Multi-AZ deployments ensure that the database is available in case of an instance failure, but they do not provide additional read capacity or distribute read queries."",
    ""Add a connection string to use an RDS read replica for read queries."": ""This choice is correct because using RDS read replicas is a best practice for scaling read-heavy workloads. Read replicas allow you to offload read queries from the primary database instance, thus improving read performance by distributing the read load across multiple instances. This is particularly useful for applications with a high volume of read operations."",
    ""Add a connection string to use a read replica on an EC2 instance."": ""This choice is incorrect because it suggests using a read replica on an EC2 instance, which is not a standard practice. RDS read replicas are managed by AWS and are part of the RDS service, not EC2. The correct approach is to use RDS read replicas directly for distributing read queries, not to involve EC2 in this context.""
}
```"
A developer needs to modify an application architecture to meet new functional requirements. Application data is stored in Amazon DynamoDB and processed for analysis in a nightly batch. The system analysts do not want to wait unit the next day to view the processed data and have asked to have it available in near-real time. Which application architect pattern would enables the data to be processed as it is received?,"[""Event driven."", ""Client served driven."", ""Fan-out driven."", ""Schedule driven.""]","[""Event driven.""]","```json
{
    ""Event driven."": ""This choice is correct because an event-driven architecture allows the application to process data as it is received, enabling near-real-time processing. In AWS, this can be implemented using services like AWS Lambda and Amazon DynamoDB Streams, where changes in DynamoDB trigger Lambda functions to process the data immediately. This pattern is suitable for applications that require immediate processing of data upon arrival, aligning with the requirement for near-real-time data availability."",
    ""Client served driven."": ""This choice is incorrect because client-served driven architecture typically refers to systems where the client initiates requests for data or services. It does not inherently provide a mechanism for processing data as it is received in real-time. This pattern is more about how clients interact with services rather than how data is processed."",
    ""Fan-out driven."": ""This choice is incorrect in this context because fan-out is a pattern where a single event is sent to multiple destinations. While it can be part of an event-driven architecture, on its own, it does not address the requirement for near-real-time processing. Fan-out is more about distributing data to multiple consumers rather than processing data immediately as it is received."",
    ""Schedule driven."": ""This choice is incorrect because schedule-driven architectures rely on predefined schedules to trigger processing tasks, such as using AWS CloudWatch Events or AWS Batch for scheduled jobs. This approach is not suitable for near-real-time processing as it introduces delays based on the schedule, which contradicts the requirement for immediate data processing.""
}
```"
"A software company needs to make sure user-uploaded documents are securely stored in Amazon S3. The documents must be encrypted at rest in Amazon S3. The company does not want to manage the security infrastructure in-house, but the company still needs extra protection to ensure it has control over its encryption keys due to industry regulations. Which encryption strategy should a developer use to meet these requirements?","[""Server-side encryption with Amazon S3 managed keys (SSE-S3)."", ""Server-side encryption with customer-provided encryption keys (SSE-C)."", ""Server-side encryption with AWS KMS managed keys (SSE-KMS)."", ""Client-side encryption.""]","[""Server-side encryption with AWS KMS managed keys (SSE-KMS).""]","```json
{
    ""Server-side encryption with Amazon S3 managed keys (SSE-S3)."": ""This option is incorrect because while SSE-S3 provides server-side encryption using keys managed by Amazon S3, it does not give the company control over the encryption keys. The company requires control over the encryption keys due to industry regulations, which SSE-S3 does not provide."",
    ""Server-side encryption with customer-provided encryption keys (SSE-C)."": ""This option is incorrect because SSE-C requires the company to manage its own encryption keys, which contradicts the requirement of not wanting to manage the security infrastructure in-house. SSE-C would require the company to handle key management and rotation, which adds complexity and overhead."",
    ""Server-side encryption with AWS KMS managed keys (SSE-KMS)."": ""This option is correct because SSE-KMS allows the company to use AWS Key Management Service (KMS) to manage encryption keys, providing additional control and audit capabilities over the keys used for encryption. It meets the requirement of not managing the security infrastructure in-house while still providing control over the encryption keys due to industry regulations. AWS KMS integrates with S3 to provide secure key management and auditing features."",
    ""Client-side encryption."": ""This option is incorrect because client-side encryption requires the company to handle encryption and decryption operations on the client side, including key management. This contradicts the requirement of not wanting to manage the security infrastructure in-house. It also adds complexity and potential performance overhead, which is unnecessary given the requirements.""
}
```"
"An application uses Amazon Kinesis Data Streams to ingest and process large streams of data records in real time. Amazon EC2 instances consume and process the data from the shards of the Kinesis data stream by using Amazon Kinesis Client Library (KCL). The application handles the failure scenarios and does not require standby workers. The application reports that a specific shard is receiving more data than expected. To adapt to the changes in the rate of data flow, the `hot` shard is resharded. Assuming that the initial number of shards in the Kinesis data stream is 4, and after resharding the number of shards increased to 6, what is the maximum number of EC2 instances that can be deployed to process data from all the shards?","[""12."", ""6."", ""4."", ""1.""]","[""6.""]","```json
{
    ""12."": ""This choice is incorrect because the number of EC2 instances should match the number of shards for optimal processing. Each shard can be processed by one KCL worker, which typically runs on one EC2 instance. Since there are 6 shards after resharding, a maximum of 6 EC2 instances can be effectively utilized. Deploying 12 instances would not provide any additional benefit as there are only 6 shards to process."",
    ""6."": ""This choice is correct because after resharding, the number of shards increased to 6. According to AWS best practices, each shard can be processed by one KCL worker, which typically runs on one EC2 instance. Therefore, to process all shards efficiently, the maximum number of EC2 instances that can be deployed is 6, with each instance handling one shard."",
    ""4."": ""This choice is incorrect because the number of shards has increased to 6 after resharding. While initially there were 4 shards, and thus 4 EC2 instances could suffice, the increase in shards means more EC2 instances are needed to handle the additional shards. Using only 4 instances would mean some shards are not being processed optimally."",
    ""1."": ""This choice is incorrect because using only one EC2 instance would not allow for parallel processing of all 6 shards. Each shard should ideally be processed by a separate KCL worker on its own EC2 instance to handle the data flow efficiently. Using only one instance would create a bottleneck and reduce the application's ability to process data in real time.""
}
```"
"A gaming company is developing a mobile game application for iOS® and Android® platforms. This mobile game securely stores user data locally on the device. The company wants to allow users to use multiple device for the game, which requires user data synchronization across device.Which service should be used to synchronize user data across devices without the need to create a backend application?","[""AWS Lambda."", ""Amazon S3."", ""Amazon DynamoDB."", ""Amazon Cognito.""]","[""Amazon Cognito.""]","```json
{
    ""AWS Lambda."": ""AWS Lambda is a compute service that lets you run code without provisioning or managing servers. It is not designed for data synchronization across devices. Lambda can be used to process data, but it does not inherently provide data synchronization capabilities for mobile applications. Therefore, it is incorrect for this use case."",
    ""Amazon S3."": ""Amazon S3 is an object storage service that can store and retrieve any amount of data. While it can be used to store user data, it does not provide built-in synchronization capabilities across devices. Developers would need to build additional logic to handle synchronization, which is not ideal for this scenario. Hence, it is not the correct choice."",
    ""Amazon DynamoDB."": ""Amazon DynamoDB is a NoSQL database service that provides fast and predictable performance with seamless scalability. While it can store user data, it does not inherently provide cross-device synchronization. Developers would need to implement additional logic to achieve synchronization, making it an incorrect choice for this requirement."",
    ""Amazon Cognito."": ""Amazon Cognito is the correct choice because it provides user identity and data synchronization capabilities across devices. It allows developers to create unique identities for users and synchronize data across devices without the need for a backend application. This service is specifically designed for scenarios where user data needs to be synchronized across multiple devices, making it the best fit for the given use case.""
}
```"
"A Developer is making changes to a custom application that is currently using AWS Elastic Beanstalk. After the Developer completes the changes, what solutions will update the Elastic Beanstalk environment with the new application version? (Choose TWO)","[""Package the application code into a `.zip` file, and upload, then deploy the packaged application from the AWS Management Console."", ""Package the application code into a `.tar` file, create a new application version from the AWS Management Console, then update the environment by using AWS CLI."", ""Package the application code into a `.tar` file, and upload and deploy the packaged application from the AWS Management Console."", ""Package the application code into a `.zip` file, create a new application version from the packaged application by using AWS CLI, then update the environment by using AWS CLI."", ""Package the application code into a `.zip` file, create a new application version from the AWS Management Console, then rebuild the environment by using AWS CLI.""]","[""Package the application code into a `.zip` file, and upload, then deploy the packaged application from the AWS Management Console."", ""Package the application code into a `.zip` file, create a new application version from the packaged application by using AWS CLI, then update the environment by using AWS CLI.""]","```json
{
    ""Package the application code into a `.zip` file, and upload, then deploy the packaged application from the AWS Management Console."": ""This choice is correct because AWS Elastic Beanstalk supports deploying applications by uploading a .zip file directly through the AWS Management Console. This is a straightforward method for updating an application version and is a common practice for developers using Elastic Beanstalk."",
    ""Package the application code into a `.tar` file, create a new application version from the AWS Management Console, then update the environment by using AWS CLI."": ""This choice is incorrect because AWS Elastic Beanstalk primarily supports .zip files for application deployment. While you can use the AWS CLI to update environments, the use of a .tar file is not supported for creating application versions in Elastic Beanstalk."",
    ""Package the application code into a `.tar` file, and upload and deploy the packaged application from the AWS Management Console."": ""This choice is incorrect because AWS Elastic Beanstalk does not support .tar files for uploading and deploying applications. The supported format is a .zip file, which is necessary for creating application versions."",
    ""Package the application code into a `.zip` file, create a new application version from the packaged application by using AWS CLI, then update the environment by using AWS CLI."": ""This choice is correct because it aligns with AWS best practices for using the AWS CLI to manage Elastic Beanstalk applications. Developers can create a new application version using a .zip file and then update the environment using the AWS CLI, which is a flexible and automated approach."",
    ""Package the application code into a `.zip` file, create a new application version from the AWS Management Console, then rebuild the environment by using AWS CLI."": ""This choice is incorrect because rebuilding the environment is not necessary to update it with a new application version. Rebuilding is a more drastic action that is typically used for different purposes, such as resetting the environment, rather than simply updating the application version.""
}
```"
A company is running an application built on AWS Lambda functions. One Lambda function has performance issues when it has to download a 50MB file from the Internet in every execution. This function is called multiple times a second. What solution would give the BEST performance increase?,"[""Cache the file in the /tmp directory."", ""Increase the Lambda maximum execution time."", ""Put an Elastic Load Balancer in front of the Lambda function."", ""Cache the file in Amazon S3.""]","[""Cache the file in the /tmp directory.""]","```json
{
    ""Cache the file in the /tmp directory."": ""This choice is correct because AWS Lambda provides a /tmp directory with up to 512 MB of storage that can be used for temporary storage during the execution of a function. Caching the file in the /tmp directory can significantly improve performance because it avoids the need to download the file from the Internet on every execution. If the Lambda function is invoked multiple times in a short period, the cached file can be reused, reducing latency and improving efficiency. This approach leverages the Lambda execution environment's ability to reuse the same instance for subsequent invocations, which is a best practice for optimizing performance."",
    ""Increase the Lambda maximum execution time."": ""This choice is incorrect because increasing the Lambda maximum execution time does not address the root cause of the performance issue, which is the repeated downloading of a large file. While increasing the timeout might prevent the function from timing out, it does not improve performance or reduce latency. The goal is to minimize the time spent downloading the file, not just to allow more time for the function to complete."",
    ""Put an Elastic Load Balancer in front of the Lambda function."": ""This choice is incorrect because an Elastic Load Balancer (ELB) is used to distribute incoming application or network traffic across multiple targets, such as EC2 instances or Lambda functions, to improve availability and fault tolerance. It does not address the specific performance issue of downloading a file from the Internet. ELB is not designed to cache files or reduce download times, so it would not provide a performance increase in this scenario."",
    ""Cache the file in Amazon S3."": ""This choice is incorrect because while Amazon S3 is a highly durable and available object storage service, using it as a cache for a Lambda function is not optimal for performance in this context. Storing the file in S3 would still require the Lambda function to download it on each execution, which would not significantly reduce latency compared to downloading from the Internet. The /tmp directory is more suitable for caching in this scenario due to its proximity and speed within the Lambda execution environment.""
}
```"
Queries to an Amazon DynamoDB table are consuming a large amount of read capacity. The table has a significant number of large attributes. The application does not need all of the attribute data. How can DynamoDB costs be minimized while maximizing application performance?,"[""Batch all the writes, and perform the write operations when no or few reads are being performed."", ""Create a global secondary index with a minimum set of projected attributes."", ""Implement exponential backoffs in the application."", ""Load balance the reads to the table using an Application Load Balancer.""]","[""Create a global secondary index with a minimum set of projected attributes.""]","```json
{
    ""Batch all the writes, and perform the write operations when no or few reads are being performed."": ""This choice is incorrect because batching writes and scheduling them during low read periods does not address the issue of high read capacity consumption. The problem is related to read operations, not write operations. Batching writes can optimize write capacity usage but does not impact read capacity or costs."",
    ""Create a global secondary index with a minimum set of projected attributes."": ""This choice is correct because creating a global secondary index (GSI) with only the necessary attributes can reduce the amount of data read during queries. By projecting only the required attributes, the read capacity units (RCUs) consumed are minimized, thus reducing costs. This approach is aligned with AWS best practices for optimizing DynamoDB read costs and performance."",
    ""Implement exponential backoffs in the application."": ""This choice is incorrect because exponential backoff is a technique used to handle throttling and transient errors by retrying operations with increasing wait times. It does not reduce read capacity consumption or costs. The issue in the question is about optimizing read capacity usage, not handling throttling or errors."",
    ""Load balance the reads to the table using an Application Load Balancer."": ""This choice is incorrect because an Application Load Balancer is used for distributing incoming application traffic across multiple targets, such as EC2 instances, not for balancing reads to a DynamoDB table. DynamoDB automatically handles read and write traffic distribution, and using a load balancer does not impact read capacity usage or costs.""
}
```"
A Developer is writing a REST service that will add items to a shopping list. The service is built on Amazon API Gateway with AWS Lambda integrations. The shopping list items are send as query string parameters in the method request. How should the Developer convert the query string parameters to arguments for the Lambda function?,"[""Enable request validation."", ""Include the Amazon Resource Name (ARN) of the Lambda function."", ""Change the integration type."", ""Create a mapping template.""]","[""Create a mapping template.""]","```json
{
    ""Enable request validation."": ""Incorrect. Request validation in API Gateway is used to ensure that the incoming request meets the expected format and data types before it reaches the backend. It does not convert query string parameters to arguments for a Lambda function. Request validation is more about ensuring the integrity and correctness of the request data rather than transforming it."",
    ""Include the Amazon Resource Name (ARN) of the Lambda function."": ""Incorrect. The ARN of a Lambda function is used to identify the function within AWS services and is necessary for API Gateway to invoke the Lambda function. However, it does not play a role in converting query string parameters to Lambda function arguments. The ARN is more about permissions and service integration rather than data transformation."",
    ""Change the integration type."": ""Incorrect. Changing the integration type refers to how API Gateway interacts with the backend, such as using HTTP, Lambda, or Mock integrations. It does not directly relate to converting query string parameters to Lambda function arguments. The integration type is about the method of communication rather than data transformation."",
    ""Create a mapping template."": ""Correct. A mapping template in API Gateway is used to transform the incoming request data before it is sent to the backend service, such as a Lambda function. This is done using Velocity Template Language (VTL). Mapping templates can extract query string parameters and convert them into a format that the Lambda function can process, such as a JSON object. This is a key feature for customizing how data is passed to Lambda functions and is a common practice when dealing with API Gateway integrations.""
}
```"
"A development team is creating a new application designed to run on AWS. While the test and production environments will run on Amazon EC2 instances, developers will each run their own environment on their laptops. Which of the following is the simplest and MOST secure way to access AWS services from the local development machines?","[""Use an IAM role to assume a role and execute API calls using the role."", ""Create an IAM user to be shared with the entire development team, provide the development team with the access key."", ""Create an IAM user for each developer on the team: provide each developer with a unique access key."", ""Set up a federation through an Amazon Cognito user pool.""]","[""Create an IAM user for each developer on the team: provide each developer with a unique access key.""]","```json
{
    ""Use an IAM role to assume a role and execute API calls using the role."": ""This option is incorrect because IAM roles are typically used for granting permissions to AWS services or applications running on AWS infrastructure, such as EC2 instances, rather than local development environments. While roles can be assumed programmatically, it requires additional setup and is not the simplest method for local development access."",
    ""Create an IAM user to be shared with the entire development team, provide the development team with the access key."": ""This option is incorrect because sharing IAM user credentials among multiple users is against AWS security best practices. It poses a security risk as it becomes difficult to track individual actions and manage permissions effectively. Each user should have their own IAM user and credentials to ensure accountability and security."",
    ""Create an IAM user for each developer on the team: provide each developer with a unique access key."": ""This option is correct because creating individual IAM users for each developer ensures that each developer has their own set of credentials. This approach follows AWS best practices for security and access management, allowing for fine-grained access control and auditing of actions performed by each user. It is the simplest and most secure way to manage access for local development environments."",
    ""Set up a federation through an Amazon Cognito user pool."": ""This option is incorrect because setting up federation through Amazon Cognito is more complex and typically used for authenticating users of an application rather than developers accessing AWS services. It involves additional configuration and is not necessary for providing developers access to AWS resources from their local machines.""
}
```"
How is provisioned throughput affected by the chosen consistency model when reading data from a DynamoDB table?,"[""Strongly consistent reads use the same amount of throughput as eventually consistent reads."", ""Strongly consistent reads use more throughput than eventually consistent reads."", ""Strongly consistent reads use less throughput than eventually consistent reads."", ""Strongly consistent reads use variable throughput depending on read activity.""]","[""Strongly consistent reads use more throughput than eventually consistent reads.""]","{""Strongly consistent reads use the same amount of throughput as eventually consistent reads."": ""This choice is incorrect. In DynamoDB, strongly consistent reads consume more read capacity units than eventually consistent reads. Specifically, a strongly consistent read requires one read capacity unit to read up to 4 KB of data, whereas an eventually consistent read requires only half a read capacity unit for the same amount of data. This is because strongly consistent reads provide the most up-to-date data by ensuring all copies of the data are consistent before returning a response, which requires more resources."", ""Strongly consistent reads use more throughput than eventually consistent reads."": ""This choice is correct. According to AWS documentation, strongly consistent reads in DynamoDB consume more throughput than eventually consistent reads. A strongly consistent read uses one read capacity unit per 4 KB of data, while an eventually consistent read uses only half a read capacity unit for the same data size. This is due to the additional resources needed to ensure that the data returned is the most recent and consistent across all copies."", ""Strongly consistent reads use less throughput than eventually consistent reads."": ""This choice is incorrect. Strongly consistent reads require more throughput because they ensure that the data returned is the most recent, which involves more processing and resource usage. Eventually consistent reads are cheaper in terms of throughput because they allow for stale data, which requires less coordination and resources."", ""Strongly consistent reads use variable throughput depending on read activity."": ""This choice is incorrect. The throughput required for strongly consistent reads is not variable based on read activity; it is determined by the size of the data being read. Each strongly consistent read requires one read capacity unit for every 4 KB of data, regardless of the level of read activity. The throughput is consistent and predictable based on the data size, not the read activity.""}"
A developer needs to deploy a new version to an AWS Elastic Beanstalk application. How can the developer accomplish this task?,"[""Upload and deploy the new application version in the Elastic Beanstalk console."", ""Use the eb init CLI command to deploy a new version."", ""Terminate the current Elastic Beanstalk environment and create a new one."", ""Modify the ebextensions folder to add a source option to services.""]","[""Upload and deploy the new application version in the Elastic Beanstalk console.""]","```json
{
    ""Upload and deploy the new application version in the Elastic Beanstalk console."": ""This choice is correct because the AWS Elastic Beanstalk console provides a straightforward way to upload and deploy new application versions. Developers can use the console to upload a new version of their application and then deploy it to their existing environment. This process is part of the standard workflow for deploying applications on Elastic Beanstalk, as outlined in AWS documentation. It allows for easy version management and deployment without disrupting the current environment."",
    ""Use the eb init CLI command to deploy a new version."": ""This choice is incorrect because the 'eb init' command is used to initialize a new Elastic Beanstalk application and set up the local repository for use with Elastic Beanstalk. It is not used for deploying new application versions. To deploy a new version using the AWS CLI, the 'eb deploy' command would be appropriate, not 'eb init'."",
    ""Terminate the current Elastic Beanstalk environment and create a new one."": ""This choice is incorrect because terminating the current environment and creating a new one is not necessary to deploy a new version of an application. Elastic Beanstalk is designed to handle application version updates without needing to recreate environments, which would be inefficient and could lead to unnecessary downtime. The correct approach is to deploy a new version to the existing environment."",
    ""Modify the ebextensions folder to add a source option to services."": ""This choice is incorrect because modifying the .ebextensions folder is typically used for customizing the environment configuration, such as setting environment variables or installing additional software. It is not used for deploying new application versions. Deploying a new version involves uploading the new code package and deploying it through the console or CLI, not modifying configuration files.""
}
```"
"A gaming application stores scores for players in an Amazon DynamoDB table that has four attributes: `user_id`, `user_name`, `user_score`, and `user_rank`. The users are allowed to update their names only if a user is authenticated by web identity federation. Which set of conditions should be added in the policy attached to the role for the DynamoDB: PutItem API call?","[""Option A."", ""Option B."", ""Option C."", ""Option D.""]","[""Option A.""]","```json
{
    ""Option A."": ""Option A is correct because it likely includes conditions that ensure the user is authenticated via web identity federation before allowing the PutItem operation on the DynamoDB table. This aligns with AWS best practices for securing access to resources by using identity federation to authenticate users, which is a common approach to manage user identities and permissions in a scalable manner."",
    ""Option B."": ""Option B is incorrect because it might not include the necessary conditions to verify that the user is authenticated through web identity federation. Without this condition, unauthorized users could potentially update the user_name attribute, which would violate the security requirements outlined in the question."",
    ""Option C."": ""Option C is incorrect because it may involve conditions or permissions that do not pertain to web identity federation or do not adequately restrict the PutItem operation to authenticated users. This could lead to security vulnerabilities by allowing unauthorized access to modify user data."",
    ""Option D."": ""Option D is incorrect because it likely lacks the specific condition to check for web identity federation authentication. This is crucial to ensure that only authenticated users can update their names, as specified in the question. Without this condition, the policy would not meet the security requirements.""
}
```"
A developer wants the ability to roll back to a previous version of an AWS Lambda function in the event of errors caused by a new deployment. How can the developer achieve this with MINIMAL impact on users?,"[""Change the application to use an alias that points to the current version. Deploy the new version of the code. Update the alias to use the newly deployed version. If too many errors are encountered, point the alias back to the previous version."", ""Change the application to use an alias that points to the current version. Deploy the new version of the code. Update the alias to direct 10% of users to the newly deployed version. If too many errors are encountered, send 100% of traffic to the previous version."", ""Do not make any changes to the application Deploy the new version of the code. If too many errors are encountered, point the application back to the previous version using the version number in the Amazon Resource Name (ARN)."", ""Create three aliases: new, existing, and router. Point the existing alias to the current version. Have the router alias direct 100% of users to the existing alias. Update the application to use the router alias. Deploy the new version of the code. Point the new alias to this version. Update the router alias to direct 10% of users to the new alias. If too many errors are encountered, send 100% of traffic to the existing alias.""]","[""Change the application to use an alias that points to the current version. Deploy the new version of the code. Update the alias to direct 10% of users to the newly deployed version. If too many errors are encountered, send 100% of traffic to the previous version.""]","```json
{
    ""Change the application to use an alias that points to the current version. Deploy the new version of the code. Update the alias to use the newly deployed version. If too many errors are encountered, point the alias back to the previous version."": ""This choice is incorrect because it does not minimize the impact on users. By updating the alias to point entirely to the new version immediately, all users are exposed to potential errors. AWS best practices suggest using a phased rollout to minimize user impact."",
    ""Change the application to use an alias that points to the current version. Deploy the new version of the code. Update the alias to direct 10% of users to the newly deployed version. If too many errors are encountered, send 100% of traffic to the previous version."": ""This choice is correct because it uses an alias to gradually direct traffic to the new version, which minimizes user impact. By directing only 10% of users initially, the developer can monitor for errors and roll back if necessary. This approach aligns with AWS best practices for safe deployments using Lambda aliases."",
    ""Do not make any changes to the application Deploy the new version of the code. If too many errors are encountered, point the application back to the previous version using the version number in the Amazon Resource Name (ARN)."": ""This choice is incorrect because it does not use aliases, which are a best practice for managing Lambda versions. Without aliases, the application would need to be updated to point to specific version ARNs, which is less flexible and can lead to more downtime during rollbacks."",
    ""Create three aliases: new, existing, and router. Point the existing alias to the current version. Have the router alias direct 100% of users to the existing alias. Update the application to use the router alias. Deploy the new version of the code. Point the new alias to this version. Update the router alias to direct 10% of users to the new alias. If too many errors are encountered, send 100% of traffic to the existing alias."": ""This choice is overly complex and not necessary for a simple rollback strategy. While it does allow for gradual traffic shifting, the use of multiple aliases introduces unnecessary complexity and potential for misconfiguration. The simpler approach of using a single alias to manage traffic between versions is more efficient and aligns better with AWS best practices.""
}
```"
"An application contains two components: one component to handle HTTP requests, and another component to handle background processing tasks. Each component must scale independently. The developer wants to deploy this application using AWS Elastic Beanstalk. How should this application be deployed, based on these requirements?","[""Deploy the application in a single Elastic Beanstalk environment."", ""Deploy each component in a separate Elastic Beanstalk environment."", ""Use multiple Elastic Beanstalk environments for the HTTP component but one environment for the background task component."", ""Use multiple Elastic Beanstalk environments for the background task component but one environment for the HTTP component.""]","[""Deploy each component in a separate Elastic Beanstalk environment.""]","```json
{
    ""Deploy the application in a single Elastic Beanstalk environment."": ""This choice is incorrect because deploying both components in a single Elastic Beanstalk environment would not allow them to scale independently. Elastic Beanstalk environments are designed to manage the scaling of applications, but if both components are in the same environment, they will scale together, which does not meet the requirement for independent scaling."",
    ""Deploy each component in a separate Elastic Beanstalk environment."": ""This choice is correct because deploying each component in a separate Elastic Beanstalk environment allows them to scale independently, which is a key requirement. Elastic Beanstalk environments can be configured to scale based on different metrics, and separating the components ensures that each can scale according to its own demand. This approach aligns with AWS best practices for microservices architecture, where components are decoupled and can be managed independently."",
    ""Use multiple Elastic Beanstalk environments for the HTTP component but one environment for the background task component."": ""This choice is incorrect because it suggests using multiple environments for one component and a single environment for another, which does not address the requirement for both components to scale independently. The background task component would not be able to scale independently if it is confined to a single environment."",
    ""Use multiple Elastic Beanstalk environments for the background task component but one environment for the HTTP component."": ""This choice is incorrect for similar reasons as the previous one. It suggests using multiple environments for the background task component but a single environment for the HTTP component, which does not allow both components to scale independently. Both components need their own environments to meet the requirement for independent scaling.""
}
```"
A company is using AWS CloudFormation templates to deploy AWS resources. The company needs to update one of its AWS CloudFormation stacks. What can the company do to find out how the changes will impact the resources that are running?,"[""Investigate the change sets."", ""Investigate the stack policies."", ""Investigate the Metadata section."", ""Investigate the Resources section.""]","[""Investigate the change sets.""]","```json
{
    ""Investigate the change sets."": ""This choice is correct. AWS CloudFormation Change Sets allow you to preview how proposed changes to a stack might impact your running resources. By creating a change set, you can see a summary of the changes that will be made, including which resources will be added, modified, or deleted. This is a best practice for managing changes in a controlled manner, as it helps avoid unintended disruptions to your applications. According to AWS documentation, using change sets is the recommended way to understand the impact of changes before applying them."",
    ""Investigate the stack policies."": ""This choice is incorrect. Stack policies in AWS CloudFormation are used to protect certain resources during stack updates by defining update actions that are allowed or denied. They do not provide insights into how changes will impact resources. Stack policies are more about controlling permissions and ensuring critical resources are not accidentally modified or deleted during updates."",
    ""Investigate the Metadata section."": ""This choice is incorrect. The Metadata section in a CloudFormation template is used to include additional information about the template, such as descriptions or configuration data that can be used by other services or tools. It does not provide information about the impact of changes on running resources. The Metadata section is more about providing context or supplementary information rather than assessing change impacts."",
    ""Investigate the Resources section."": ""This choice is incorrect. The Resources section of a CloudFormation template defines the AWS resources that will be created or updated. While it specifies what resources are involved, it does not provide a mechanism to understand the impact of changes. The Resources section is static and does not dynamically show the effects of proposed changes like a change set does.""
}
```"
"A developer is creating a serverless web application and maintains different branches of code. The developer wants to avoid updating the Amazon API Gateway target endpoint each time a new code push is performed. What solution would allow the developer to perform a code push efficiently, without the need to update the API Gateway?","[""Associate different AWS Lambda functions to an API Gateway target endpoint."", ""Create different stages in API Gateway, then associate API Gateway with AWS Lambda."", ""Create aliases and versions In AWS Lambda."", ""Tag the AWS Lambda functions with different names.""]","[""Create aliases and versions In AWS Lambda.""]","```json
{
    ""Associate different AWS Lambda functions to an API Gateway target endpoint."": ""This choice is incorrect because associating different Lambda functions to an API Gateway endpoint does not solve the problem of updating the endpoint with each code push. This approach would require manually changing the Lambda function associated with the endpoint each time, which is what the developer wants to avoid. It is not an efficient solution for managing different branches of code."",
    ""Create different stages in API Gateway, then associate API Gateway with AWS Lambda."": ""This choice is incorrect because while API Gateway stages can be used to manage different environments (e.g., dev, test, prod), they do not inherently solve the problem of managing different code branches without updating the endpoint. Stages are more suited for environment separation rather than branch management. The developer would still need to update the API Gateway configuration to point to different Lambda functions for each stage."",
    ""Create aliases and versions In AWS Lambda."": ""This choice is correct because AWS Lambda versions and aliases allow developers to manage different versions of their code without changing the API Gateway configuration. By using Lambda versions, developers can deploy new code as a new version, and aliases can be used to point to specific versions. This way, the API Gateway can always point to an alias, and the alias can be updated to point to the new version of the Lambda function, avoiding the need to update the API Gateway endpoint itself. This is a best practice for managing different code branches efficiently."",
    ""Tag the AWS Lambda functions with different names."": ""This choice is incorrect because tagging Lambda functions with different names does not affect the API Gateway configuration or the deployment process. Tags are metadata used for organizing and managing resources, not for managing code versions or branches. They do not provide a mechanism to switch between different code branches or versions in a way that avoids updating the API Gateway.""
}
```"
An application running on EC2 instances is storing data in an S3 bucket. Security policy mandates that all data must be encrypted in transit. How can the Developer ensure that all traffic to the S3 bucket is encrypted?,"[""Install certificates on the EC2 instances."", ""Create a bucket policy that allows traffic where SecureTransport is true."", ""Create an HTTPS redirect on the EC2 instances."", ""Create a bucket policy that denies traffic where SecureTransport is false.""]","[""Create a bucket policy that allows traffic where SecureTransport is true.""]","```json
{
    ""Install certificates on the EC2 instances."": ""This choice is incorrect because installing certificates on the EC2 instances would ensure that the traffic from the client to the EC2 instances is encrypted, but it does not guarantee that the traffic from the EC2 instances to the S3 bucket is encrypted. The requirement is to ensure encryption in transit to the S3 bucket, which is not addressed by this action alone."",
    ""Create a bucket policy that allows traffic where SecureTransport is true."": ""This choice is incorrect because allowing traffic where SecureTransport is true does not enforce encryption. It merely permits encrypted traffic but does not prevent unencrypted traffic. The correct approach is to deny unencrypted traffic, ensuring that only encrypted traffic is allowed."",
    ""Create an HTTPS redirect on the EC2 instances."": ""This choice is incorrect because creating an HTTPS redirect on the EC2 instances would ensure that incoming traffic to the EC2 instances is encrypted, but it does not affect the traffic from the EC2 instances to the S3 bucket. The requirement is to ensure encryption in transit to the S3 bucket, which is not addressed by this action."",
    ""Create a bucket policy that denies traffic where SecureTransport is false."": ""This choice is correct because creating a bucket policy that denies traffic where SecureTransport is false ensures that only requests that are made over HTTPS (which is encrypted) are allowed. This directly enforces the security policy requirement that all data must be encrypted in transit to the S3 bucket. According to AWS best practices, using bucket policies to enforce encryption in transit is a recommended approach.""
}
```"
A supplier is writing a new RESTful API for customers to query the status of orders. The customers requested the following API endpoint `http://www.supplierdomain.com/status/customerID`. Which of the following application designs meet the requirements? (Select TWO),"[""Amazon SQS; Amazon SNS."", ""Elastic Load Balancing; Amazon EC2."", ""Amazon ElastiCache; Amazon Elacticsearch Service."", ""Amazon API Gateway; AWS Lambda."", ""Amazon S3; Amazon CloudFront.""]","[""Elastic Load Balancing; Amazon EC2."", ""Amazon API Gateway; AWS Lambda.""]","```json
{
    ""Amazon SQS; Amazon SNS."": ""This choice is incorrect because Amazon SQS and Amazon SNS are messaging services. SQS is used for message queuing and SNS for pub/sub messaging. They are not suitable for directly handling RESTful API requests or serving HTTP endpoints. These services are more appropriate for decoupling microservices or distributing messages, not for serving API endpoints."",
    ""Elastic Load Balancing; Amazon EC2."": ""This choice is correct because Elastic Load Balancing (ELB) can distribute incoming HTTP requests to multiple Amazon EC2 instances, which can host the RESTful API. This setup is suitable for handling RESTful API requests as it provides scalability and reliability. EC2 instances can run the application logic to process the API requests, making it a valid solution for the given requirement."",
    ""Amazon ElastiCache; Amazon Elacticsearch Service."": ""This choice is incorrect because Amazon ElastiCache and Amazon Elasticsearch Service are not designed to directly handle RESTful API requests. ElastiCache is used for caching data to improve application performance, and Elasticsearch is used for search and analytics. They can be part of a backend architecture but do not serve as endpoints for RESTful APIs."",
    ""Amazon API Gateway; AWS Lambda."": ""This choice is correct because Amazon API Gateway can be used to create, publish, and manage RESTful APIs. It can directly handle HTTP requests and route them to AWS Lambda functions, which can execute the application logic. This serverless architecture is cost-effective and scalable, making it an ideal choice for building RESTful APIs. AWS documentation supports using API Gateway with Lambda for such use cases."",
    ""Amazon S3; Amazon CloudFront."": ""This choice is incorrect because Amazon S3 and Amazon CloudFront are primarily used for static content delivery. S3 is a storage service, and CloudFront is a content delivery network. They are not suitable for serving dynamic RESTful API requests, which require processing logic that these services do not provide.""
}
```"
A developer Is designing an AWS Lambda function that create temporary files that are less than 10 MB during execution. The temporary files will be accessed and modified multiple times during execution. The developer has no need to save or retrieve these files in the future. Where should the temporary file be stored?,"[""the /tmp directory."", ""Amazon EFS."", ""Amazon EBS."", ""Amazon S3.""]","[""the /tmp directory.""]","```json
{
    ""the /tmp directory."": ""The /tmp directory is the correct choice because AWS Lambda provides a temporary file system that is available at /tmp. This storage is ephemeral and can be used to store temporary files up to 512 MB in size during the execution of a Lambda function. Since the files are less than 10 MB and only needed during execution, /tmp is ideal. This aligns with AWS best practices for handling temporary data in Lambda functions."",
    ""Amazon EFS."": ""Amazon EFS is incorrect because it is a scalable file storage service that can be mounted to Lambda functions, but it is designed for persistent storage rather than temporary storage. It would be overkill for temporary files that do not need to be saved after execution. Additionally, using EFS would introduce unnecessary complexity and cost for this use case."",
    ""Amazon EBS."": ""Amazon EBS is incorrect because it provides block storage for use with Amazon EC2 instances, not AWS Lambda. It is not directly accessible from Lambda functions and is designed for persistent storage, not temporary storage. Using EBS for temporary files in a Lambda function is not feasible."",
    ""Amazon S3."": ""Amazon S3 is incorrect because it is an object storage service designed for persistent storage of data. While it can store files, it is not suitable for temporary files that need to be accessed and modified multiple times during a Lambda execution. Additionally, S3 operations introduce latency and cost that are unnecessary for this use case.""
}
```"
A website's page load times are gradually increasing as more users access the system at the same time. Analysis indicates that a user profile is being loaded from a database in all the web pages being visited by each user and this is increasing the database load and the page load latency. To address this issue the Developer decides to cache the user profile data. Which caching strategy will address this situation MOST efficiently?,"[""Create a new Amazon EC2 Instance and run a NoSQL database on it. Cache the profile data within this database using the write-through caching strategy."", ""Create an Amazon ElastiCache cluster to cache the user profile data. Use a cache-aside caching strategy."", ""Use a dedicated Amazon RDS instance for caching profile data. Use a write-through caching strategy."", ""Create an ElastiCache cluster to cache the user profile data. Use a write-through caching strategy.""]","[""Create an Amazon ElastiCache cluster to cache the user profile data. Use a cache-aside caching strategy.""]","```json
{
    ""Create a new Amazon EC2 Instance and run a NoSQL database on it. Cache the profile data within this database using the write-through caching strategy."": ""This option is incorrect because using an EC2 instance to run a NoSQL database for caching is not the most efficient or scalable solution. It requires managing the infrastructure, which adds complexity and overhead. Additionally, a write-through caching strategy is less efficient for read-heavy workloads like user profile data, as it involves writing to the cache every time data is updated, regardless of whether it will be read soon."",
    ""Create an Amazon ElastiCache cluster to cache the user profile data. Use a cache-aside caching strategy."": ""This option is correct because Amazon ElastiCache is a fully managed in-memory data store service that is optimized for caching. It provides high performance and scalability, which is ideal for reducing database load and improving page load times. The cache-aside strategy is suitable for this scenario as it allows the application to load data into the cache only when needed, reducing unnecessary writes and optimizing read performance, which aligns with best practices for caching frequently accessed data."",
    ""Use a dedicated Amazon RDS instance for caching profile data. Use a write-through caching strategy."": ""This option is incorrect because Amazon RDS is a relational database service that is not optimized for caching purposes. Using RDS for caching would be inefficient and costly compared to using an in-memory data store like ElastiCache. Additionally, the write-through strategy is not ideal for caching scenarios where read performance is critical, as it involves writing every update to the cache, which can increase latency."",
    ""Create an ElastiCache cluster to cache the user profile data. Use a write-through caching strategy."": ""This option is incorrect because, although ElastiCache is the right service for caching, the write-through caching strategy is not the most efficient for this use case. Write-through caching involves writing data to the cache every time it is updated, which can lead to unnecessary cache writes and increased latency, especially in read-heavy scenarios like user profile data access. Cache-aside is more suitable as it loads data into the cache only when needed.""
}
```"
An advertising company has a dynamic website with heavy traffic. The company wants to migrate the website infrastructure to AWS to handle everything except website development. Which solution BEST meets these requirements?,"[""Use AWS VM Import to migrate a web server image to AWS Launch the image on a compute-optimized Amazon EC2 instance."", ""Launch multiple Amazon Lightsail instance behind a load balancer. Set up the website on those instances."", ""Deploy the website code in an AWS Elastic Beanstalk environment. Use Auto Scaling to scale the numbers of instance."", ""Use Amazon S3 to host the website. Use Amazon CloudFornt to deliver the content at scale.""]","[""Deploy the website code in an AWS Elastic Beanstalk environment. Use Auto Scaling to scale the numbers of instance.""]","```json
{
    ""Use AWS VM Import to migrate a web server image to AWS Launch the image on a compute-optimized Amazon EC2 instance."": ""This option is incorrect because while VM Import/Export can be used to migrate virtual machines to AWS, it does not inherently provide scalability or manageability for a dynamic website with heavy traffic. Compute-optimized EC2 instances are suitable for compute-intensive applications but do not automatically handle scaling or load balancing, which are essential for a high-traffic website."",
    ""Launch multiple Amazon Lightsail instance behind a load balancer. Set up the website on those instances."": ""This option is incorrect because while Amazon Lightsail is a simple and cost-effective option for small applications, it lacks the advanced scaling and management features needed for a dynamic website with heavy traffic. Lightsail is more suited for simpler applications and does not provide the same level of integration and scalability as Elastic Beanstalk or EC2 with Auto Scaling."",
    ""Deploy the website code in an AWS Elastic Beanstalk environment. Use Auto Scaling to scale the numbers of instance."": ""This option is correct because AWS Elastic Beanstalk provides an easy way to deploy and manage applications in the cloud. It automatically handles the deployment, from capacity provisioning, load balancing, and auto-scaling to application health monitoring. Elastic Beanstalk is ideal for dynamic websites with heavy traffic as it can automatically scale the number of instances based on demand, ensuring high availability and performance."",
    ""Use Amazon S3 to host the website. Use Amazon CloudFornt to deliver the content at scale."": ""This option is incorrect because while Amazon S3 and CloudFront are excellent for hosting static websites and delivering content globally with low latency, they are not suitable for hosting dynamic websites that require server-side processing. S3 is designed for static content, and using it for a dynamic website would not meet the requirements of handling server-side logic and dynamic content generation.""
}
```"
A developer is writing an AWS Lambda function. The developer wants to log key events that occur during the Lambda function and include a unique identifier to associate the events with a specific function invocation. Which of the following will help the developer accomplish this objective?,"[""Obtain the request identifier from the Lambda context object. Architect the application to write logs to the console."", ""Obtain the request identifier from the Lambda event object. Architect the application to write logs to a file."", ""Obtain the request identifier from the Lambda event object. Architect the application to write logs to the console."", ""Obtain the request identifier from the Lambda context object. Architect the application to write logs to a file.""]","[""Obtain the request identifier from the Lambda context object. Architect the application to write logs to the console.""]","```json
{
    ""Obtain the request identifier from the Lambda context object. Architect the application to write logs to the console."": ""This choice is correct because the Lambda context object provides a unique request identifier (AWS Request ID) for each invocation, which can be used to track and associate logs with specific invocations. Writing logs to the console is a best practice in AWS Lambda as it automatically integrates with Amazon CloudWatch Logs, making it easy to monitor and analyze logs without additional setup."",
    ""Obtain the request identifier from the Lambda event object. Architect the application to write logs to a file."": ""This choice is incorrect because the Lambda event object does not contain a request identifier. The event object contains data passed to the function, such as event source information, but not the AWS Request ID. Additionally, writing logs to a file is not a recommended practice in AWS Lambda, as Lambda functions are stateless and ephemeral, meaning any file written to the local filesystem will not persist beyond the function's execution."",
    ""Obtain the request identifier from the Lambda event object. Architect the application to write logs to the console."": ""This choice is incorrect because the Lambda event object does not provide a request identifier. While writing logs to the console is correct for integration with CloudWatch Logs, the lack of a request identifier from the event object makes it impossible to uniquely associate logs with specific invocations."",
    ""Obtain the request identifier from the Lambda context object. Architect the application to write logs to a file."": ""This choice is incorrect because, although the Lambda context object correctly provides the request identifier, writing logs to a file is not suitable for AWS Lambda functions. Lambda functions are designed to be stateless, and any files written to the local filesystem are lost after the function execution completes. Logging to the console is the recommended approach for capturing logs in CloudWatch.""
}
```"
"A company stores all personally identifiable information (PII) in an Amazon DynamoDB table named PII in Account A. An application running on Amazon EC2 instances in Account B requires access to the PII table. An administrators in Account A created an IAM role named AccessPII with privileges to access the PII table, and made account B a trusted entity. Which combination of actional steps should Developers take to access the table? (Select TWO)","[""Allow the EC2 IAM role the permission to assume the AccessPII role."", ""Allow the EC2 IAM role the permission to access the PII table."", ""Include the AWS API in the application code logic to obtain temporary credentials from the EC2 IAM role to access the PII table."", ""Include the AssumeRole API operation in the application code logic to obtain temporary credentials to access the PII table."", ""Include the GetSessionToken API operation in the application code logic to obtain temporary credentials to access the PII table.""]","[""Allow the EC2 IAM role the permission to assume the AccessPII role."", ""Include the AssumeRole API operation in the application code logic to obtain temporary credentials to access the PII table.""]","```json
{
    ""Allow the EC2 IAM role the permission to assume the AccessPII role."": ""This choice is correct because in order for the EC2 instances in Account B to access the PII table in Account A, they must assume the AccessPII role. This requires the EC2 IAM role to have the necessary permissions to call the AssumeRole API, which allows it to assume the AccessPII role and gain the permissions associated with it. This is a common cross-account access pattern in AWS."",
    ""Allow the EC2 IAM role the permission to access the PII table."": ""This choice is incorrect because the EC2 IAM role in Account B should not directly have permissions to access the PII table in Account A. Instead, it should assume the AccessPII role in Account A, which has the necessary permissions. Direct access would violate the principle of least privilege and cross-account access best practices."",
    ""Include the AWS API in the application code logic to obtain temporary credentials from the EC2 IAM role to access the PII table."": ""This choice is incorrect because while the application running on EC2 will need to obtain temporary credentials, it should do so by assuming the AccessPII role, not directly from the EC2 IAM role. The EC2 role's purpose is to allow the application to assume the AccessPII role, which then provides the necessary permissions."",
    ""Include the AssumeRole API operation in the application code logic to obtain temporary credentials to access the PII table."": ""This choice is correct because the AssumeRole API operation is used to obtain temporary security credentials that allow the application to assume the AccessPII role. This is necessary for cross-account access to the PII table, as it provides the application with the permissions defined in the AccessPII role."",
    ""Include the GetSessionToken API operation in the application code logic to obtain temporary credentials to access the PII table."": ""This choice is incorrect because the GetSessionToken API is used to obtain temporary credentials for the current AWS account, not for assuming a role in another account. In this scenario, the application needs to assume a role in a different account (Account A), which requires the AssumeRole API operation.""
}
```"
An AWS Lambda function accesses two Amazon DynamoDB tables. A developer wants to improve the performance of the Lambda function by identifying bottlenecks in the function. How can the developer inspect the timing of the DynamoDB API calls?,"[""Add DynamoDB as an event source to the Lambda function. View the performance with Amazon CloudWatch metrics."", ""Place an Application Load Balancer (ALB) in front of the two DynamoDB tables. Inspect the ALB logs."", ""Limit Lambda to no more than five concurrent invocations Monitor from the Lambda console."", ""Enable AWS X-Ray tracing for the function. View the traces from the X-Ray service.""]","[""Enable AWS X-Ray tracing for the function. View the traces from the X-Ray service.""]","```json
{
    ""Add DynamoDB as an event source to the Lambda function. View the performance with Amazon CloudWatch metrics."": ""This choice is incorrect because adding DynamoDB as an event source is used for triggering Lambda functions based on changes in the DynamoDB table, not for inspecting the timing of API calls. CloudWatch metrics can provide some performance data, but they do not offer detailed tracing of API call timings like AWS X-Ray does."",
    ""Place an Application Load Balancer (ALB) in front of the two DynamoDB tables. Inspect the ALB logs."": ""This choice is incorrect because an Application Load Balancer is used for distributing incoming application or network traffic across multiple targets, such as EC2 instances, not for DynamoDB tables. ALB logs provide information about HTTP requests, not DynamoDB API call timings."",
    ""Limit Lambda to no more than five concurrent invocations Monitor from the Lambda console."": ""This choice is incorrect because limiting the concurrency of Lambda functions does not provide insights into the timing of DynamoDB API calls. Monitoring from the Lambda console can show invocation metrics but does not provide detailed tracing information."",
    ""Enable AWS X-Ray tracing for the function. View the traces from the X-Ray service."": ""This choice is correct because AWS X-Ray is designed to trace requests as they travel through your application, including API calls to DynamoDB. It provides detailed insights into the performance of the Lambda function, including the timing of DynamoDB API calls, which helps identify bottlenecks. This is a best practice for diagnosing performance issues in distributed applications.""
}
```"
"An Amazon RDS database instance is used by many applications to look up historical data. The query rate is relatively constant. When the historical data is updated each day, the resulting write traffic slows the read query performance and affects all application users. What can be done to eliminate the performance impact on application users?","[""Make sure Amazon RDS is Multi-AZ so it can better absorb increased traffic."", ""Create an RDS Read Replica and direct all read traffic to the replica."", ""Implement Amazon ElastiCache in front of Amazon RDS to buffer the write traffic."", ""Use Amazon DynamoDB instead of Amazon RDS to buffer the read traffic.""]","[""Create an RDS Read Replica and direct all read traffic to the replica.""]","```json
{
    ""Make sure Amazon RDS is Multi-AZ so it can better absorb increased traffic."": ""This choice is incorrect because Multi-AZ deployments are designed for high availability and failover support, not for improving read performance. Multi-AZ keeps a standby instance in another Availability Zone to ensure database availability in case of an outage, but it does not provide additional read capacity. Therefore, it does not address the issue of read performance being affected by write traffic."",
    ""Create an RDS Read Replica and direct all read traffic to the replica."": ""This choice is correct because RDS Read Replicas are specifically designed to offload read traffic from the primary database instance. By directing read queries to the Read Replica, you can reduce the load on the primary instance, which is handling write operations, thereby improving read performance. This is a common best practice for scaling read-heavy applications and is supported by AWS documentation."",
    ""Implement Amazon ElastiCache in front of Amazon RDS to buffer the write traffic."": ""This choice is incorrect because Amazon ElastiCache is primarily used to cache frequently accessed data to improve read performance, not to buffer write traffic. While ElastiCache can reduce the load on a database by caching read queries, it does not address the issue of write traffic impacting read performance. The problem described in the question is related to write operations affecting read performance, which ElastiCache does not solve."",
    ""Use Amazon DynamoDB instead of Amazon RDS to buffer the read traffic."": ""This choice is incorrect because Amazon DynamoDB is a NoSQL database service that is not used to buffer read traffic for an RDS instance. Switching to DynamoDB would require significant changes to the application architecture and data model, which is not a direct solution to the problem of write traffic affecting read performance in an RDS instance. Additionally, DynamoDB does not buffer read traffic for RDS.""
}
```"
"A company is developing a serverless ecommerce web application. The application needs to make coordinated, all-or-nothing changes to multiple items in the company's inventory table in Amazon DynamoDB. Which solution will meet these requirements?","[""Enable transactions for the DynamoDB table. Use the BatchWriteltem operation to update the items."", ""Use the TransactWriteitems operation to group the changes. Update the items in the table."", ""Set up a FIFO queue using Amazon SQS. Group the changes in the queue. Update the table based on the grouped changes."", ""Create a transaction table in an Amazon Aurora DB cluster to manage the transactions. Write a backend process to sync the Aurora DB table and the DynamoDB table.""]","[""Use the TransactWriteitems operation to group the changes. Update the items in the table.""]","```json
{
    ""Enable transactions for the DynamoDB table. Use the BatchWriteltem operation to update the items."": ""This choice is incorrect because the BatchWriteItem operation in DynamoDB does not support transactions. It is used for batch writes but does not guarantee all-or-nothing semantics. If a batch write partially fails, it does not roll back the changes, which is not suitable for transactional requirements."",
    ""Use the TransactWriteitems operation to group the changes. Update the items in the table."": ""This choice is correct because the TransactWriteItems operation in DynamoDB allows you to perform coordinated, all-or-nothing changes to multiple items in one or more tables. It ensures that either all the operations succeed or none of them are applied, which is essential for transactional integrity in a serverless application."",
    ""Set up a FIFO queue using Amazon SQS. Group the changes in the queue. Update the table based on the grouped changes."": ""This choice is incorrect because while a FIFO queue can ensure the order of operations, it does not provide transactional capabilities. Using SQS alone does not guarantee that all grouped changes will be applied atomically to the DynamoDB table."",
    ""Create a transaction table in an Amazon Aurora DB cluster to manage the transactions. Write a backend process to sync the Aurora DB table and the DynamoDB table."": ""This choice is incorrect because it introduces unnecessary complexity and overhead. While Amazon Aurora supports transactions, using it to manage transactions for DynamoDB data requires additional synchronization logic and does not leverage DynamoDB's native transactional capabilities, making it an inefficient solution.""
}
```"
An application is running on an EC2 instance. The Developer wants to store an application metric in Amazon CloudWatch. What is the best practice for implementing this requirement?,"[""Use the PUT Object API call to send data to an S3 bucket. Use an event notification to invoke a Lambda function to publish data to CloudWatch."", ""Publish the metric data to an Amazon Kinesis Stream using a PutRecord API call. Subscribe a Lambda function that publishes data to CloudWatch."", ""Use the CloudWatch PutMetricData API call to submit a custom metric to CloudWatch. Provide the required credentials to enable the API call."", ""Use the CloudWatch PutMetricData API call to submit a custom metric to CloudWatch. Launch the EC2 instance with the required IAM role to enable the API call.""]","[""Use the CloudWatch PutMetricData API call to submit a custom metric to CloudWatch. Launch the EC2 instance with the required IAM role to enable the API call.""]","```json
{
    ""Use the PUT Object API call to send data to an S3 bucket. Use an event notification to invoke a Lambda function to publish data to CloudWatch."": ""This approach is incorrect because it introduces unnecessary complexity and latency. The best practice for submitting custom metrics to CloudWatch is to use the CloudWatch PutMetricData API directly. Using S3 and Lambda adds additional components that are not needed for this task, which can increase costs and potential points of failure."",
    ""Publish the metric data to an Amazon Kinesis Stream using a PutRecord API call. Subscribe a Lambda function that publishes data to CloudWatch."": ""This choice is incorrect because it involves unnecessary components for the task of publishing metrics to CloudWatch. Kinesis Streams are designed for real-time data streaming and processing, which is not required for simply sending metrics to CloudWatch. This approach adds complexity and cost without providing any benefit for the task at hand."",
    ""Use the CloudWatch PutMetricData API call to submit a custom metric to CloudWatch. Provide the required credentials to enable the API call."": ""This option is partially correct but not the best practice. While using the PutMetricData API is correct, providing credentials directly is not recommended due to security risks. Instead, using an IAM role with the necessary permissions is the best practice for securely granting access."",
    ""Use the CloudWatch PutMetricData API call to submit a custom metric to CloudWatch. Launch the EC2 instance with the required IAM role to enable the API call."": ""This is the correct answer. Using the CloudWatch PutMetricData API is the recommended way to submit custom metrics. Launching the EC2 instance with an IAM role that has the necessary permissions to call PutMetricData is a best practice, as it securely manages access without embedding credentials in the application code. This approach follows AWS security best practices by using IAM roles for EC2 instances.""
}
```"
A Developer needs to design an application running on AWS that will be used to consume Amazon SQS messages that range from 1 KB up to 1GB in size. How should the Amazon SQS messages be managed?,"[""Use Amazon S3 and the Amazon SQS CLI."", ""Use Amazon S3 and the Amazon SQS Extended Client Library for Java."", ""Use Amazon EBS and the Amazon SQS CLI."", ""Use Amazon EFS and the Amazon SQS CLI.""]","[""Use Amazon S3 and the Amazon SQS Extended Client Library for Java.""]","```json
{
    ""Use Amazon S3 and the Amazon SQS CLI."": ""This choice is incorrect because the Amazon SQS CLI does not support handling messages larger than 256 KB directly. For messages larger than this limit, the Amazon SQS Extended Client Library for Java is required to store the payloads in Amazon S3 and reference them in SQS messages."",
    ""Use Amazon S3 and the Amazon SQS Extended Client Library for Java."": ""This choice is correct because Amazon SQS has a maximum message size of 256 KB. For messages larger than this, the Amazon SQS Extended Client Library for Java can be used to store the message payloads in Amazon S3 and send a reference to the S3 object in the SQS message. This approach allows handling of messages up to 2 GB, which covers the 1 GB size mentioned in the question."",
    ""Use Amazon EBS and the Amazon SQS CLI."": ""This choice is incorrect because Amazon EBS is a block storage service used for EC2 instances and is not suitable for storing SQS message payloads. Moreover, the Amazon SQS CLI does not handle messages larger than 256 KB, and EBS does not integrate with SQS for this purpose."",
    ""Use Amazon EFS and the Amazon SQS CLI."": ""This choice is incorrect because Amazon EFS is a file storage service for use with Amazon EC2 and is not designed for storing SQS message payloads. Additionally, the Amazon SQS CLI cannot manage messages larger than 256 KB, and EFS does not provide direct integration with SQS for handling large messages.""
}
```"
A developer has written a multi-threaded application that is running on a fleet of Amazon EC2 instances. The operations team has requested a graphical method to monitor the number of running threads over time. What is the MOST efficient way to fulfill this request?,"[""Periodically send the thread count to AWS X-Ray segments, then generate a service graph on demand."", ""Create a custom Amazon CloudWatch metric and periodically perform a PutMetricData call with the current thread count."", ""Periodically log thread count data to Amazon S3. Use Amazon Kinesis to process the data into a graph."", ""Periodically write the current thread count to a table using Amazon DynarnoDB and use Amazon CloudFront to create a graph.""]","[""Create a custom Amazon CloudWatch metric and periodically perform a PutMetricData call with the current thread count.""]","```json
{
    ""Periodically send the thread count to AWS X-Ray segments, then generate a service graph on demand."": ""This choice is incorrect because AWS X-Ray is primarily used for tracing requests and analyzing the performance of applications, not for monitoring metrics like thread count. X-Ray service graphs are designed to show the flow of requests through an application, not to provide time-series data on metrics such as thread counts."",
    ""Create a custom Amazon CloudWatch metric and periodically perform a PutMetricData call with the current thread count."": ""This choice is correct because Amazon CloudWatch is the AWS service specifically designed for monitoring and logging metrics. By creating a custom CloudWatch metric, the developer can efficiently track the thread count over time and use CloudWatch's built-in graphing capabilities to visualize the data. This approach aligns with AWS best practices for monitoring custom application metrics."",
    ""Periodically log thread count data to Amazon S3. Use Amazon Kinesis to process the data into a graph."": ""This choice is incorrect because while Amazon S3 and Amazon Kinesis can be used for data storage and processing, they are not the most efficient tools for real-time monitoring and graphing of metrics. This approach would require additional infrastructure and complexity to achieve what CloudWatch can do natively with custom metrics."",
    ""Periodically write the current thread count to a table using Amazon DynarnoDB and use Amazon CloudFront to create a graph."": ""This choice is incorrect because Amazon DynamoDB is a NoSQL database service and not intended for time-series data visualization. Additionally, Amazon CloudFront is a content delivery network and does not have capabilities to create graphs. This approach would be inefficient and overly complex for the task of monitoring thread counts.""
}
```"
The Lambda function below is being called through an API using Amazon API Gateway. The average execution time for the Lambda function is about 1 second. The pseudocode for the Lambda function is as shown in the exhibit. What two actions can be taken to improve the performance of this Lambda function without increasing the cost of the solution? (Select TWO),"[""Package only the modules the Lambda function requires."", ""Use Amazon DynamoDB instead of Amazon RDS."", ""Move the initialization of the variable Amazon RDS connection outside of the handler function."", ""Implement custom database connection pooling with the Lambda function."", ""Implement local caching of Amazon RDS data so Lambda can re-use the cache.""]","[""Package only the modules the Lambda function requires."", ""Move the initialization of the variable Amazon RDS connection outside of the handler function.""]","```json
{
    ""Package only the modules the Lambda function requires."": ""This choice is correct because minimizing the package size of a Lambda function can reduce the cold start time. Lambda functions need to load their code into memory before execution, and larger packages take longer to load. By packaging only the necessary modules, you can reduce the deployment package size, which can improve performance without increasing costs. This is a best practice for optimizing Lambda performance."",
    ""Use Amazon DynamoDB instead of Amazon RDS."": ""This choice is incorrect because switching from Amazon RDS to DynamoDB is not guaranteed to improve performance without considering the specific use case. While DynamoDB is a highly scalable NoSQL database, it may not be suitable for all workloads, especially if the application relies on relational database features. Additionally, changing the database service could involve significant refactoring and might not align with the application's requirements."",
    ""Move the initialization of the variable Amazon RDS connection outside of the handler function."": ""This choice is correct because initializing the database connection outside of the handler function allows the connection to be reused across multiple invocations of the Lambda function, reducing the overhead of establishing a new connection each time. This can significantly improve performance, especially in a high-throughput scenario, without increasing costs. This practice is known as connection reuse and is recommended for optimizing Lambda functions that interact with databases."",
    ""Implement custom database connection pooling with the Lambda function."": ""This choice is incorrect because AWS Lambda does not maintain state between invocations, which makes traditional connection pooling ineffective. Instead, reusing connections by initializing them outside the handler is the recommended approach. Custom connection pooling would not provide the intended benefits in a Lambda environment and could add unnecessary complexity."",
    ""Implement local caching of Amazon RDS data so Lambda can re-use the cache."": ""This choice is incorrect because Lambda functions do not maintain state between invocations, meaning local caching would not persist across function executions. While caching can improve performance, it would need to be implemented using an external service like Amazon ElastiCache to be effective across multiple invocations, which could increase costs.""
}
```"
"An application on AWS is using third-party APIs. The Developer needs to monitor API errors in the code, and wants to receive notifications if failures go above a set threshold value. How can the Developer achieve these requirements?","[""Publish a custom metric on Amazon CloudWatch and use Amazon Simple Email Service (SES) for notification."", ""Use an Amazon CloudWatch API-error metric and use Amazon Simple Notification Service (SNS) for notification."", ""Use an Amazon CloudWatch API-error metric and use Amazon SES for notification."", ""Publish a custom metric on Amazon CloudWatch and use Amazon SNS for notification.""]","[""Publish a custom metric on Amazon CloudWatch and use Amazon SNS for notification.""]","```json
{
    ""Publish a custom metric on Amazon CloudWatch and use Amazon Simple Email Service (SES) for notification."": ""This choice is incorrect because while you can publish a custom metric to CloudWatch, using Amazon SES for notifications is not the standard practice. Amazon SES is primarily used for sending emails, not for triggering notifications based on CloudWatch alarms. The standard practice is to use Amazon SNS for notifications, which can then trigger email alerts."",
    ""Use an Amazon CloudWatch API-error metric and use Amazon Simple Notification Service (SNS) for notification."": ""This choice is incorrect because there is no built-in CloudWatch API-error metric that automatically tracks third-party API errors. You would need to publish a custom metric to CloudWatch to track these errors. However, using Amazon SNS for notifications is correct, as it is designed to send notifications when CloudWatch alarms are triggered."",
    ""Use an Amazon CloudWatch API-error metric and use Amazon SES for notification."": ""This choice is incorrect for two reasons. First, there is no existing CloudWatch API-error metric for third-party APIs; you must publish a custom metric. Second, using Amazon SES for notifications is not the standard practice; Amazon SNS is typically used for this purpose."",
    ""Publish a custom metric on Amazon CloudWatch and use Amazon SNS for notification."": ""This choice is correct. Since there is no built-in CloudWatch metric for third-party API errors, the developer must publish a custom metric to track these errors. By setting up a CloudWatch alarm on this custom metric, the developer can use Amazon SNS to send notifications when the error rate exceeds a specified threshold. This approach aligns with AWS best practices for monitoring and alerting.""
}
```"
The release process workflow of an application requires a manual approval before the code is deployed into the production environment. What is the BEST way to achieve this using AWS CodePipeline?,"[""Use multiple pipelines to allow approval."", ""Use an approval action in a stage."", ""Disable the stage transition to allow manual approval."", ""Disable a stage just prior the deployment stage.""]","[""Use an approval action in a stage.""]","{""Use multiple pipelines to allow approval."": ""This choice is incorrect because using multiple pipelines to allow approval is not an efficient or standard practice in AWS CodePipeline. CodePipeline provides a built-in feature called 'approval actions' that can be used within a single pipeline to pause the pipeline execution and wait for manual approval. Using multiple pipelines would unnecessarily complicate the workflow and is not aligned with AWS best practices."", ""Use an approval action in a stage."": ""This choice is correct because AWS CodePipeline supports manual approval actions as a native feature. An approval action can be added to a stage in the pipeline, which pauses the execution of the pipeline and waits for a manual approval before proceeding to the next stage. This is the best practice for implementing manual approvals in a CI/CD pipeline, as it is straightforward and leverages AWS CodePipeline's built-in capabilities. AWS documentation recommends using approval actions for scenarios where manual intervention is required before proceeding to production."", ""Disable the stage transition to allow manual approval."": ""This choice is incorrect because disabling a stage transition is not intended for manual approvals. Disabling a stage transition would stop the pipeline execution at a specific point, but it does not provide a mechanism for manual approval or resumption of the pipeline. This approach does not align with AWS CodePipeline's intended use for manual approvals."", ""Disable a stage just prior the deployment stage."": ""This choice is incorrect because disabling a stage is not a method for implementing manual approvals. Disabling a stage would prevent the pipeline from executing that stage entirely, rather than pausing for approval. This would disrupt the pipeline's workflow and is not a recommended practice for managing manual approvals in AWS CodePipeline.""}"
A Developer is asked to implement a caching layer in front of Amazon RDS. Cached content is expensive to regenerate in case of service failure. Which implementation below would work while maintaining maximum uptime?,"[""Implement Amazon ElastiCache Redis in Cluster Mode."", ""Install Redis on an Amazon EC2 instance."", ""Implement Amazon ElastiCache Memcached."", ""Migrate the database to Amazon Redshift.""]","[""Implement Amazon ElastiCache Redis in Cluster Mode.""]","{""Implement Amazon ElastiCache Redis in Cluster Mode."": ""This choice is correct because Amazon ElastiCache Redis in Cluster Mode provides high availability and scalability, which are crucial for maintaining maximum uptime. Redis in Cluster Mode allows for data partitioning and replication, ensuring that the cache remains available even if some nodes fail. This setup is ideal for scenarios where cached content is expensive to regenerate, as it minimizes downtime and data loss. According to AWS best practices, using managed services like ElastiCache for Redis ensures that the infrastructure is automatically managed and monitored, reducing the risk of service failure. AWS documentation supports this choice as ElastiCache Redis is designed for high availability and fault tolerance."", ""Install Redis on an Amazon EC2 instance."": ""This choice is incorrect because installing Redis on an Amazon EC2 instance requires manual management of the infrastructure, which can lead to increased downtime and maintenance overhead. Unlike ElastiCache, which is a managed service, running Redis on EC2 does not provide built-in high availability or automatic failover, making it less reliable for maintaining maximum uptime. This approach also lacks the scalability and ease of management provided by ElastiCache Redis in Cluster Mode. AWS best practices recommend using managed services to reduce operational complexity and improve reliability."", ""Implement Amazon ElastiCache Memcached."": ""This choice is incorrect because while Amazon ElastiCache Memcached is a viable caching solution, it does not offer the same level of data durability and high availability as Redis. Memcached does not support data replication or persistence, which means that in the event of a node failure, cached data could be lost, leading to potential downtime and the need to regenerate expensive content. Redis, on the other hand, offers features like data persistence and replication, making it more suitable for scenarios where uptime and data durability are critical."", ""Migrate the database to Amazon Redshift."": ""This choice is incorrect because Amazon Redshift is a data warehousing service, not a caching solution. Migrating the database to Redshift would not address the requirement for a caching layer in front of Amazon RDS. Redshift is designed for complex analytical queries and large-scale data processing, not for caching frequently accessed data to reduce load on a database. This choice reflects a misunderstanding of the purpose of caching and the specific use case for which a caching layer is needed. AWS best practices emphasize using the right tool for the job, and in this case, a caching service like ElastiCache is more appropriate.""}"
"A company has written a Java AWS Lambda function to be triggered whenever a user uploads an image to an Amazon S3 bucket. The function converts the original image to several different formats and then copies the resulting images to another Amazon S3 bucket. The Developers find that no images are being copied to the second Amazon S3 bucket. They have tested the code on an Amazon EC2 instance with 1GB of RAM, and it takes an average of 500 seconds to complete. What is the MOST likely cause of the problem?","[""The Lambda function has insufficient memory and needs to be increased to 1 GB to match the Amazon EC2 instance."", ""Files need to be copied to the same Amazon S3 bucket for processing, so the second bucket needs to be deleted."", ""Lambda functions have a maximum execution limit of 15 minutes, therefore the function is not completing."", ""There is a problem with the Java runtime for Lambda, and the function needs to be converted to node.js.""]","[""Lambda functions have a maximum execution limit of 15 minutes, therefore the function is not completing.""]","```json
{
    ""The Lambda function has insufficient memory and needs to be increased to 1 GB to match the Amazon EC2 instance."": ""This choice is incorrect because the problem is not related to memory allocation but to the execution time limit of AWS Lambda. AWS Lambda functions have a maximum execution time of 15 minutes (900 seconds), and since the function takes an average of 500 seconds, it is not a memory issue but a time constraint issue. Increasing memory would not resolve the execution time limit problem."",
    ""Files need to be copied to the same Amazon S3 bucket for processing, so the second bucket needs to be deleted."": ""This choice is incorrect because there is no requirement for files to be copied to the same S3 bucket for processing. AWS Lambda can interact with multiple S3 buckets, and the issue described is not related to bucket configuration but to the execution time limit of the Lambda function."",
    ""Lambda functions have a maximum execution limit of 15 minutes, therefore the function is not completing."": ""This choice is correct because AWS Lambda functions have a maximum execution time of 15 minutes (900 seconds). Since the function takes an average of 500 seconds to complete, it is likely exceeding the time limit, causing it to terminate before completion. This is a common issue when migrating processes from environments like EC2 to Lambda, where execution time limits must be considered."",
    ""There is a problem with the Java runtime for Lambda, and the function needs to be converted to node.js."": ""This choice is incorrect because there is no inherent problem with the Java runtime for AWS Lambda. AWS supports multiple runtimes, including Java and Node.js, and the issue described is related to the execution time limit, not the runtime environment. Converting the function to Node.js would not address the execution time limit issue.""
}
```"
A web application is using Amazon Kinesis Streams for clickstream data that may not be consumed for up to 12 hours. How can the Developer implement encryption at rest for data within the Kinesis Streams?,"[""Enable SSL connections to Kinesis."", ""Use Amazon Kinesis Consumer Library."", ""Encrypt the data once it is at rest with a Lambda function."", ""Enable server-side encryption in Kinesis Streams.""]","[""Enable server-side encryption in Kinesis Streams.""]","```json
{
    ""Enable SSL connections to Kinesis."": ""This choice is incorrect because SSL connections ensure data is encrypted in transit, not at rest. The question specifically asks about encryption at rest, which SSL does not address. SSL/TLS is used to secure data as it moves between the client and the Kinesis service, but it does not provide encryption for data stored within Kinesis Streams."",
    ""Use Amazon Kinesis Consumer Library."": ""This choice is incorrect because the Amazon Kinesis Consumer Library (KCL) is used for building consumer applications to process data from Kinesis Streams. It does not provide encryption capabilities. KCL helps in managing the complexity of distributed stream processing but does not address encryption at rest."",
    ""Encrypt the data once it is at rest with a Lambda function."": ""This choice is incorrect because while it is technically possible to use AWS Lambda to process and encrypt data, it is not the recommended or most efficient method for encrypting data at rest in Kinesis Streams. Server-side encryption is a built-in feature of Kinesis Streams that simplifies the process and is more efficient."",
    ""Enable server-side encryption in Kinesis Streams."": ""This choice is correct because enabling server-side encryption in Kinesis Streams is the recommended way to encrypt data at rest. AWS Kinesis Streams provides server-side encryption using AWS Key Management Service (KMS) to encrypt data stored in the stream. This method is efficient, secure, and aligns with AWS best practices for data protection. Enabling server-side encryption is a straightforward process and ensures that data is automatically encrypted as it is written to the stream and decrypted when accessed by authorized users.""
}
```"
A Developer is creating a mobile application with a limited budget. The solution requires a scalable service that will enable customers to sign up and authenticate into the mobile application while using the organization's current SAML 2.0 identity provider. Which AWS service should be used to meet these requirements?,"[""AWS Lambda."", ""Amazon Cognito."", ""AWS IAM."", ""Amazon EC2.""]","[""Amazon Cognito.""]","```json
{
    ""AWS Lambda."": ""AWS Lambda is a serverless compute service that allows you to run code without provisioning or managing servers. While it is excellent for running backend code in response to events, it is not designed for user authentication or integration with SAML 2.0 identity providers. Therefore, it is not suitable for managing user sign-up and authentication in a mobile application."",
    ""Amazon Cognito."": ""Amazon Cognito is the correct choice because it is specifically designed for user authentication and management in web and mobile applications. It supports integration with SAML 2.0 identity providers, allowing users to sign up and authenticate using existing organizational credentials. Cognito provides scalable user pools and identity pools, making it ideal for applications with a limited budget that require scalability and integration with existing identity systems."",
    ""AWS IAM."": ""AWS Identity and Access Management (IAM) is primarily used for managing access to AWS resources for users and services. It is not designed for managing user authentication and sign-up for applications, especially when integrating with SAML 2.0 identity providers. IAM is more suitable for managing permissions and roles within AWS itself, not for application-level user management."",
    ""Amazon EC2."": ""Amazon EC2 provides scalable compute capacity in the cloud, allowing you to run virtual servers. However, it does not provide built-in user authentication or integration with SAML 2.0 identity providers. Using EC2 for this purpose would require significant custom development and management, making it an inefficient and costly choice for user authentication in a mobile application.""
}
```"
"A company wants to migrate its web application to AWS and leverage Auto Scaling to handle peak workloads. The Solutions Architect determined that the best metric for an Auto Scaling event is the number of concurrent users. Based on this information, what should the Developer use to autoscale based on concurrent users?","[""An Amazon SNS topic to be triggered when a concurrent user threshold is met."", ""An Amazon Cloudwatch NetworkIn metric."", ""Amazon CloudFront to leverage AWS Edge Locations."", ""A Custom Amazon CloudWatch metric for concurrent users.""]","[""A Custom Amazon CloudWatch metric for concurrent users.""]","```json
{
    ""An Amazon SNS topic to be triggered when a concurrent user threshold is met."": ""This choice is incorrect because Amazon SNS (Simple Notification Service) is primarily used for sending notifications and messages, not for directly triggering Auto Scaling events based on metrics. While SNS can be used in conjunction with other services to notify when thresholds are met, it does not inherently measure or trigger scaling based on concurrent users."",
    ""An Amazon Cloudwatch NetworkIn metric."": ""This choice is incorrect because the NetworkIn metric measures the number of bytes received on all network interfaces by the instance. While it can give some indication of network traffic, it does not directly correlate to the number of concurrent users. Auto Scaling based on concurrent users requires a metric that directly measures user activity, which NetworkIn does not provide."",
    ""Amazon CloudFront to leverage AWS Edge Locations."": ""This choice is incorrect because Amazon CloudFront is a content delivery network (CDN) that helps deliver content with low latency by caching it at edge locations. While it can improve performance for end-users, it does not provide metrics for Auto Scaling based on concurrent users. CloudFront is not used for scaling decisions directly."",
    ""A Custom Amazon CloudWatch metric for concurrent users."": ""This choice is correct because CloudWatch allows you to create custom metrics that can be used to monitor specific application-level metrics, such as the number of concurrent users. By publishing a custom metric for concurrent users, you can set up Auto Scaling policies that trigger scaling actions based on this specific metric, which aligns with the requirement to scale based on concurrent user load. This approach is supported by AWS best practices for monitoring application-specific metrics.""
}
```"
"A Developer has written a serverless application using multiple AWS services. The business logic is written as a Lambda function which has dependencies on third-party libraries. The Lambda function endpoints will be exposed using Amazon API Gateway. The Lambda function will write the information to Amazon DynamoDB. The Developer is ready to deploy the application but must have the ability to rollback. How can this deployment be automated, based on these requirements?","[""Deploy using Amazon Lambda API operations to create the Lambda function by providing a deployment package."", ""Use an AWS CloudFormation template and use CloudFormation syntax to define the Lambda function resource in the template."", ""Use syntax conforming to the Serverless Application Model in the AWS CloudFormation template to define the Lambda function resource."", ""Create a bash script which uses AWS CLI to package and deploy the application.""]","[""Use syntax conforming to the Serverless Application Model in the AWS CloudFormation template to define the Lambda function resource.""]","```json
{
    ""Deploy using Amazon Lambda API operations to create the Lambda function by providing a deployment package."": ""This choice is incorrect because while you can deploy a Lambda function using the AWS Lambda API operations, it does not inherently provide a rollback mechanism. Rollback capabilities are crucial for maintaining application stability and are better supported by infrastructure as code tools like AWS CloudFormation."",
    ""Use an AWS CloudFormation template and use CloudFormation syntax to define the Lambda function resource in the template."": ""This choice is partially correct but not the best option. While AWS CloudFormation can define and deploy Lambda functions, using the AWS Serverless Application Model (SAM) provides a more streamlined and simplified approach specifically designed for serverless applications. SAM also offers additional features like simplified syntax and built-in best practices for serverless applications."",
    ""Use syntax conforming to the Serverless Application Model in the AWS CloudFormation template to define the Lambda function resource."": ""This choice is correct because AWS SAM is an extension of AWS CloudFormation that provides a simplified way of defining serverless applications. It supports rollback capabilities and integrates well with other AWS services like API Gateway and DynamoDB. SAM simplifies the deployment process and is designed to handle serverless application deployments efficiently."",
    ""Create a bash script which uses AWS CLI to package and deploy the application."": ""This choice is incorrect because while using a bash script with AWS CLI can automate deployment, it lacks the built-in rollback capabilities and infrastructure management features provided by AWS CloudFormation and SAM. Using scripts can lead to more complex and error-prone deployments compared to using SAM or CloudFormation.""
}
```"
A game stores user game data in an Amazon DynamoDB table. Individual users should not have access to other users' game data. How can this be accomplished?,"[""Encrypt the game data with individual user keys."", ""Restrict access to specific items based on certain primary key values."", ""Stage data in SQS queues to inject metadata before accessing DynamoDB."", ""Read records from DynamoDB and discard irrelevant data client-side.""]","[""Restrict access to specific items based on certain primary key values.""]","```json
{
    ""Encrypt the game data with individual user keys."": ""This choice is incorrect because encryption alone does not control access to data. While encryption can protect data at rest and in transit, it does not inherently restrict access to specific users. Access control in DynamoDB is typically managed through IAM policies and fine-grained access control, which allows you to restrict access to specific items based on primary key values."",
    ""Restrict access to specific items based on certain primary key values."": ""This choice is correct because it aligns with AWS best practices for securing data in DynamoDB. By using IAM policies and DynamoDB's fine-grained access control, you can restrict access to specific items based on primary key values, ensuring that users can only access their own data. This method leverages IAM conditions to enforce access control at the item level, which is a recommended approach for multi-tenant applications like games."",
    ""Stage data in SQS queues to inject metadata before accessing DynamoDB."": ""This choice is incorrect because using SQS queues to inject metadata does not address the core issue of access control in DynamoDB. SQS is a messaging service designed for decoupling applications and does not provide mechanisms for controlling access to DynamoDB items. Access control should be implemented directly within DynamoDB using IAM policies and fine-grained access control."",
    ""Read records from DynamoDB and discard irrelevant data client-side."": ""This choice is incorrect because it is inefficient and does not provide secure access control. Reading all records and discarding irrelevant data on the client side can lead to unnecessary data exposure and increased latency. The best practice is to use DynamoDB's fine-grained access control to ensure that only relevant data is accessible to each user, reducing the risk of data leaks and improving performance.""
}
```"
"A Developer is creating a web application that requires authentication, but also needs to support guest access to provide users limited access without having to authenticate. What service can provide support for the application to allow guest access?","[""IAM temporary credentials using AWS STS."", ""Amazon Directory Service."", ""Amazon Cognito with unauthenticated access enabled."", ""IAM with SAML integration""]","[""Amazon Cognito with unauthenticated access enabled.""]","```json
{
    ""IAM temporary credentials using AWS STS."": ""This choice is incorrect because AWS STS (Security Token Service) is primarily used for granting temporary credentials to authenticated users, not for providing guest access. STS is used to allow users to assume roles and gain temporary access to resources, but it requires some form of initial authentication to obtain these credentials. It does not inherently support unauthenticated or guest access."",
    ""Amazon Directory Service."": ""This choice is incorrect because Amazon Directory Service is used for integrating AWS resources with on-premises Active Directory or for setting up a standalone directory in the cloud. It is not designed to provide authentication or guest access for web applications. Directory Service is more suited for enterprise environments needing directory-based authentication and management, not for handling guest access in web applications."",
    ""Amazon Cognito with unauthenticated access enabled."": ""This choice is correct because Amazon Cognito provides a feature called 'unauthenticated identities' that allows developers to grant limited access to users without requiring them to sign in. This is ideal for applications that need to support both authenticated users and guest users. Cognito can manage user identities and provide temporary AWS credentials for accessing AWS resources, making it a comprehensive solution for handling both authenticated and guest access in web applications."",
    ""IAM with SAML integration"": ""This choice is incorrect because IAM with SAML integration is used for federated authentication, allowing users to authenticate using external identity providers that support SAML. While it is useful for integrating with enterprise identity systems, it does not provide a mechanism for guest access. SAML integration is focused on authenticated access, not unauthenticated or guest access.""
}
```"
Given the source code for an AWS Lambda function in the local store.py containing a handler function called get_store and the following AWS CloudFormation template. What should be done to prepare the template so that it can be deployed using the AWS CLI command `aws cloudformation deploy`?,"[""Use AWS CloudFormation compile to base64 encode and embed the source file into a modified CloudFormation template."", ""Use AWS CloudFormation package to upload the source code to an Amazon S3 bucket and produce a modified CloudFormation template."", ""Use AWS Lambda zip to package the source file together with the CloudFormation template and deploy the resulting zip archive."", ""Use AWS Serverless create-package to embed the source file directly into the existing CloudFormation template.""]","[""Use AWS CloudFormation package to upload the source code to an Amazon S3 bucket and produce a modified CloudFormation template.""]","```json
{
    ""Use AWS CloudFormation compile to base64 encode and embed the source file into a modified CloudFormation template."": ""This choice is incorrect because AWS CloudFormation does not have a 'compile' command to base64 encode and embed source files. The correct approach for deploying AWS Lambda functions using CloudFormation is to use the 'package' command, which handles uploading code to S3 and modifying the template accordingly."",
    ""Use AWS CloudFormation package to upload the source code to an Amazon S3 bucket and produce a modified CloudFormation template."": ""This choice is correct because the 'aws cloudformation package' command is specifically designed to upload local artifacts, such as Lambda function code, to an S3 bucket and update the CloudFormation template with the S3 location. This is a best practice for deploying Lambda functions using CloudFormation, as it separates the code from the template and leverages S3 for storage."",
    ""Use AWS Lambda zip to package the source file together with the CloudFormation template and deploy the resulting zip archive."": ""This choice is incorrect because there is no 'aws lambda zip' command. Additionally, packaging the source file and CloudFormation template together in a zip archive is not the recommended method for deploying Lambda functions with CloudFormation. The 'package' command should be used to handle code uploads and template modifications."",
    ""Use AWS Serverless create-package to embed the source file directly into the existing CloudFormation template."": ""This choice is incorrect because there is no 'aws serverless create-package' command. The AWS Serverless Application Model (SAM) uses the 'sam package' command for similar purposes, but it is not directly embedding source files into the CloudFormation template. Instead, it uploads code to S3 and updates the template, similar to 'aws cloudformation package'.""
}
```"
"A Developer has created a large Lambda function, and deployment is failing with the following error: `ClientError: An error occurred (InvalidParameterValueException) when calling the CreateFunction operation: Unzipped size must be smaller than XXXXXXXXX bytes.`, where `XXXXXXXXX` is the current Lambda limit. What can the Developer do to fix this problem?","[""Submit a limit increase request to AWS Support to increase the function to the size needed."", ""Use a compression algorithm that is more efficient than ZIP."", ""Break the function into multiple smaller Lambda functions."", ""`ZIP` the `ZIP` file twice to compress it further.""]","[""Break the function into multiple smaller Lambda functions.""]","```json
{
    ""Submit a limit increase request to AWS Support to increase the function to the size needed."": ""This choice is incorrect because AWS does not allow increasing the Lambda deployment package size limit through a support request. The limit is a hard limit set by AWS, and developers must work within these constraints by optimizing their code or using other strategies."",
    ""Use a compression algorithm that is more efficient than ZIP."": ""This choice is incorrect because AWS Lambda requires deployment packages to be in ZIP format. Even if a more efficient compression algorithm exists, it cannot be used for Lambda deployment packages as AWS Lambda only accepts ZIP files."",
    ""Break the function into multiple smaller Lambda functions."": ""This choice is correct because breaking a large function into smaller, more manageable functions is a recommended best practice in AWS Lambda. This approach not only helps stay within the size limits but also promotes better modularity, easier maintenance, and improved performance. Each smaller function can handle a specific part of the task, which can then be orchestrated using AWS Step Functions or other AWS services."",
    ""ZIP the ZIP file twice to compress it further."": ""This choice is incorrect because double-zipping a file does not effectively reduce the overall size. AWS Lambda requires a single ZIP file for deployment, and double-zipping would not be recognized as a valid deployment package. The ZIP format is already a compressed format, and additional zipping does not significantly reduce the size.""
}
```"
A serverless application uses an API Gateway and AWS Lambda. Where should the Lambda function store its session information across function calls?,"[""In an Amazon DynamoDB table."", ""In an Amazon SQS queue."", ""In the local filesystem."", ""In an SQLite session table using CDSQLITE_ENABLE_SESSION.""]","[""In an Amazon DynamoDB table.""]","```json
{
    ""In an Amazon DynamoDB table."": ""This choice is correct because Amazon DynamoDB is a fully managed NoSQL database service that is designed to store and retrieve any amount of data, and serve any level of request traffic. It is ideal for storing session information across multiple invocations of a Lambda function due to its scalability, low latency, and ability to handle concurrent access. DynamoDB is often used in serverless applications for storing state information, as it integrates well with AWS Lambda and provides features like TTL (Time to Live) for session expiration, which is a common requirement for session management."",
    ""In an Amazon SQS queue."": ""This choice is incorrect because Amazon SQS is a message queuing service that is used to decouple and scale microservices, distributed systems, and serverless applications. It is not designed for storing session information across function calls. SQS is used for passing messages between different components of an application, not for persistent storage of session data. Using SQS for session storage would be inefficient and not align with its intended use case."",
    ""In the local filesystem."": ""This choice is incorrect because AWS Lambda functions are stateless by design, meaning that each invocation is independent and does not have access to any previous invocation's data. The local filesystem in a Lambda execution environment is ephemeral and only available during the lifetime of the function execution. Any data stored locally would be lost after the function execution completes, making it unsuitable for storing session information across function calls."",
    ""In an SQLite session table using CDSQLITE_ENABLE_SESSION."": ""This choice is incorrect because SQLite is a lightweight, file-based database that is not suitable for use in AWS Lambda for session storage. Lambda's execution environment does not persist between invocations, and any local database file would be lost after the function execution. Additionally, using SQLite would require managing the database file within the limited local storage of the Lambda environment, which is not practical or scalable for session management in a serverless architecture.""
}
```"
"An application reads data from an Amazon DynamoDB table. Several times a day, for a period of 15 seconds, the application receives multiple ProvisionedThroughputExceeded errors. How should this exception be handled?","[""Create a new global secondary index for the table to help with the additional requests."", ""Retry the failed read requests with exponential backoff."", ""Immediately retry the failed read requests."", ""Use the DynamoDB `UpdateItem` API to increase the provisioned throughput capacity of the table.""]","[""Retry the failed read requests with exponential backoff.""]","```json
{
    ""Create a new global secondary index for the table to help with the additional requests."": ""This choice is incorrect because creating a new global secondary index (GSI) does not directly address the issue of ProvisionedThroughputExceeded errors. GSIs are used to query data with different attributes but do not inherently increase the provisioned throughput of the table. The problem described is related to exceeding the provisioned throughput, which requires handling through retry strategies or increasing capacity."",
    ""Retry the failed read requests with exponential backoff."": ""This choice is correct because using exponential backoff is a best practice for handling ProvisionedThroughputExceeded errors in DynamoDB. Exponential backoff involves retrying the request with increasing wait times between retries, which helps to reduce the load on the database and allows time for the throughput to stabilize. This approach is recommended by AWS to handle temporary spikes in demand without immediately increasing provisioned capacity."",
    ""Immediately retry the failed read requests."": ""This choice is incorrect because immediately retrying the failed requests can exacerbate the issue by continuously overloading the table with requests, leading to more ProvisionedThroughputExceeded errors. AWS recommends using exponential backoff to manage retries more effectively and prevent further strain on the system."",
    ""Use the DynamoDB `UpdateItem` API to increase the provisioned throughput capacity of the table."": ""This choice is incorrect because the `UpdateItem` API is used to modify individual items in a DynamoDB table, not to change the table's provisioned throughput. To increase the provisioned throughput, you would use the `UpdateTable` API. However, the scenario describes a temporary spike in demand, and exponential backoff is a more cost-effective solution than permanently increasing throughput.""
}
```"
A Developer is writing a Linux-based application to run on AWS Elastic Beanstalk. Application requirements state that the application must maintain full capacity during updates while minimizing cost. Which type of Elastic Beanstalk deployment policy should the Developer specify for the environment?,"[""Immutable."", ""Rolling."", ""All at Once."", ""Rolling with additional batch.""]","[""Rolling with additional batch.""]","{""Immutable."": ""Immutable deployments create a new set of instances alongside the existing ones and switch traffic to the new instances once they are ready. While this ensures full capacity and minimizes downtime, it is more costly because it requires double the resources during the deployment. This does not align with the requirement to minimize cost."", ""Rolling."": ""Rolling deployments update a portion of the instances at a time, which helps in maintaining some capacity but not full capacity during updates. This approach might lead to reduced capacity temporarily, which does not meet the requirement of maintaining full capacity during updates."", ""All at Once."": ""All at Once deployments update all instances simultaneously, which can lead to downtime as all instances are replaced at the same time. This approach does not maintain full capacity during updates, which is a key requirement in the question."", ""Rolling with additional batch."": ""Rolling with additional batch deployments add an extra batch of instances to the environment during the update process. This ensures that the application maintains full capacity by temporarily increasing the number of instances, thus meeting the requirement of maintaining full capacity while minimizing cost, as the additional instances are only temporary.""}"
"When writing a Lambda function, what is the benefit of instantiating AWS clients outside the scope of the handler?","[""Legibility and stylistic convention."", ""Taking advantage of connection re-use."", ""Better error handling."", ""Creating a new instance per invocation.""]","[""Taking advantage of connection re-use.""]","```json
{
    ""Legibility and stylistic convention."": ""This choice is incorrect. While organizing code for legibility and adhering to stylistic conventions is important, it is not the primary reason for instantiating AWS clients outside the handler. The main benefit is related to performance optimization through connection re-use, not code style."",
    ""Taking advantage of connection re-use."": ""This choice is correct. Instantiating AWS clients outside the Lambda handler function allows the Lambda execution environment to cache and reuse connections across multiple invocations. This reduces the overhead of establishing new connections each time the function is invoked, leading to improved performance and reduced latency. This practice is aligned with AWS best practices for optimizing Lambda function performance."",
    ""Better error handling."": ""This choice is incorrect. While error handling is crucial in Lambda functions, instantiating AWS clients outside the handler does not inherently improve error handling. Error handling is typically managed within the handler function itself, where exceptions can be caught and managed appropriately."",
    ""Creating a new instance per invocation."": ""This choice is incorrect. Creating a new instance of AWS clients per invocation would actually negate the benefits of connection re-use. It would lead to increased latency and resource consumption, as each invocation would have to establish a new connection. The goal is to minimize this overhead by reusing existing connections.""
}
```"
A current architecture uses many Lambda functions invoking one another as large state machine. The coordination of this state machine is legacy custom code that breaks easily. Which AWS Service can help refactor and manage the state machine?,"[""AWS Data Pipeline."", ""AWS SNS with AWS SQS."", ""Amazon Elastic MapReduce."", ""AWS Step Functions.""]","[""AWS Step Functions.""]","```json
{
    ""AWS Data Pipeline."": ""AWS Data Pipeline is a web service that helps automate the movement and transformation of data. It is not designed for managing state machines or coordinating workflows of Lambda functions. Therefore, it is not suitable for refactoring and managing a state machine architecture."",
    ""AWS SNS with AWS SQS."": ""AWS SNS (Simple Notification Service) and AWS SQS (Simple Queue Service) are messaging services that can be used for decoupling microservices and distributed systems. However, they do not provide the orchestration and state management capabilities required to manage a complex state machine involving Lambda functions. Thus, they are not appropriate for this use case."",
    ""Amazon Elastic MapReduce."": ""Amazon Elastic MapReduce (EMR) is a service for processing and analyzing large amounts of data using Hadoop and Spark. It is not intended for managing state machines or coordinating workflows, making it unsuitable for this scenario."",
    ""AWS Step Functions."": ""AWS Step Functions is a serverless orchestration service that allows you to coordinate multiple AWS services into serverless workflows. It is specifically designed to manage complex state machines and can easily coordinate the execution of Lambda functions, making it the ideal choice for refactoring and managing the state machine described in the question. Step Functions provide built-in error handling, retry logic, and state management, which aligns with AWS best practices for building resilient and scalable applications.""
}
```"
"A company is developing a new online game that will run on top of Amazon ECS. Four distinct Amazon ECS services will be part of the architecture, each requiring specific permissions to various AWS services. The company wants to optimize the use of the underlying Amazon EC2 instances by bin packing the containers based on memory reservation. Which configuration would allow the Development team to meet these requirements MOST securely?","[""Create a new Identity and Access Management (IAM) instance profile containing the required permissions for the various ECS services, then associate that instance role with the underlying EC2 instances."", ""Create four distinct IAM roles, each containing the required permissions for the associated ECS service, then configure each ECS service to reference the associated IAM role."", ""Create four distinct IAM roles, each containing the required permissions for the associated ECS service, then, create an IAM group and configure the ECS cluster to reference that group."", ""Create four distinct IAM roles, each containing the required permissions for the associated ECS service, then configure each ECS task definition to referenсe the associated IAM role.""]","[""Create four distinct IAM roles, each containing the required permissions for the associated ECS service, then configure each ECS task definition to referenсe the associated IAM role.""]","```json
{
    ""Create a new Identity and Access Management (IAM) instance profile containing the required permissions for the various ECS services, then associate that instance role with the underlying EC2 instances."": ""This choice is incorrect because associating an IAM instance profile with EC2 instances grants permissions to the entire instance, not individual ECS tasks. This approach does not adhere to the principle of least privilege, as all containers running on the instance would have access to the same permissions, which is not secure or optimal for managing distinct permissions for each ECS service."",
    ""Create four distinct IAM roles, each containing the required permissions for the associated ECS service, then configure each ECS service to reference the associated IAM role."": ""This choice is incorrect because ECS services themselves do not directly assume IAM roles. Instead, IAM roles are associated with ECS task definitions. The correct approach is to assign IAM roles to ECS task definitions to ensure that each task has the specific permissions it needs, adhering to the principle of least privilege."",
    ""Create four distinct IAM roles, each containing the required permissions for the associated ECS service, then, create an IAM group and configure the ECS cluster to reference that group."": ""This choice is incorrect because IAM groups are used to manage permissions for IAM users, not for ECS services or tasks. ECS tasks require IAM roles, not groups, to define the permissions they need to interact with other AWS services."",
    ""Create four distinct IAM roles, each containing the required permissions for the associated ECS service, then configure each ECS task definition to referenсe the associated IAM role."": ""This choice is correct because it aligns with AWS best practices for security and permission management. By associating distinct IAM roles with each ECS task definition, you ensure that each task has only the permissions it needs, adhering to the principle of least privilege. This approach also allows for fine-grained access control and better security management. According to AWS documentation, IAM roles for tasks are the recommended way to provide permissions to containers running in ECS.""
}
```"
A Developer must re-implement the business logic for an order fulfilment system. The business logic has to make requests to multiple vendors to decide where to purchase an item. The whole process can take up to a week to complete. What is the MOST efficient and SIMPLEST way to implement a system that meets these requirements?,"[""Use AWS Step Functions to execute parallel Lambda functions, and join the results."", ""Create an AWS SQS for each vendor, poll the queue from a worker instance, and joint the results."", ""Use AWS Lambda to asynchronously call a Lambda function for each vendor, and join the results."", ""Use Amazon CloudWatch Events to orchestrate the Lambda functions.""]","[""Use AWS Step Functions to execute parallel Lambda functions, and join the results.""]","```json
{
    ""Use AWS Step Functions to execute parallel Lambda functions, and join the results."": ""This choice is correct because AWS Step Functions is designed to coordinate multiple AWS services into serverless workflows, making it ideal for orchestrating complex business logic that involves multiple steps and can take a long time to complete. Step Functions can easily manage parallel execution of tasks, such as calling multiple vendor APIs, and can wait for all tasks to complete before proceeding. This is efficient and simplifies the process of handling long-running tasks, as it abstracts the complexity of managing state and retries. AWS Step Functions also provides built-in error handling and state management, which are crucial for long-running processes that can take up to a week."",
    ""Create an AWS SQS for each vendor, poll the queue from a worker instance, and joint the results."": ""This choice is incorrect because while SQS can be used for decoupling and buffering requests, it requires additional infrastructure to poll the queues and process messages, such as EC2 instances or Lambda functions. This adds complexity and overhead, as you would need to manage the lifecycle of these instances and handle the logic for joining results manually. Additionally, SQS does not natively support long-running workflows or parallel execution without additional orchestration, making it less suitable for this use case compared to Step Functions."",
    ""Use AWS Lambda to asynchronously call a Lambda function for each vendor, and join the results."": ""This choice is incorrect because while Lambda can be used to execute functions asynchronously, it lacks built-in orchestration capabilities for managing complex workflows that involve multiple steps and long-running processes. Manually handling the orchestration, state management, and error handling across multiple asynchronous Lambda invocations would add significant complexity and is not the simplest approach. AWS Step Functions provides these capabilities out of the box, making it a more suitable choice for this scenario."",
    ""Use Amazon CloudWatch Events to orchestrate the Lambda functions."": ""This choice is incorrect because Amazon CloudWatch Events is primarily used for triggering actions in response to changes in AWS resources or according to a schedule. It is not designed for orchestrating complex workflows or managing the execution of multiple parallel tasks. CloudWatch Events lacks the state management, error handling, and coordination features provided by AWS Step Functions, making it unsuitable for implementing a long-running, multi-step business logic process.""
}
```"
"A Developer is receiving HTTP 400: `ThrottlingException` errors intermittently when calling the Amazon CloudWatch API. When a call fails, no data is retrieved. What best practice should first be applied to address this issue?","[""Contact AWS Support for a limit increase."", ""Use the AWS CLI to get the metrics."", ""Analyze the applications and remove the API call."", ""Retry the call with exponential backoff.""]","[""Retry the call with exponential backoff.""]","```json
{
    ""Contact AWS Support for a limit increase."": ""This choice is incorrect because a ThrottlingException indicates that the request rate is too high, not necessarily that the account has reached a service limit. AWS best practices suggest handling throttling by implementing retry logic with exponential backoff rather than immediately requesting a limit increase."",
    ""Use the AWS CLI to get the metrics."": ""This choice is incorrect because using the AWS CLI does not inherently solve the issue of throttling. The ThrottlingException is related to the rate of requests, and using a different tool to make the same requests will not prevent throttling."",
    ""Analyze the applications and remove the API call."": ""This choice is incorrect because removing the API call does not address the underlying issue of throttling. The API call is likely necessary for the application's functionality, and the correct approach is to handle throttling through retry logic."",
    ""Retry the call with exponential backoff."": ""This choice is correct because AWS best practices recommend using exponential backoff to handle throttling errors. Exponential backoff involves retrying the request after increasingly longer wait times, which helps to reduce the load on the service and increases the chances of a successful request. This approach is specifically recommended in AWS documentation for handling ThrottlingExceptions.""
}
```"
An application is real-time processing millions of events that are received through an API. What service could be used to allow multiple consumers to process the data concurrently and MOST cost-effectively?,"[""Amazon SNS with fanout to an SQS queue for each application."", ""Amazon SNS with fanout to an SQS FIFO (first-in, first-out) queue for each application."", ""Amazon Kinesis Firehose."", ""Amazon Kinesis Streams.""]","[""Amazon Kinesis Streams.""]","```json
{
    ""Amazon SNS with fanout to an SQS queue for each application."": ""This option is incorrect because while SNS with SQS fanout allows for multiple consumers, it is not the most cost-effective or efficient solution for real-time processing of millions of events. SNS is generally used for pub/sub messaging and SQS for decoupling microservices, but they are not optimized for high-throughput real-time data processing like Kinesis Streams."",
    ""Amazon SNS with fanout to an SQS FIFO (first-in, first-out) queue for each application."": ""This option is incorrect because while SNS with SQS FIFO queues can ensure message ordering and allow multiple consumers, it is not the most cost-effective solution for real-time processing of millions of events. FIFO queues have higher costs and throughput limitations compared to standard queues and are not optimized for high-throughput real-time data processing like Kinesis Streams."",
    ""Amazon Kinesis Firehose."": ""This option is incorrect because Kinesis Firehose is primarily used for loading streaming data into data stores like S3, Redshift, or Elasticsearch, and not for real-time processing by multiple consumers. Firehose is a fully managed service for data delivery, but it does not provide the same level of real-time processing capabilities as Kinesis Streams."",
    ""Amazon Kinesis Streams."": ""This option is correct because Amazon Kinesis Streams is designed for real-time processing of streaming data at scale. It allows multiple consumers to process data concurrently and is optimized for high-throughput and low-latency data processing. Kinesis Streams is cost-effective for scenarios involving millions of events, as it allows for parallel processing and can handle large volumes of data efficiently. This makes it the best choice for real-time applications requiring concurrent processing.""
}
```"
Where should the `appspec.yml` file be placed in order for AWS CodeDeploy to work?,"[""In the root of the application source code directory structure."", ""In the bin folder along with all the complied code."", ""In an S3 bucket."", ""In the same folder as the application configuration files.""]","[""In the root of the application source code directory structure.""]","{""In the root of the application source code directory structure."": ""This is the correct answer because AWS CodeDeploy requires the appspec.yml file to be located at the root of the application's source code directory. This file defines the deployment actions and specifies how the application should be deployed. Placing it at the root ensures that CodeDeploy can easily locate it during the deployment process. According to AWS best practices, the appspec.yml file should be in the root to avoid any issues with CodeDeploy not finding the necessary instructions for deployment."", ""In the bin folder along with all the complied code."": ""This is incorrect because the bin folder typically contains compiled binaries or executable files, not configuration files like appspec.yml. CodeDeploy expects the appspec.yml file to be at the root of the source code directory, not buried in a subdirectory like bin. Placing it in the bin folder would prevent CodeDeploy from finding it, leading to deployment failures."", ""In an S3 bucket."": ""This is incorrect because, while AWS CodeDeploy can source application files from an S3 bucket, the appspec.yml file itself must be included in the application's source code package and placed at the root directory of that package. The appspec.yml file is not directly referenced from an S3 bucket during the deployment process; it must be part of the deployment bundle."", ""In the same folder as the application configuration files."": ""This is incorrect because, although it might seem logical to place configuration files together, AWS CodeDeploy specifically requires the appspec.yml file to be at the root of the application's source code directory. This ensures that CodeDeploy can consistently locate the file regardless of how the application is structured. Placing it in a subdirectory with other configuration files would prevent CodeDeploy from accessing it properly.""}"
An application will ingest data at a very high throughput from many sources and must store the data in an Amazon S3 bucket. Which service would BEST accomplish this task?,"[""Amazon Kinesis Firehose."", ""Amazon S3 Acceleration Transfer."", ""Amazon SQS."", ""Amazon SNS.""]","[""Amazon Kinesis Firehose.""]","```json
{
    ""Amazon Kinesis Firehose."": ""Amazon Kinesis Firehose is the correct choice because it is specifically designed for loading streaming data into data lakes, data stores, and analytics services. It can capture, transform, and load streaming data into Amazon S3, Amazon Redshift, Amazon Elasticsearch Service, and Splunk. It is fully managed and can automatically scale to match the throughput of incoming data, making it ideal for high-throughput data ingestion scenarios. This aligns with the requirement to ingest data at a very high throughput from many sources and store it in an Amazon S3 bucket."",
    ""Amazon S3 Acceleration Transfer."": ""Amazon S3 Transfer Acceleration is incorrect in this context because it is a feature that speeds up the transfer of files over long distances between your client and an S3 bucket. While it can improve upload speeds, it is not designed to handle high-throughput data ingestion from multiple sources like Kinesis Firehose. It is more suitable for scenarios where you need to upload large files quickly from a distant location."",
    ""Amazon SQS."": ""Amazon SQS is incorrect because it is a message queuing service, which is used to decouple and scale microservices, distributed systems, and serverless applications. While it can handle high-throughput messages, it is not specifically designed for direct data ingestion into Amazon S3. It is more suitable for scenarios where you need to manage message queues rather than direct data ingestion."",
    ""Amazon SNS."": ""Amazon SNS is incorrect because it is a pub/sub messaging service that enables applications to send time-critical messages to multiple subscribers. It is not designed for high-throughput data ingestion into Amazon S3. SNS is more appropriate for scenarios where you need to send notifications or messages to multiple endpoints or services.""
}
```"
A Developer is creating a Lambda function and will be using external libraries that are not included in the standard Lambda libraries. What action would minimize the Lambda compute time consumed?,"[""Install the dependencies and external libraries at the beginning of the Lambda function."", ""Create a Lambda deployment package that includes the external libraries."", ""Copy the external libraries to Amazon S3, and reference the external libraries to the S3 location."", ""Install the external libraries in Lambda Layer to be available to all Lambda functions.""]","[""Install the external libraries in Lambda Layer to be available to all Lambda functions.""]","```json
{
    ""Install the dependencies and external libraries at the beginning of the Lambda function."": ""This choice is incorrect because installing dependencies at the beginning of the Lambda function execution would increase the function's cold start time and overall execution time. This approach is inefficient as it requires downloading and installing the libraries every time the function is invoked, which consumes additional compute time and can lead to increased latency."",
    ""Create a Lambda deployment package that includes the external libraries."": ""This choice is partially correct but not optimal. Including external libraries in the deployment package can work, but it can lead to larger package sizes, which might slow down deployment and increase the cold start time. Additionally, if multiple Lambda functions require the same libraries, each function would need its own copy, leading to redundancy and increased maintenance overhead."",
    ""Copy the external libraries to Amazon S3, and reference the external libraries to the S3 location."": ""This choice is incorrect because while storing libraries in S3 might reduce the deployment package size, it introduces additional latency and complexity. The Lambda function would need to download the libraries from S3 during execution, which increases execution time and can lead to higher costs and slower performance."",
    ""Install the external libraries in Lambda Layer to be available to all Lambda functions."": ""This choice is correct because using Lambda Layers is a best practice for managing dependencies. Layers allow you to package libraries separately from your function code, reducing the deployment package size and improving reusability across multiple functions. This approach minimizes the Lambda compute time consumed by avoiding redundant installations and reducing cold start times. AWS documentation recommends using layers for shared code and dependencies, making it an efficient and scalable solution.""
}
```"
"During non-peak hours, a Developer wants to minimize the execution time of a full Amazon DynamoDB table scan without affecting normal workloads. The workloads average half of the strongly consistent read capacity units during non-peak hours. How would the Developer optimize this scan?","[""Use parallel scans while limiting the rate."", ""Use sequential scans."", ""Increase read capacity units during the scan operation."", ""Change consistency to eventually consistent during the scan operation.""]","[""Use parallel scans while limiting the rate.""]","```json
{
    ""Use parallel scans while limiting the rate."": ""This choice is correct because parallel scans can significantly reduce the time it takes to complete a full table scan by dividing the table into segments and scanning them concurrently. Limiting the rate ensures that the scan does not consume all available read capacity, which would affect normal workloads. This approach balances speed and resource usage, adhering to best practices for optimizing DynamoDB scans without impacting other operations."",
    ""Use sequential scans."": ""This choice is incorrect because sequential scans process the table one segment at a time, which can be slow for large tables. This method does not take advantage of DynamoDB's ability to perform parallel scans, which can significantly reduce scan time. Sequential scans are less efficient and do not optimize the use of available read capacity units during non-peak hours."",
    ""Increase read capacity units during the scan operation."": ""This choice is incorrect because increasing read capacity units is not necessary if the current capacity is underutilized during non-peak hours. The question specifies that the workloads average half of the read capacity units, indicating there is already unused capacity that can be leveraged. Increasing capacity would incur additional costs without providing a significant benefit in this scenario."",
    ""Change consistency to eventually consistent during the scan operation."": ""This choice is incorrect because changing to eventually consistent reads does not directly affect the speed of a scan operation. While eventually consistent reads consume less capacity than strongly consistent reads, the question specifies that there is already sufficient unused capacity. The main goal is to reduce scan time, which is better achieved through parallel scans rather than altering consistency.""
}
```"
A large e-commerce site is being designed to deliver static objects from Amazon S3. The Amazon S3 bucket will server more than 300 GET requests per second. What should be done to optimize performance? (Choose TWO),"[""Integrate Amazon CloudFront with Amazon S3."", ""Enable Amazon S3 cross-region replication."", ""Delete expired Amazon S3 server log files."", ""Configure Amazon S3 lifecycle rules.Randomize Amazon S3 key name prefixes."", ""Randomize Amazon S3 key name prefixes.""]","[""Integrate Amazon CloudFront with Amazon S3."", ""Enable Amazon S3 cross-region replication.""]","```json
{
    ""Integrate Amazon CloudFront with Amazon S3."": ""This choice is correct because Amazon CloudFront is a content delivery network (CDN) that caches copies of your content close to your users, which can significantly reduce latency and improve the performance of delivering static objects stored in Amazon S3. By using CloudFront, you can distribute the load of GET requests across multiple edge locations, reducing the burden on the S3 bucket itself. This is a best practice for optimizing performance for high-frequency access to static content."",
    ""Enable Amazon S3 cross-region replication."": ""This choice is incorrect in the context of optimizing performance for GET requests. Cross-region replication is used for replicating objects across different AWS regions for disaster recovery or compliance purposes, not for improving read performance. It does not reduce latency or distribute GET request load, and thus does not directly address the performance optimization for high-frequency GET requests."",
    ""Delete expired Amazon S3 server log files."": ""This choice is incorrect because deleting expired server log files does not impact the performance of GET requests. While managing log files is important for cost and storage management, it does not optimize the retrieval performance of static objects from S3."",
    ""Configure Amazon S3 lifecycle rules."": ""This choice is incorrect in the context of optimizing GET request performance. Lifecycle rules are used to manage the lifecycle of objects in S3, such as transitioning objects to different storage classes or deleting them after a certain period. They do not affect the performance of serving static content."",
    ""Randomize Amazon S3 key name prefixes."": ""This choice is correct because randomizing S3 key name prefixes can help distribute the load evenly across Amazon S3's internal partitions. S3 uses a partitioning system to manage data, and having a high request rate on a single partition can lead to performance bottlenecks. By randomizing prefixes, you can ensure that requests are spread across multiple partitions, improving performance for high-frequency GET requests.""
}
```"
A legacy service has an XML-based SOAP interface. The Developer wants to expose the functionality of the service to external clients with the Amazon API Gateway. Which technique will accomplish this?,"[""Create a RESTful API with the API Gateway; transform the incoming JSON into a valid XML message for the SOAP interface using mapping templates."", ""Create a RESTful API with the API Gateway; pass the incoming JSON to the SOAP interface through an Application Load Balancer."", ""Create a RESTful API with the API Gateway; pass the incoming XML to the SOAP interface through an Application Load Balancer."", ""Create a RESTful API with the API Gateway; transform the incoming XML into a valid message for the SOAP interface using mapping templates.""]","[""Create a RESTful API with the API Gateway; transform the incoming JSON into a valid XML message for the SOAP interface using mapping templates.""]","```json
{
    ""Create a RESTful API with the API Gateway; transform the incoming JSON into a valid XML message for the SOAP interface using mapping templates."": ""This choice is correct because Amazon API Gateway supports request and response transformation using mapping templates. When exposing a SOAP-based service via API Gateway, the incoming JSON payload can be transformed into XML using a mapping template, which is necessary to communicate with the SOAP interface. This approach leverages API Gateway's ability to handle different data formats and transform them as needed, which is a common requirement when integrating legacy SOAP services with modern RESTful APIs."",
    ""Create a RESTful API with the API Gateway; pass the incoming JSON to the SOAP interface through an Application Load Balancer."": ""This choice is incorrect because simply passing JSON to a SOAP interface will not work, as SOAP requires XML-formatted messages. An Application Load Balancer does not perform data transformation, so it cannot convert JSON to XML. The correct approach is to use API Gateway's mapping templates to transform the data format before sending it to the SOAP service."",
    ""Create a RESTful API with the API Gateway; pass the incoming XML to the SOAP interface through an Application Load Balancer."": ""This choice is incorrect because the question specifies that the incoming data is JSON, not XML. Therefore, passing JSON directly to a SOAP interface without transformation would not be compatible. Additionally, an Application Load Balancer does not handle data transformation, so it cannot convert JSON to XML. The correct method involves using API Gateway's mapping templates to perform the necessary transformation."",
    ""Create a RESTful API with the API Gateway; transform the incoming XML into a valid message for the SOAP interface using mapping templates."": ""This choice is incorrect because the question specifies that the incoming data is JSON, not XML. The transformation needs to convert JSON to XML, not XML to XML. API Gateway's mapping templates are indeed used for data transformation, but the direction of transformation in this choice does not match the requirements of the scenario.""
}
```"
"A Developer has an application that can upload tens of thousands of objects per second to Amazon S3 in parallel within a single AWS account. As part of new requirements, data stored in S3 must use server side encryption with AWS KMS (SSE-KMS). After creating this change, performance of the application is slower. Which of the following is MOST likely the cause of the application latency?","[""Amazon S3 throttles the rate at which uploaded objects can be encrypted using Customer Master Keys."", ""The AWS KMS API calls limit is less than needed to achieve the desired performance."", ""The client encryption of the objects is using a poor algorithm."", ""KMS requires that an alias be used to create an independent display name that can be mapped to a CM.""]","[""The AWS KMS API calls limit is less than needed to achieve the desired performance.""]","```json
{
    ""Amazon S3 throttles the rate at which uploaded objects can be encrypted using Customer Master Keys."": ""This choice is incorrect because Amazon S3 itself does not throttle encryption operations. The bottleneck is more likely related to AWS KMS, which handles the encryption keys. S3 is designed to handle high throughput and scales automatically to accommodate large volumes of requests."",
    ""The AWS KMS API calls limit is less than needed to achieve the desired performance."": ""This choice is correct because AWS KMS has a quota on the number of requests per second for cryptographic operations. When using SSE-KMS, each object upload requires a call to KMS to encrypt the data encryption key, which can lead to throttling if the rate of requests exceeds the KMS limits. This is a common issue when applications perform a high volume of encryption operations in parallel."",
    ""The client encryption of the objects is using a poor algorithm."": ""This choice is incorrect because the question specifies server-side encryption with AWS KMS (SSE-KMS), which means encryption is handled by AWS, not the client. The client-side encryption algorithm is irrelevant in this context."",
    ""KMS requires that an alias be used to create an independent display name that can be mapped to a CM."": ""This choice is incorrect because while KMS aliases are useful for managing and referencing keys, they do not impact performance. The use of an alias is a naming convenience and does not affect the rate at which KMS can process encryption requests.""
}
```"
A customer wants to deploy its source code on an AWS Elastic Beanstalk environment. The customer needs to perform deployment with minimal outage and should only use existing instances to retain application access log. What deployment policy would satisfy these requirements?,"[""Rolling."", ""All at once."", ""Rolling with an additional batch."", ""Immutable.""]","[""Rolling.""]","```json
{
    ""Rolling."": ""Rolling deployments update a few instances at a time, which helps in minimizing downtime and allows the application to continue running on other instances while the update is in progress. This approach retains the existing instances, preserving access logs, and is suitable for scenarios where minimal outage is required. AWS Elastic Beanstalk supports rolling deployments, making it a best practice for this requirement."",
    ""All at once."": ""All at once deployment replaces all instances simultaneously, resulting in a complete outage during the deployment process. This method does not meet the requirement of minimal outage and retaining application access logs on existing instances, as all instances are replaced at once."",
    ""Rolling with an additional batch."": ""Rolling with an additional batch involves deploying updates in batches with an extra batch of instances to handle traffic during the update. While this minimizes downtime, it involves launching new instances temporarily, which does not align with the requirement to only use existing instances to retain application access logs."",
    ""Immutable."": ""Immutable deployments create a new set of instances alongside the existing ones and switch traffic to the new instances once they pass health checks. This approach ensures zero downtime but does not meet the requirement of using only existing instances, as it involves launching new instances, which does not retain application access logs on the existing instances.""
}
```"
A Developer has setup an Amazon Kinesis Stream with 4 shards to ingest a maximum of 2500 records per second. A Lambda function has been configured to process these records. In which order will these records be processed?,"[""Lambda will receive each record in the reverse order it was placed into the stream following a LIFO (last-in, first-out) method."", ""Lambda will receive each record in the exact order it was placed into the stream following a FIFO (first­-in, first-out) method."", ""Lambda will receive each record in the exact order it was placed into the shard following a FIFO (first-in, first-out) method. There is no guarantee of order across shards."", ""The Developer can select FIFO, (first-in, first-out), LIFO (last-in, last-out), random, or request specific record using the getRecords API.""]","[""Lambda will receive each record in the exact order it was placed into the shard following a FIFO (first-in, first-out) method. There is no guarantee of order across shards.""]","```json
{
    ""Lambda will receive each record in the reverse order it was placed into the stream following a LIFO (last-in, first-out) method."": ""This choice is incorrect because Amazon Kinesis Streams process records in a FIFO (first-in, first-out) order within each shard, not LIFO. Records are processed in the order they are added to a shard, and Lambda functions triggered by Kinesis Streams will receive records in the same order they were added to the shard."",
    ""Lambda will receive each record in the exact order it was placed into the stream following a FIFO (first­-in, first-out) method."": ""This choice is incorrect because while records are processed in FIFO order within each shard, there is no guarantee of order across multiple shards. The statement implies a global order across the entire stream, which is not how Kinesis Streams operate."",
    ""Lambda will receive each record in the exact order it was placed into the shard following a FIFO (first-in, first-out) method. There is no guarantee of order across shards."": ""This choice is correct because Amazon Kinesis Streams ensure that records are processed in FIFO order within each individual shard. However, since a stream can have multiple shards, there is no guarantee of the order of records across different shards. This is a key concept in understanding how Kinesis Streams and Lambda integration works."",
    ""The Developer can select FIFO, (first-in, first-out), LIFO (last-in, last-out), random, or request specific record using the getRecords API."": ""This choice is incorrect because the order of record processing in Kinesis Streams is not configurable by the developer. The getRecords API retrieves records in FIFO order within a shard, and there is no option to select LIFO or random order. The processing order is determined by the stream's architecture and not by developer preference.""
}
```"
An organization must store thousands of sensitive audio and video files in an Amazon S3 bucket. Organizational security policies require that all data written to this bucket be encrypted. How can compliance with this policy be ensured?,"[""Use AWS Lambda to send notifications to the security team if unencrypted objects are put in the bucket."", ""Configure an Amazon S3 bucket policy to prevent the upload of objects that do not contain the x-amz­-server-side-encryption header."", ""Create an Amazon CloudWatch event rule to verify that all objects stored in the Amazon S3 bucket are encrypted."", ""Configure an Amazon S3 bucket policy to prevent the upload of objects that contain the x-amz-server­side-encryption header.""]","[""Configure an Amazon S3 bucket policy to prevent the upload of objects that do not contain the x-amz­-server-side-encryption header.""]","```json
{
    ""Use AWS Lambda to send notifications to the security team if unencrypted objects are put in the bucket."": ""This choice is incorrect because while AWS Lambda can be used to trigger actions or notifications, it does not prevent the upload of unencrypted objects. The requirement is to ensure compliance by preventing such uploads, not just notifying after the fact. This approach is reactive rather than proactive, which does not align with the requirement to ensure all data is encrypted upon upload."",
    ""Configure an Amazon S3 bucket policy to prevent the upload of objects that do not contain the x-amz­-server-side-encryption header."": ""This choice is correct because configuring a bucket policy to deny uploads that do not include the x-amz-server-side-encryption header ensures that only encrypted objects can be uploaded. This is a proactive measure that enforces compliance with the encryption policy at the time of upload, aligning with AWS best practices for data protection and security."",
    ""Create an Amazon CloudWatch event rule to verify that all objects stored in the Amazon S3 bucket are encrypted."": ""This choice is incorrect because CloudWatch event rules are used for monitoring and triggering actions based on events, not for enforcing policies. While it can be used to monitor the state of objects, it does not prevent the upload of unencrypted objects, which is the requirement. This approach would be more suitable for auditing rather than enforcing compliance."",
    ""Configure an Amazon S3 bucket policy to prevent the upload of objects that contain the x-amz-server­side-encryption header."": ""This choice is incorrect because it contradicts the requirement. Preventing uploads of objects that contain the x-amz-server-side-encryption header would mean only allowing unencrypted objects, which is the opposite of the security policy requirement. This would not ensure compliance with the encryption policy.""
}
```"
An application is designed to use Amazon SQS to manage messages from many independent senders. Each sender's messages must be processed in the order they are received. Which SQS feature should be implemented by the Developer?,"[""Configure each sender with a unique MessageGroupId."", ""Enable MessageDeduplicationIds on the SQS queue."", ""Configure each message with unique MessageGroupIds."", ""Enable ContentBasedDeduplication on the SQS queue.""]","[""Configure each message with unique MessageGroupIds.""]","```json
{
    ""Configure each sender with a unique MessageGroupId."": ""This choice is incorrect because configuring each sender with a unique MessageGroupId would not ensure that messages from the same sender are processed in order. MessageGroupId is used in Amazon SQS FIFO queues to group messages that should be processed in order. If each sender has a unique MessageGroupId, it would mean that messages from different senders are grouped separately, but it does not guarantee the order of messages from the same sender."",
    ""Enable MessageDeduplicationIds on the SQS queue."": ""This choice is incorrect because MessageDeduplicationId is used to avoid sending duplicate messages within a 5-minute deduplication interval in FIFO queues. It does not ensure message ordering. The deduplication ID helps in ensuring that a message is not processed more than once, but it does not relate to maintaining the order of messages."",
    ""Configure each message with unique MessageGroupIds."": ""This choice is correct because in Amazon SQS FIFO queues, the MessageGroupId is used to ensure that messages with the same group ID are processed in the order they are received. By configuring each message with a unique MessageGroupId per sender, you ensure that messages from the same sender are processed in order. This is a key feature of SQS FIFO queues, which are designed for exactly-once processing and maintaining the order of messages."",
    ""Enable ContentBasedDeduplication on the SQS queue."": ""This choice is incorrect because ContentBasedDeduplication is a feature that automatically generates a deduplication ID based on the content of the message. While it helps in preventing duplicate messages, it does not ensure that messages are processed in order. ContentBasedDeduplication is related to deduplication, not ordering.""
}
```"
"A Developer created a dashboard for an application using Amazon API Gateway, Amazon S3, AWS Lambda, and Amazon RDS. The Developer needs an authentication mechanism allowing a user to sign in and view the dashboard. It must be accessible from mobile applications, desktops, and tablets, and must remember user preferences across platforms. Which AWS service should the Developer use to support this authentication scenario?","[""AWS KMS."", ""Amazon Cognito."", ""AWS Directory Service."", ""Amazon IAM.""]","[""Amazon Cognito.""]","```json
{
    ""AWS KMS."": ""AWS Key Management Service (KMS) is primarily used for encryption and key management, not for user authentication. It helps to create and manage cryptographic keys and control their use across a wide range of AWS services and in your applications. It does not provide user authentication or session management capabilities, which are required for the scenario described."",
    ""Amazon Cognito."": ""Amazon Cognito is the correct choice because it provides user authentication, authorization, and user management for web and mobile applications. It supports sign-in through social identity providers and enterprise identity providers via SAML 2.0 and OpenID Connect. Cognito also offers features like user pools and identity pools, which can remember user preferences across platforms, making it ideal for applications that need to be accessible from mobile, desktop, and tablets. This aligns with the requirements of the scenario described."",
    ""AWS Directory Service."": ""AWS Directory Service is used to set up and run Microsoft Active Directory (AD) in the AWS Cloud or connect your AWS resources with an existing on-premises AD. While it can be used for authentication in certain scenarios, it is more suited for enterprise environments that require integration with Microsoft AD. It does not natively support the cross-platform user preference management needed in this scenario."",
    ""Amazon IAM."": ""AWS Identity and Access Management (IAM) is used to manage access to AWS services and resources securely. While IAM is crucial for managing permissions and roles within AWS, it is not designed for end-user authentication in applications. It does not provide the user management features required for the scenario, such as remembering user preferences across different platforms.""
}
```"
"A Lambda function is packaged for deployment to multiple environments, including development, test, production, etc. Each environment has unique set of resources such as databases, etc. How can the Lambda function use the resources for the current environment?","[""Apply tags to the Lambda functions."", ""Hardcore resources in the source code."", ""Use environment variables for the Lambda functions."", ""Use separate function for development and production.""]","[""Use environment variables for the Lambda functions.""]","```json
{
    ""Apply tags to the Lambda functions."": ""Tags are used for organizing and managing AWS resources, such as for billing or access control, but they do not provide a mechanism for configuring environment-specific resources. Therefore, using tags would not allow a Lambda function to dynamically use resources specific to an environment."",
    ""Hardcore resources in the source code."": ""Hardcoding resources in the source code is not a best practice because it makes the code less flexible and harder to maintain. It requires code changes and redeployment for each environment, which is error-prone and does not adhere to the principle of separation of configuration from code."",
    ""Use environment variables for the Lambda functions."": ""Using environment variables is a best practice for configuring Lambda functions for different environments. Environment variables allow you to set configuration values that can be accessed by the function code at runtime, enabling the same code to be deployed across multiple environments with different configurations. This approach adheres to the principle of separating configuration from code, making it easier to manage and deploy."",
    ""Use separate function for development and production."": ""Using separate functions for different environments can lead to code duplication and increased maintenance overhead. It is not a scalable or efficient solution, especially when the only difference between environments is configuration. This approach contradicts the best practice of using a single codebase with environment-specific configurations.""
}
```"
A Developer needs temporary access to resources in a second account. What is the MOST secure way to achieve this?,"[""Use the Amazon Cognito user pools to get short-lived credentials for the second account."", ""Create a dedicated IAM access key for the second account, and send it by mail."", ""Create a cross-account access role, and use sts: AssumeRole API to get short-lived credentials."", ""Establish trust, and add an SSH key for the second account to the IAM user.""]","[""Create a cross-account access role, and use sts: AssumeRole API to get short-lived credentials.""]","```json
{
    ""Use the Amazon Cognito user pools to get short-lived credentials for the second account."": ""This choice is incorrect because Amazon Cognito is primarily used for managing user authentication and access for web and mobile applications, not for cross-account access between AWS accounts. While Cognito can provide temporary credentials, it is not designed for cross-account access scenarios where an AWS account needs to access resources in another AWS account."",
    ""Create a dedicated IAM access key for the second account, and send it by mail."": ""This choice is incorrect because creating dedicated IAM access keys and sending them by mail is not secure. Access keys are long-term credentials and should be protected. Sending them via email exposes them to potential interception and misuse. AWS best practices recommend using temporary credentials for access, especially for cross-account scenarios."",
    ""Create a cross-account access role, and use sts: AssumeRole API to get short-lived credentials."": ""This choice is correct because creating a cross-account IAM role and using the sts:AssumeRole API is the most secure and recommended way to provide temporary access to resources in another AWS account. This approach allows for granting temporary, limited permissions that can be tightly controlled and audited. It follows AWS best practices for security by minimizing the use of long-term credentials and leveraging temporary security credentials."",
    ""Establish trust, and add an SSH key for the second account to the IAM user."": ""This choice is incorrect because SSH keys are used for secure shell access to EC2 instances, not for granting access to AWS resources across accounts. Establishing trust between accounts is done through IAM roles and policies, not by adding SSH keys. This method does not align with AWS best practices for cross-account access.""
}
```"
A Developer needs to use AWS X-Ray to monitor an application that is deployed on EC2 instances. What steps have to be executed to perform the monitoring?,"[""Deploy the X-Ray SDK with the application and use X-Ray annotation."", ""Install the X-Ray daemon and instrument the application code."", ""Install the X-Ray daemon and configure it to forward data to Amazon CloudWatch Events."", ""Deploy the X-Ray SDK with the application and instrument the application code.""]","[""Install the X-Ray daemon and instrument the application code.""]","```json
{
    ""Deploy the X-Ray SDK with the application and use X-Ray annotation."": ""This choice is incorrect because while deploying the X-Ray SDK with the application is necessary, using only X-Ray annotations is not sufficient for monitoring. Annotations are used to add metadata to segments, but they do not replace the need for the X-Ray daemon and instrumentation of the application code, which are required to capture and send trace data to X-Ray."",
    ""Install the X-Ray daemon and instrument the application code."": ""This choice is correct because installing the X-Ray daemon is necessary for collecting trace data and sending it to the X-Ray service. Instrumenting the application code with the X-Ray SDK is also required to generate trace data. This combination allows for end-to-end tracing of requests through the application, which is a best practice for monitoring and debugging distributed applications on AWS."",
    ""Install the X-Ray daemon and configure it to forward data to Amazon CloudWatch Events."": ""This choice is incorrect because while the X-Ray daemon is necessary, configuring it to forward data to Amazon CloudWatch Events is not part of the standard setup for monitoring with X-Ray. The X-Ray daemon collects trace data and sends it directly to the X-Ray service, not to CloudWatch Events. CloudWatch Events can be used for other purposes, such as triggering actions based on certain conditions, but it is not directly involved in the X-Ray data flow."",
    ""Deploy the X-Ray SDK with the application and instrument the application code."": ""This choice is partially correct because deploying the X-Ray SDK and instrumenting the application code are necessary steps. However, without the X-Ray daemon, the trace data cannot be sent to the X-Ray service. The daemon acts as a buffer and forwarder of trace data, making it a crucial component of the setup.""
}
```"
A Developer is creating an Auto Scaling group whose instances need to publish a custom metric to Amazon CloudWatch. Which method would be the MOST secure way to authenticate a CloudWatch PUT request?,"[""Create an IAM user with PutMetricData permission and put the user credentials in a private repository; have applications pull the credentials as needed."", ""Create an IAM user with PutMetricData permission, and modify the Auto Scaling launch configuration to inject the user credentials into the instance user data."", ""Modify the CloudWatch metric policies to allow the PutMetricData permission to instances from the Auto Scaling group."", ""Create an IAM role with PutMetricData permission and modify the Auto Scaling launching configuration to launch instances using that role.""]","[""Create an IAM role with PutMetricData permission and modify the Auto Scaling launching configuration to launch instances using that role.""]","```json
{
    ""Create an IAM user with PutMetricData permission and put the user credentials in a private repository; have applications pull the credentials as needed."": ""This method is insecure because it involves storing credentials in a repository, which can be a security risk if the repository is compromised. AWS best practices recommend avoiding hardcoding credentials or storing them in locations where they can be easily accessed or leaked. Instead, using IAM roles is preferred as they provide temporary security credentials that are automatically rotated."",
    ""Create an IAM user with PutMetricData permission, and modify the Auto Scaling launch configuration to inject the user credentials into the instance user data."": ""Injecting credentials into user data is insecure because user data can be accessed by anyone with access to the instance, potentially exposing sensitive information. AWS best practices recommend using IAM roles for EC2 instances, which provide secure and temporary credentials without the need to manage or distribute them manually."",
    ""Modify the CloudWatch metric policies to allow the PutMetricData permission to instances from the Auto Scaling group."": ""While modifying CloudWatch metric policies might seem like a solution, it does not directly address the need for secure authentication of requests. IAM roles are specifically designed to provide secure, temporary access to AWS services for EC2 instances, making them a more appropriate choice for this scenario."",
    ""Create an IAM role with PutMetricData permission and modify the Auto Scaling launching configuration to launch instances using that role."": ""This is the correct and most secure method because IAM roles provide temporary security credentials that are automatically rotated and managed by AWS. By attaching an IAM role with the necessary permissions to the instances in the Auto Scaling group, you ensure that each instance can securely publish metrics to CloudWatch without the need for manual credential management. This approach aligns with AWS security best practices and reduces the risk of credential exposure.""
}
```"
"A Developer is working on an application that tracks hundreds of millions of product reviews in an Amazon DynamoDB table. The records include the data elements shown in the table. Which field, when used as the partition key, would result in the MOST consistent performance using DynamoDB?","[""starRating."", ""reviewID."", ""comment."", ""productID.""]","[""reviewID.""]","```json
{
    ""starRating."": ""Using 'starRating' as a partition key would likely result in uneven data distribution. 'starRating' typically has a limited range of values (e.g., 1 to 5 stars), which would cause a high concentration of records for each partition key value. This can lead to hot partitions, where some partitions are accessed much more frequently than others, resulting in inconsistent performance. According to AWS best practices, partition keys should have a wide range of values to ensure even data distribution across partitions."",
    ""reviewID."": ""Using 'reviewID' as a partition key is the correct choice because it is likely to be unique for each review, ensuring an even distribution of data across all partitions. This uniqueness helps avoid hot partitions and provides consistent performance, as no single partition is accessed more frequently than others. AWS documentation recommends using a partition key that has high cardinality (many unique values) to distribute workload evenly across partitions, which 'reviewID' satisfies."",
    ""comment."": ""Using 'comment' as a partition key is not ideal because comments are typically lengthy text fields and can vary greatly in size. This does not guarantee uniqueness or even distribution, as some comments might be repeated or similar, leading to potential hot partitions. Moreover, using large text fields as partition keys is not efficient. AWS best practices suggest using partition keys with high cardinality and relatively small size for optimal performance."",
    ""productID."": ""Using 'productID' as a partition key could lead to uneven data distribution if certain products receive significantly more reviews than others. This could create hot partitions, where some partitions are accessed much more frequently than others, resulting in inconsistent performance. While 'productID' might have more unique values than 'starRating', it still does not guarantee the even distribution that 'reviewID' provides. AWS best practices emphasize the importance of choosing a partition key that ensures even data distribution across all partitions.""
}
```"
"A development team consists of 10 team members. Similar to a home directory for each team member, the manager wants to grant access to user-specific folders in an Amazon S3 bucket. For the team member with the username `TeamMemberX`, the snippet of the IAM policy looks like this. Instead of creating distinct policies for each team member, what approach can be used to make this policy snippet generic for all team members?","[""Use IAM policy condition."", ""Use IAM policy principal."", ""Use IAM policy variables."", ""Use IAM policy resource.""]","[""Use IAM policy condition.""]","```json
{
    ""Use IAM policy condition."": ""This choice is incorrect. IAM policy conditions are used to specify conditions under which a policy is in effect, such as time of day or IP address range, but they are not used to dynamically reference user-specific attributes like usernames in resource paths. Therefore, this approach does not help in making the policy generic for all team members."",
    ""Use IAM policy principal."": ""This choice is incorrect. The principal element in an IAM policy specifies the user, account, service, or other entity that is allowed or denied access to a resource. It does not help in dynamically referencing user-specific folders in an S3 bucket. Therefore, it is not suitable for making the policy generic for all team members."",
    ""Use IAM policy variables."": ""This choice is correct. IAM policy variables allow you to use placeholders in your policy that are replaced with values from the request context when the policy is evaluated. For example, you can use the ${aws:username} variable to dynamically reference the username of the requester, making it possible to create a single policy that applies to all team members and grants them access to their specific folders in an S3 bucket."",
    ""Use IAM policy resource."": ""This choice is incorrect. The resource element in an IAM policy specifies the AWS resources to which the actions apply. While you can specify user-specific folders as resources, this does not inherently make the policy generic. You would still need to define each user's folder explicitly unless you use variables to generalize the policy.""
}
```"
"A company needs to encrypt data at rest, but it wants to leverage an AWS managed service using its own master key. Which of the following AWS service can be used to meet these requirements?","[""SSE with Amazon S3."", ""SSE with AWS KMS."", ""Client-side encryption."", ""AWS IAM roles and policies.""]","[""SSE with AWS KMS.""]","```json
{
    ""SSE with Amazon S3."": ""This option refers to Server-Side Encryption with Amazon S3, which uses Amazon S3-managed keys (SSE-S3) by default. While it provides encryption at rest, it does not allow the use of a customer-managed key. Therefore, it does not meet the requirement of using the company's own master key."",
    ""SSE with AWS KMS."": ""This is the correct choice. Server-Side Encryption with AWS Key Management Service (SSE-KMS) allows you to use your own customer master key (CMK) managed by AWS KMS. This service provides additional security controls and audit capabilities, meeting the requirement of using the company's own master key for encryption at rest. AWS KMS is a managed service that simplifies the creation and control of encryption keys used to encrypt data."",
    ""Client-side encryption."": ""Client-side encryption involves encrypting data before sending it to AWS services. While this allows the use of customer-managed keys, it does not leverage an AWS managed service for encryption at rest, which is the requirement specified in the question. Therefore, it is not the correct choice."",
    ""AWS IAM roles and policies."": ""IAM roles and policies are used for access control and permissions management within AWS. They do not provide encryption capabilities themselves. Therefore, this option does not meet the requirement of encrypting data at rest using a customer-managed key.""
}
```"
A Developer has created a software package to be deployed on multiple EC2 instances using IAM roles. What actions could be performed to verify IAM access to get records from Amazon Kinesis Streams? (Select TWO),"[""Use the AWS CLI to retrieve the IAM group."", ""Query Amazon EC2 metadata for in-line IAM policies."", ""Request a token from AWS STS, and perform a describe action."", ""Perform a get action using the `--dry-run` argument."", ""Validate the IAM role policy with the IAM policy simulator.""]","[""Perform a get action using the `--dry-run` argument."", ""Validate the IAM role policy with the IAM policy simulator.""]","```json
{
    ""Use the AWS CLI to retrieve the IAM group."": ""This choice is incorrect because IAM groups are not directly related to verifying access for EC2 instances using IAM roles. IAM groups are used to manage permissions for users, not instances. The question is about verifying IAM role access for EC2 instances to interact with Amazon Kinesis Streams, which is unrelated to IAM groups."",
    ""Query Amazon EC2 metadata for in-line IAM policies."": ""This choice is incorrect because querying EC2 metadata will provide information about the IAM role attached to the instance, but it will not directly verify access to Kinesis Streams. EC2 metadata can show the IAM role name and instance profile, but it does not provide a way to test or simulate permissions."",
    ""Request a token from AWS STS, and perform a describe action."": ""This choice is incorrect because while requesting a token from AWS STS can provide temporary credentials, performing a 'describe' action does not verify access to get records from Kinesis Streams. The 'describe' action is typically used for listing resources, not for verifying specific access permissions."",
    ""Perform a get action using the `--dry-run` argument."": ""This choice is correct because using the `--dry-run` argument with a get action simulates the action without actually performing it. This can be used to verify if the necessary permissions are in place to access Kinesis Streams. If the permissions are correct, the dry run will succeed, indicating that the IAM role has the required access."",
    ""Validate the IAM role policy with the IAM policy simulator."": ""This choice is correct because the IAM policy simulator is a tool provided by AWS to test and validate IAM policies. It allows developers to simulate policy evaluation and verify if the IAM role has the necessary permissions to perform actions on Kinesis Streams. This is a best practice for ensuring that permissions are correctly configured.""
}
```"
"A company wants to implement a continuous integration for its workloads on AWS. The company wants to trigger unit test in its pipeline for commits-on its code repository, and wants to be notified of failure events in the pipeline. How can these requirements be met?","[""Store the source code in AWS CodeCommit. Create a CodePipeline to automate unit testing. Use Amazon SNS to trigger notifications of failure events."", ""Store the source code in GitHub. Create a CodePipeline to automate unit testing. Use Amazon SES to trigger notifications of failure events."", ""Store the source code on GitHub. Create a CodePipeline to automate unit testing. Use Amazon CloudWatch to trigger notifications of failure events."", ""Store the source code in AWS CodeCommit. Create a CodePipeline to automate unit testing. Use Amazon CloudWatch to trigger notification of failure events.""]","[""Store the source code in AWS CodeCommit. Create a CodePipeline to automate unit testing. Use Amazon SNS to trigger notifications of failure events.""]","```json
{
    ""Store the source code in AWS CodeCommit. Create a CodePipeline to automate unit testing. Use Amazon SNS to trigger notifications of failure events."": ""This choice is correct because AWS CodeCommit is a fully managed source control service that makes it easy to host secure and scalable Git repositories. AWS CodePipeline is a continuous integration and continuous delivery (CI/CD) service for fast and reliable application and infrastructure updates. Amazon SNS (Simple Notification Service) is a fully managed messaging service for both application-to-application (A2A) and application-to-person (A2P) communication, which can be used to send notifications about pipeline events, including failures. This setup aligns with AWS best practices for implementing CI/CD pipelines and handling notifications."",
    ""Store the source code in GitHub. Create a CodePipeline to automate unit testing. Use Amazon SES to trigger notifications of failure events."": ""This choice is incorrect because while GitHub can be used as a source repository for AWS CodePipeline, Amazon SES (Simple Email Service) is primarily used for sending email and not for triggering notifications directly from AWS services like CodePipeline. Amazon SNS is the appropriate service for sending notifications about pipeline events, including failures."",
    ""Store the source code on GitHub. Create a CodePipeline to automate unit testing. Use Amazon CloudWatch to trigger notifications of failure events."": ""This choice is partially correct. While GitHub can be integrated with AWS CodePipeline for source control and Amazon CloudWatch can be used to monitor AWS resources and applications, CloudWatch itself does not directly trigger notifications. Instead, CloudWatch Alarms can be set up to monitor metrics and then trigger actions like sending notifications through Amazon SNS. Therefore, using Amazon SNS directly for notifications is more straightforward and aligns with AWS best practices."",
    ""Store the source code in AWS CodeCommit. Create a CodePipeline to automate unit testing. Use Amazon CloudWatch to trigger notification of failure events."": ""This choice is partially correct. While AWS CodeCommit and CodePipeline are correctly used for source control and CI/CD, Amazon CloudWatch itself does not directly send notifications. CloudWatch can be used to monitor metrics and set alarms, but notifications should be sent via Amazon SNS. Therefore, using Amazon SNS for notifications is the recommended approach.""
}
```"
"An application takes 40 seconds to process instructions received in an Amazon SQS message. Assuming the SQS queue is configured with the default VisibilityTimeout value, what is the BEST way, upon receiving a message, to ensure that no other instances can retrieve a message that has already been processed or is currently being processed?","[""Use the ChangeMessageVisibility API to increase the VisibilityTimeout, then use the DeleteMessage API to delete the message."", ""Use the DeleteMessage API call to delete the message from the queue, then call DeleteQueue API to remove the queue."", ""Use the ChangeMessageVisibility API to decrease the timeout value, then use the DeleteMessage API to delete the message."", ""Use the DeleteMessageVisibility API to cancel the VisibilityTimeout, then use the DeleteMessage API to delete the message.""]","[""Use the ChangeMessageVisibility API to increase the VisibilityTimeout, then use the DeleteMessage API to delete the message.""]","```json
{
    ""Use the ChangeMessageVisibility API to increase the VisibilityTimeout, then use the DeleteMessage API to delete the message."": ""This choice is correct because the default VisibilityTimeout for an SQS queue is 30 seconds. Since the application takes 40 seconds to process a message, the VisibilityTimeout needs to be increased to at least 40 seconds to ensure that no other instances can retrieve the message while it is being processed. After processing, the DeleteMessage API is used to remove the message from the queue, which is the correct way to handle message deletion after processing."",
    ""Use the DeleteMessage API call to delete the message from the queue, then call DeleteQueue API to remove the queue."": ""This choice is incorrect because deleting the queue is not necessary or practical for processing individual messages. The DeleteMessage API is correctly used to remove a processed message, but the DeleteQueue API is used to delete the entire queue, which is not relevant or appropriate in this context."",
    ""Use the ChangeMessageVisibility API to decrease the timeout value, then use the DeleteMessage API to delete the message."": ""This choice is incorrect because decreasing the VisibilityTimeout would increase the likelihood of another instance retrieving the message before it is processed. The correct approach is to increase the VisibilityTimeout to ensure the message is not visible to other consumers while being processed."",
    ""Use the DeleteMessageVisibility API to cancel the VisibilityTimeout, then use the DeleteMessage API to delete the message."": ""This choice is incorrect because there is no DeleteMessageVisibility API in AWS SQS. The correct API to adjust the visibility timeout is ChangeMessageVisibility. Additionally, cancelling the VisibilityTimeout would not prevent other instances from retrieving the message during processing.""
}
```"
"A Developer is developing an application that manages financial transactions. To improve security, multi-factor authentication (MFA) will be required as part of the login protocol. What services can the Developer use to meet these requirements?","[""Amazon DynamoDB to store MFA session data, and Amazon SNS to send MFA codes."", ""Amazon Cognito with MFA."", ""AWS Directory Service."", ""AWS IAM with MFA enabled.""]","[""Amazon Cognito with MFA.""]","```json
{
    ""Amazon DynamoDB to store MFA session data, and Amazon SNS to send MFA codes."": ""This choice is incorrect because while Amazon DynamoDB can be used to store session data and Amazon SNS can send messages, they are not designed specifically for handling MFA in authentication workflows. Implementing MFA securely involves more than just storing session data and sending codes; it requires proper integration with authentication protocols, which is not inherently provided by this combination of services."",
    ""Amazon Cognito with MFA."": ""This choice is correct because Amazon Cognito is specifically designed to handle user authentication, including the implementation of multi-factor authentication (MFA). Cognito provides built-in support for MFA, allowing developers to easily enable and manage MFA for their applications. It integrates with various identity providers and supports different authentication flows, making it a comprehensive solution for secure user authentication. This aligns with AWS best practices for managing user identities and access securely."",
    ""AWS Directory Service."": ""This choice is incorrect because AWS Directory Service is primarily used for integrating AWS resources with on-premises Active Directory or setting up a standalone directory in the cloud. While it can be part of an authentication solution, it does not natively provide MFA capabilities for application-level authentication. It is more suitable for managing directory-based access rather than handling MFA directly."",
    ""AWS IAM with MFA enabled."": ""This choice is incorrect in the context of application-level authentication. AWS IAM is used for managing access to AWS resources and can enforce MFA for IAM users accessing AWS services. However, it is not designed for authenticating end-users of an application. For application-level MFA, services like Amazon Cognito are more appropriate as they are specifically built to handle user authentication and authorization.""
}
```"
A Developer is writing transactions into a DynamoDB table called `SystemUpdates` that has 5 write capacity units. Which option has the highest read throughput?,"[""Eventually consistent reads of 5 read capacity units reading items that are 4 KB in size."", ""Strongly consistent reads of 5 read capacity units reading items that are 4 KB in size."", ""Eventually consistent reads of 15 read capacity units reading items that are 1 KB in size."", ""Strongly consistent reads of 15 read capacity units reading items that are 1 KB in size.""]","[""Strongly consistent reads of 5 read capacity units reading items that are 4 KB in size.""]","```json
{
    ""Eventually consistent reads of 5 read capacity units reading items that are 4 KB in size."": ""This choice is incorrect because eventually consistent reads consume half the read capacity units compared to strongly consistent reads. With 5 read capacity units, eventually consistent reads can handle 10 read operations per second for 4 KB items, but this is not the highest throughput option available."",
    ""Strongly consistent reads of 5 read capacity units reading items that are 4 KB in size."": ""This choice is incorrect. Strongly consistent reads consume one read capacity unit per 4 KB item, so with 5 read capacity units, it can handle 5 read operations per second for 4 KB items. However, this is not the highest throughput option available."",
    ""Eventually consistent reads of 15 read capacity units reading items that are 1 KB in size."": ""This choice is correct. Eventually consistent reads consume half the read capacity units compared to strongly consistent reads. With 15 read capacity units and 1 KB items, it can handle 30 read operations per second, which is the highest throughput option among the choices."",
    ""Strongly consistent reads of 15 read capacity units reading items that are 1 KB in size."": ""This choice is incorrect because while it provides a high throughput, it is not as high as eventually consistent reads. Strongly consistent reads consume one read capacity unit per 4 KB item, so with 15 read capacity units for 1 KB items, it can handle 15 read operations per second, which is less than the throughput of eventually consistent reads.""
}
```"
"A Developer has created an S3 bucket` s3://mycoolapp` and has enabled server across logging that points to the folder `s3://mycoolapp/logs`. The Developer moved 100 KB of Cascading Style Sheets (CSS) documents to the folder `s3://mycoolapp/css`, and then stopped work. When the developer came back a few days later, the bucket was 50 GB. What is the MOST likely cause of this situation?","[""The CSS files were not compressed and S3 versioning was enabled."", ""S3 replication was enabled on the bucket."", ""Logging into the same bucket caused exponential log growth."", ""An S3 lifecycle policy has moved the entire CSS file to S3 Infrequent Access.""]","[""Logging into the same bucket caused exponential log growth.""]","{""The CSS files were not compressed and S3 versioning was enabled."": ""This choice is incorrect because the size increase from 100 KB to 50 GB is not explained by versioning alone, especially if no new versions of the files were uploaded. Compression of CSS files would not affect the bucket size unless multiple versions were uploaded, which is not mentioned in the scenario."", ""S3 replication was enabled on the bucket."": ""This choice is incorrect because S3 replication would require a destination bucket in a different AWS region or account, and the scenario does not mention any replication configuration or additional buckets. Replication alone would not cause such a significant increase in size unless there was a large amount of data being replicated."", ""Logging into the same bucket caused exponential log growth."": ""This choice is correct because enabling server access logging on an S3 bucket and pointing the logs to the same bucket can cause exponential growth in log files. Each access to the bucket generates a log entry, and if logs are stored in the same bucket, accessing the logs themselves generates more log entries, leading to rapid growth in storage size."", ""An S3 lifecycle policy has moved the entire CSS file to S3 Infrequent Access."": ""This choice is incorrect because moving objects to S3 Infrequent Access would not increase the bucket size. In fact, it is intended to reduce storage costs for infrequently accessed data. The scenario does not mention any lifecycle policies that would explain the increase in size.""}"
"A Developer is testing a Docker-based application that uses the AWS SDK to interact with Amazon DynamoDB. In the local development environment, the application has used IAM access keys. The application is now ready for deployment onto an ECS cluster. How should the application authenticate with AWS services in production?","[""Configure an ECS task IAM role for the application to use."", ""Refactor the application to call AWS STS AssumeRole based on an instance role."", ""Configure AWS access key/secret access key environment variables with new credentials."", ""Configure the credentials file with a new access key/secret access key.""]","[""Configure an ECS task IAM role for the application to use.""]","```json
{
    ""Configure an ECS task IAM role for the application to use."": ""This is the correct choice because using an ECS task IAM role is the recommended way to provide credentials to applications running in ECS. This allows the application to securely access AWS services without hardcoding credentials or managing them manually. The ECS task IAM role provides temporary credentials that are automatically rotated, enhancing security. According to AWS best practices, using IAM roles for tasks is preferred over embedding access keys in your application."",
    ""Refactor the application to call AWS STS AssumeRole based on an instance role."": ""This option is incorrect because it introduces unnecessary complexity. While STS AssumeRole can be used to obtain temporary credentials, it is not needed in this scenario. ECS already provides a built-in mechanism to assign IAM roles to tasks, which simplifies the process and aligns with AWS best practices for managing credentials in containerized environments."",
    ""Configure AWS access key/secret access key environment variables with new credentials."": ""This option is incorrect because it involves manually managing credentials, which is not recommended. Storing access keys in environment variables can lead to security vulnerabilities, such as accidental exposure in logs or version control. AWS best practices advise against hardcoding credentials and recommend using IAM roles for secure and automatic credential management."",
    ""Configure the credentials file with a new access key/secret access key."": ""This option is incorrect because it involves storing credentials in a file, which poses security risks similar to storing them in environment variables. Managing credentials manually is error-prone and increases the risk of credential leakage. AWS recommends using IAM roles for tasks to handle credentials securely and automatically.""
}
```"
A company is using AWS CodeBuild to compile a website from source code stored in AWS CodeCommit. A recent change to the source code has resulted in the CodeBuild project being unable to successfully compile the website. How should the Developer identify the cause of the failures?,"[""Modify the `buildspec.yml` file to include steps to send the output of build commands to Amazon CloudWatch."", ""Use a custom Docker image that includes the AWS X-Ray agent in the AWS CodeBuild project configuration."", ""Check the build logs of the failed phase in the last build attempt in the AWS CodeBuild project build history."", ""Manually re-run the build process on a local machine so that the output can be visualized.""]","[""Check the build logs of the failed phase in the last build attempt in the AWS CodeBuild project build history.""]","```json
{
    ""Modify the `buildspec.yml` file to include steps to send the output of build commands to Amazon CloudWatch."": ""This choice is incorrect because AWS CodeBuild automatically sends build logs to Amazon CloudWatch Logs by default. There is no need to modify the `buildspec.yml` file specifically for this purpose. The build logs can be accessed directly from the AWS CodeBuild console, making this step unnecessary."",
    ""Use a custom Docker image that includes the AWS X-Ray agent in the AWS CodeBuild project configuration."": ""This choice is incorrect because AWS X-Ray is used for tracing requests through an application, not for debugging build failures. The issue at hand is related to build failures, which are best diagnosed by examining build logs, not by using X-Ray."",
    ""Check the build logs of the failed phase in the last build attempt in the AWS CodeBuild project build history."": ""This choice is correct because AWS CodeBuild provides detailed logs for each build phase, which are crucial for diagnosing build failures. By checking the build logs, a developer can identify exactly where and why the build is failing. This is a best practice for troubleshooting build issues in AWS CodeBuild."",
    ""Manually re-run the build process on a local machine so that the output can be visualized."": ""This choice is incorrect because running the build locally may not replicate the exact environment and conditions of the AWS CodeBuild environment. Additionally, it is inefficient compared to using the build logs provided by AWS CodeBuild, which are designed to help diagnose such issues directly within the AWS environment.""
}
```"
"For a deployment using AWS CodeDeploy, what is the run order of the hooks for in-place deployments?","[""Before Install -> Application Stop -> Application Start -> After Install."", ""Application Stop -> Before Install -> After Install -> Application Start."", ""Before Install -> Application Stop -> Validate Service -> Application Start."", ""Application Stop -> Before Install -> Validate Service -> Application Start.""]","[""Application Stop -> Before Install -> After Install -> Application Start.""]","```json
{
    ""Before Install -> Application Stop -> Application Start -> After Install."": ""This sequence is incorrect because it does not follow the correct order of lifecycle hooks for an in-place deployment in AWS CodeDeploy. The 'Application Stop' hook should occur before 'Before Install' to ensure that the application is stopped before any installation begins. 'Application Start' should occur after 'After Install' to ensure that the application is started after the installation is complete."",
    ""Application Stop -> Before Install -> After Install -> Application Start."": ""This sequence is correct according to AWS CodeDeploy's lifecycle hooks for in-place deployments. The process begins with 'Application Stop' to halt the current application, followed by 'Before Install' to prepare the instance for the new deployment. 'After Install' ensures that any post-installation tasks are completed before 'Application Start' restarts the application. This order ensures a smooth and error-free deployment process."",
    ""Before Install -> Application Stop -> Validate Service -> Application Start."": ""This sequence is incorrect because it misplaces the 'Application Stop' hook. 'Before Install' should not occur before 'Application Stop' as the application needs to be stopped before any installation preparations. Additionally, 'Validate Service' is not a standard hook in the in-place deployment lifecycle. The correct order should include 'After Install' before 'Application Start'."",
    ""Application Stop -> Before Install -> Validate Service -> Application Start."": ""This sequence is incorrect because it includes 'Validate Service', which is not a standard lifecycle hook in the in-place deployment process. The correct sequence should include 'After Install' instead of 'Validate Service', ensuring that all necessary post-installation steps are completed before restarting the application.""
}
```"
A Developer executed a AWS CLI command and received the error shown below. What action should the Developer perform to make this error human-readable?,"[""Make a call to AWS KMS to decode the message."", ""Use the AWS STS `decode-authorization-message` API to decode the message."", ""Use an open source decoding library to decode the message."", ""Use the AWS IAM `decode-authorization-message` API to decode this message.""]","[""Use the AWS STS `decode-authorization-message` API to decode the message.""]","{""Make a call to AWS KMS to decode the message."": ""Incorrect. AWS KMS (Key Management Service) is used for encryption and decryption of data using cryptographic keys. It is not used for decoding authorization messages from AWS CLI errors. The error message in question is related to AWS IAM permissions, not encrypted data."", ""Use the AWS STS `decode-authorization-message` API to decode the message."": ""Correct. AWS STS (Security Token Service) provides the `decode-authorization-message` API, which is specifically designed to decode authorization failure messages. This API helps developers understand the permissions issues that caused the error, making it human-readable. This is a common practice when troubleshooting IAM permission errors."", ""Use an open source decoding library to decode the message."": ""Incorrect. Open source decoding libraries are not designed to interpret AWS-specific authorization messages. These messages are encoded in a format that requires AWS-specific tools, such as the AWS STS `decode-authorization-message` API, to decode."", ""Use the AWS IAM `decode-authorization-message` API to decode this message."": ""Incorrect. While the message pertains to IAM permissions, there is no `decode-authorization-message` API in AWS IAM. The correct service to use is AWS STS, which provides the specific API for decoding authorization messages.""}"
"A Developer uses AWS CodeDeploy to automate application deployment that connects to an external MySQL database. The Developer wants to securely access the encrypted secrets, such as API keys and database passwords. Which of the following solutions would involve the LEAST administrative effort?","[""Save the secrets in Amazon S3 with AWS KMS server-side encryption, and use a signed URL to access them by using the IAM role from Amazon EC2 instances."", ""Use the instance metadata to store the secrets and to programmatically access the secrets from EC2 instances."", ""Use the Amazon DynamoDB client-side encryption library to save the secrets in DynamoDB and to programmatically access the secrets from EC2 instances."", ""Use AWS SSM Parameter Store to store the secrets and to programmatically access them by using the IAM role from EC2 instances.""]","[""Use AWS SSM Parameter Store to store the secrets and to programmatically access them by using the IAM role from EC2 instances.""]","```json
{
    ""Save the secrets in Amazon S3 with AWS KMS server-side encryption, and use a signed URL to access them by using the IAM role from Amazon EC2 instances."": ""This option is incorrect because while Amazon S3 with AWS KMS can securely store secrets, it involves more administrative effort to manage signed URLs and ensure secure access. Additionally, S3 is not specifically designed for secrets management, which can lead to increased complexity and potential security risks."",
    ""Use the instance metadata to store the secrets and to programmatically access the secrets from EC2 instances."": ""This option is incorrect because instance metadata is not intended for storing sensitive information like secrets. Instance metadata is accessible without authentication from the instance itself, which poses a significant security risk. AWS best practices recommend using dedicated secrets management services for handling sensitive data."",
    ""Use the Amazon DynamoDB client-side encryption library to save the secrets in DynamoDB and to programmatically access the secrets from EC2 instances."": ""This option is incorrect because while DynamoDB can be used to store encrypted data, it requires additional client-side encryption logic and key management, which increases administrative overhead. DynamoDB is not optimized for secrets management, making it less suitable compared to services specifically designed for this purpose."",
    ""Use AWS SSM Parameter Store to store the secrets and to programmatically access them by using the IAM role from EC2 instances."": ""This option is correct because AWS SSM Parameter Store is specifically designed for storing and managing configuration data and secrets securely. It integrates with AWS IAM for access control, reducing administrative effort. Parameter Store supports encryption with AWS KMS and allows for easy programmatic access from EC2 instances, making it a best practice for secrets management.""
}
```"
An application stops working with the following error: `The specified bucket does not exist`. Where is the BEST place to start the root cause analysis?,"[""Check the Elastic Load Balancer logs for DeleteBucket requests."", ""Check the application logs in Amazon CloudWatch Logs for Amazon S3 DeleteBucket errors."", ""Check AWS X-Ray for Amazon S3 DeleteBucket alarms."", ""Check AWS CloudTrail for a DeleteBucket event.""]","[""Check AWS CloudTrail for a DeleteBucket event.""]","```json
{
    ""Check the Elastic Load Balancer logs for DeleteBucket requests."": ""Elastic Load Balancer logs are primarily used for monitoring and troubleshooting issues related to load balancing, such as request routing and response times. They do not log S3 operations like DeleteBucket requests. Therefore, this choice is incorrect because it does not relate to the specific issue of a bucket being deleted in Amazon S3."",
    ""Check the application logs in Amazon CloudWatch Logs for Amazon S3 DeleteBucket errors."": ""While application logs in CloudWatch Logs can provide insights into application-level errors, they may not capture specific AWS service-level events like DeleteBucket operations unless explicitly logged by the application. This choice is incorrect because it assumes the application logs such events, which is not guaranteed without specific implementation."",
    ""Check AWS X-Ray for Amazon S3 DeleteBucket alarms."": ""AWS X-Ray is used for tracing requests through an application and visualizing the application architecture. It does not natively monitor or log S3 operations such as DeleteBucket events. This choice is incorrect because AWS X-Ray is not designed to track S3 bucket deletions."",
    ""Check AWS CloudTrail for a DeleteBucket event."": ""AWS CloudTrail is the correct choice because it provides a history of AWS API calls for your account, including S3 operations like DeleteBucket. CloudTrail logs are the best source for tracking changes to AWS resources, making it the best place to start root cause analysis for the specified error. This aligns with AWS best practices for auditing and monitoring AWS resource changes.""
}
```"
A Developer will be using the AWS CLI on a local development server to manage AWS services. What can be done to ensure that the CLI uses the Developer's IAM permissions when making commands?,"[""Specify the Developer's IAM access key ID and secret access key as parameters for each CLI command."", ""Run the `aws configure` CLI command, and provide the Developer's IAM access key ID and secret access key."", ""Specify the Developer's IAM user name and password as parameters for each CLI command."", ""Use the Developer's IAM role when making the CLI command.""]","[""Run the `aws configure` CLI command, and provide the Developer's IAM access key ID and secret access key.""]","```json
{
    ""Specify the Developer's IAM access key ID and secret access key as parameters for each CLI command."": ""This choice is incorrect because while it is technically possible to specify the access key ID and secret access key as parameters for each CLI command, it is not practical or secure. This method would require the developer to repeatedly enter sensitive information, increasing the risk of exposure. It also goes against AWS best practices, which recommend using the AWS CLI configuration file to securely store credentials."",
    ""Run the `aws configure` CLI command, and provide the Developer's IAM access key ID and secret access key."": ""This choice is correct because the `aws configure` command is the recommended way to set up the AWS CLI with a user's credentials. It securely stores the IAM access key ID and secret access key in the AWS CLI configuration file, allowing the CLI to automatically use these credentials for subsequent commands. This method adheres to AWS best practices for managing credentials securely and efficiently."",
    ""Specify the Developer's IAM user name and password as parameters for each CLI command."": ""This choice is incorrect because the AWS CLI does not use IAM user names and passwords for authentication. Instead, it uses access key IDs and secret access keys. IAM user names and passwords are used for AWS Management Console access, not for programmatic access via the CLI."",
    ""Use the Developer's IAM role when making the CLI command."": ""This choice is incorrect in the context of using the AWS CLI on a local development server. IAM roles are typically used for granting permissions to AWS resources and services, not for direct CLI authentication. While roles can be assumed programmatically, the initial authentication still requires valid credentials, such as an access key ID and secret access key, which are set up using the `aws configure` command.""
}
```"
An application stores images in an S3 bucket. Amazon S3 event notifications are used to trigger a Lambda function that resizes the images. Processing each image takes less than a second. How will AWS Lambda handle the additional traffic?,"[""Lambda will scale out to execute the requests concurrently."", ""Lambda will handle the requests sequentially in the order received."", ""Lambda will process multiple images in a single execution."", ""Lambda will add more compute to each execution to reduce processing time.""]","[""Lambda will scale out to execute the requests concurrently.""]","```json
{
    ""Lambda will scale out to execute the requests concurrently."": ""This choice is correct because AWS Lambda is designed to automatically scale out to handle incoming requests concurrently. When an S3 event notification triggers a Lambda function, Lambda can invoke multiple instances of the function in parallel, each handling a separate event. This is a key feature of Lambda's serverless architecture, allowing it to handle high volumes of requests efficiently without manual intervention. According to AWS best practices, Lambda can scale to handle thousands of concurrent executions, making it well-suited for tasks like processing images in parallel."",
    ""Lambda will handle the requests sequentially in the order received."": ""This choice is incorrect because AWS Lambda does not process requests sequentially by default. Instead, Lambda is designed to handle multiple requests concurrently, as long as the concurrency limits are not exceeded. Sequential processing would be inefficient for high-volume, parallelizable tasks like image processing, and it is not how Lambda is typically configured to operate."",
    ""Lambda will process multiple images in a single execution."": ""This choice is incorrect because each Lambda invocation typically processes a single event or request. While it is technically possible to batch process multiple images in a single execution by configuring the event source or the function logic, this is not the default behavior for S3 event notifications. Each image upload to S3 typically triggers a separate Lambda invocation."",
    ""Lambda will add more compute to each execution to reduce processing time."": ""This choice is incorrect because AWS Lambda does not dynamically add more compute resources to a single execution to reduce processing time. Instead, Lambda scales by increasing the number of concurrent executions. The compute resources (memory and CPU) for each execution are determined by the memory allocation setting for the function, which can be adjusted, but this does not dynamically change during execution.""
}
```"
"A company is building a stock trading application that requires sub-millisecond latency in processing trading requests. Amazon DynamoDB is used to store all the trading data that is used to process each request. After load testing the application, the development team found that due to data retrieval times, the latency requirement is not satisfied. Because of sudden high spikes in the number of requests, DynamoDB read capacity has to be significantly over-provisioned to avoid throttling. What steps should be taken to meet latency requirements and reduce the cost of running the application?","[""Add Global Secondary Indexes for trading data."", ""Store trading data in Amazon S3 and use Transfer Acceleration."", ""Add retries with exponential back-off for DynamoDB queries."", ""Use DynamoDB Accelerator to cache trading data.""]","[""Add Global Secondary Indexes for trading data.""]","```json
{
    ""Add Global Secondary Indexes for trading data."": ""This choice is incorrect. Global Secondary Indexes (GSIs) in DynamoDB allow for querying on non-primary key attributes, which can improve query flexibility but do not inherently reduce latency or address over-provisioning issues. The problem described is related to latency and over-provisioning due to high request spikes, which GSIs do not directly solve. GSIs can also add complexity and cost if not managed properly."",
    ""Store trading data in Amazon S3 and use Transfer Acceleration."": ""This choice is incorrect. Amazon S3 is not suitable for use cases requiring sub-millisecond latency, as it is designed for high durability and scalability rather than low-latency access. Transfer Acceleration is used to speed up uploads to S3 over long distances, not to reduce data retrieval latency. This does not address the latency or over-provisioning issues with DynamoDB."",
    ""Add retries with exponential back-off for DynamoDB queries."": ""This choice is incorrect. While adding retries with exponential back-off is a best practice for handling transient errors and throttling in DynamoDB, it does not address the root cause of high latency or the need for over-provisioning. It is a mitigation strategy for handling errors rather than a solution for reducing latency."",
    ""Use DynamoDB Accelerator to cache trading data."": ""This choice is correct. DynamoDB Accelerator (DAX) is an in-memory caching service that can significantly reduce read latency from milliseconds to microseconds, which is suitable for applications requiring sub-millisecond latency. DAX helps reduce the load on DynamoDB by caching frequently accessed data, which can also reduce the need for over-provisioning read capacity. This aligns with AWS best practices for optimizing performance and cost in high-traffic applications.""
}
```"
"A Developer created a Lambda function for a web application backend. When testing the Lambda function from the AWS Lambda console, the Developer can see that the function is being executed, but there is no log data being generated in Amazon CloudWatch Logs, even after several minutes. What could cause this situation?","[""The Lambda function does not have any explicit log statements for the log data to send it to CloudWatch Logs."", ""The Lambda function is missing CloudWatch Logs as a source trigger to send log data."", ""The execution role for the Lambda function is missing permissions to write log data to the CloudWatch Logs."", ""The Lambda function is missing a target CloudWatch Log group.""]","[""The execution role for the Lambda function is missing permissions to write log data to the CloudWatch Logs.""]","```json
{
    ""The Lambda function does not have any explicit log statements for the log data to send it to CloudWatch Logs."": ""This choice is incorrect because AWS Lambda automatically generates log entries for each invocation, including start and end logs, even if the function code does not contain explicit log statements. Therefore, the absence of explicit log statements in the code does not prevent logs from being sent to CloudWatch Logs."",
    ""The Lambda function is missing CloudWatch Logs as a source trigger to send log data."": ""This choice is incorrect because CloudWatch Logs is not a source trigger for Lambda functions. Instead, Lambda automatically sends logs to CloudWatch Logs without needing it to be configured as a trigger. The issue is more likely related to permissions rather than configuration of triggers."",
    ""The execution role for the Lambda function is missing permissions to write log data to the CloudWatch Logs."": ""This choice is correct. For a Lambda function to send logs to CloudWatch Logs, its execution role must have the necessary permissions, specifically the 'AWSLambdaBasicExecutionRole' policy, which includes permissions to create log groups and write log streams. If these permissions are missing, logs will not appear in CloudWatch Logs."",
    ""The Lambda function is missing a target CloudWatch Log group."": ""This choice is incorrect because AWS Lambda automatically creates a CloudWatch Log group for the function if it doesn't already exist. Therefore, the absence of a target log group is not a reason for the lack of log data in CloudWatch Logs.""
}
```"
"A Developer wants to use AWS X-Ray to trace a user request end-to-end throughput the software stack. The Developer made the necessary changes in the application tested it, and found that the application is able to send the traces to AWS X-Ray. However, when the application is deployed to an EC2 instance, the traces are not availableWhich of the following could create this situation? (Choose TWO)","[""The traces are reaching X-Ray, but the Developer does not have access to view the records."", ""The X-Ray daemon is not installed on the EC2 instance."", ""The X-Ray endpoint specified in the application configuration is incorrect."", ""The instance role does not have `xray:BatchGetTraces` and `xray:GetTraceGraph` permissions.The instance role does not have `xray:PutTraceSegments` and `xray:PutTelemetryRecords` permissions."", ""The instance role does not have `xray:PutTraceSegments` and `xray:PutTelemetryRecords` permissions.""]","[""The X-Ray daemon is not installed on the EC2 instance."", ""The instance role does not have `xray:PutTraceSegments` and `xray:PutTelemetryRecords` permissions.""]","```json
{
    ""The traces are reaching X-Ray, but the Developer does not have access to view the records."": ""This choice is incorrect because the issue described is about traces not being available at all, not about access permissions to view existing traces. If traces were reaching X-Ray, they would be visible to someone with the correct permissions, but the problem is that traces are not being sent from the EC2 instance."",
    ""The X-Ray daemon is not installed on the EC2 instance."": ""This choice is correct because the X-Ray daemon is responsible for sending trace data from the application running on the EC2 instance to the X-Ray service. Without the daemon installed and running, traces will not be sent to X-Ray, which explains why they are not available."",
    ""The X-Ray endpoint specified in the application configuration is incorrect."": ""This choice is incorrect because the question specifies that the application was tested and able to send traces to X-Ray, indicating that the endpoint configuration was correct during testing. The problem arises only when deployed to the EC2 instance, suggesting a different issue."",
    ""The instance role does not have `xray:BatchGetTraces` and `xray:GetTraceGraph` permissions."": ""This choice is incorrect because these permissions are related to retrieving trace data from X-Ray, not sending it. The problem is with traces not being sent to X-Ray, which requires different permissions (`xray:PutTraceSegments` and `xray:PutTelemetryRecords`)."",
    ""The instance role does not have `xray:PutTraceSegments` and `xray:PutTelemetryRecords` permissions."": ""This choice is correct because these permissions are necessary for the EC2 instance to send trace data and telemetry records to X-Ray. Without these permissions, the X-Ray daemon cannot send data, resulting in traces not being available.""
}
```"
An application has hundreds of users. Each user may use multiple devices to access the application. The Developer wants to assign unique identifiers to these users regardless of the device they use. Which of the following methods should be used to obtain unique identifiers?,"[""Create a user table in Amazon DynamoDB as key-value pairs of users and their devices. Use these keys as unique identifiers."", ""Use IAM-generated access key IDs for the users as the unique identifier, but do not store secret keys."", ""Implement developer-authenticated identities by using Amazon Cognito, and get credentials for these identities."", ""Assign IAM users and roles to the users. Use the unique IAM resource ID as the unique identifier.""]","[""Assign IAM users and roles to the users. Use the unique IAM resource ID as the unique identifier.""]","{""Create a user table in Amazon DynamoDB as key-value pairs of users and their devices. Use these keys as unique identifiers."": ""This choice is incorrect because while DynamoDB can store user data, it does not inherently provide a mechanism for generating unique identifiers across multiple devices for the same user. DynamoDB is a NoSQL database service that is suitable for storing structured data, but it does not solve the problem of uniquely identifying users across devices without additional logic implemented by the developer."", ""Use IAM-generated access key IDs for the users as the unique identifier, but do not store secret keys."": ""This choice is incorrect because IAM access key IDs are intended for programmatic access to AWS services and are not designed to serve as unique identifiers for application users. Additionally, IAM is not meant to manage individual application users, especially in high-volume scenarios like hundreds of users with multiple devices. Using IAM in this way would not follow best practices for user management."", ""Implement developer-authenticated identities by using Amazon Cognito, and get credentials for these identities."": ""This choice is correct because Amazon Cognito is designed to manage user identities and provides a scalable solution for handling user authentication and authorization across multiple devices. Cognito allows developers to create unique identifiers for users through developer-authenticated identities, making it ideal for applications with users accessing from multiple devices. This aligns with AWS best practices for user management."", ""Assign IAM users and roles to the users. Use the unique IAM resource ID as the unique identifier."": ""This choice is incorrect because IAM users and roles are intended for managing AWS resources and permissions, not for managing application users. Assigning IAM users and roles to each application user is not scalable or practical for applications with hundreds of users, especially when each user may access the application from multiple devices. This approach does not align with AWS best practices for managing application user identities.""}"
What are the steps to using the AWS CLI to launch a templatized serverless application?,"[""Use AWS CloudFormation get-template then CloudFormation execute-change-set."", ""Use AWS CloudFormation validate-template then CloudFormation create-change-set."", ""Use AWS CloudFormation package then CloudFormation deploy."", ""Use AWS CloudFormation create-stack then CloudFormation update-stack.""]","[""Use AWS CloudFormation package then CloudFormation deploy.""]","```json
{
    ""Use AWS CloudFormation get-template then CloudFormation execute-change-set."": ""This choice is incorrect because 'get-template' is used to retrieve the template body for an existing stack, not for launching a new application. 'execute-change-set' is used to apply changes to an existing stack based on a change set, not for initially deploying a serverless application. This does not align with the process of packaging and deploying a serverless application using AWS CLI."",
    ""Use AWS CloudFormation validate-template then CloudFormation create-change-set."": ""This choice is incorrect because 'validate-template' is used to check the syntax of a CloudFormation template, and 'create-change-set' is used to propose changes to an existing stack. While these commands are useful for ensuring template correctness and planning updates, they are not part of the typical workflow for launching a new serverless application from a template."",
    ""Use AWS CloudFormation package then CloudFormation deploy."": ""This choice is correct because 'package' is used to package local artifacts (such as code for Lambda functions) and upload them to an S3 bucket, replacing local references with S3 URLs in the template. 'deploy' is then used to create or update the stack with the packaged template. This is the recommended process for deploying serverless applications, as it handles the packaging of code and deployment in one streamlined workflow."",
    ""Use AWS CloudFormation create-stack then CloudFormation update-stack."": ""This choice is incorrect because 'create-stack' and 'update-stack' are used for directly creating and updating stacks without the intermediate packaging step. While these commands are part of the stack management process, they do not handle the packaging of serverless application code, which is necessary for deploying applications that include local artifacts.""
}
```"
"A deployment package uses the AWS CLI to copy files into any S3 bucket in the account, using access keys stored in environment variables. The package is running on EC2 instances, and the instances have been modified to run with an assumed IAM role and a more restrictive policy that allows access to only one bucket. After the change, the Developer logs into the host and still has the ability to write into all of the S3 buckets in that account. What is the MOST likely cause of this situation?","[""An IAM inline policy is being used on the IAM role."", ""An IAM managed policy is being used on the IAM role."", ""The AWS CLI is corrupt and needs to be reinstalled."", ""The AWS credential provider looks for instance profile credentials last.""]","[""An IAM managed policy is being used on the IAM role.""]","```json
{
    ""An IAM inline policy is being used on the IAM role."": ""This choice is incorrect because an inline policy would be directly attached to the IAM role and would override any other policies if it were more permissive. However, the problem states that the role has a restrictive policy, so an inline policy being used would not explain the ability to access all buckets."",
    ""An IAM managed policy is being used on the IAM role."": ""This choice is correct because managed policies can be attached to multiple roles and might not have been updated to reflect the new restrictive access requirements. If the managed policy allows access to all S3 buckets, it would override the more restrictive inline policy or instance profile, explaining why the developer can still write to all buckets."",
    ""The AWS CLI is corrupt and needs to be reinstalled."": ""This choice is incorrect because corruption of the AWS CLI would not cause it to bypass IAM permissions. The CLI simply uses the credentials and permissions provided to it, and corruption would more likely result in errors or inability to execute commands rather than incorrect permissions."",
    ""The AWS credential provider looks for instance profile credentials last."": ""This choice is incorrect because the AWS CLI uses a specific order for credential resolution, where instance profile credentials are prioritized over environment variables if they are present. If the instance profile was correctly configured, it should override the environment variables, not the other way around.""
}
```"
"An application overwrites an object in Amazon S3, and then immediately reads the same object. Why would the application sometimes retrieve the old version of the object?","[""S3 overwrite PUTS are eventually consistent, so the application may read the old object."", ""The application needs to add extra metadata to label the latest version when uploading to Amazon S3."", ""All S3 PUTS are eventually consistent, so the application may read the old object."", ""The application needs to explicitly specify latest version when retrieving the object.""]","[""S3 overwrite PUTS are eventually consistent, so the application may read the old object.""]","```json
{
    ""S3 overwrite PUTS are eventually consistent, so the application may read the old object."": ""This choice is correct. Amazon S3 provides eventual consistency for overwrite PUTS and DELETES in all regions. This means that when an object is overwritten, there may be a delay before the change is reflected in all regions, leading to the possibility of reading the old version of the object immediately after an overwrite. This behavior is well-documented in AWS's consistency model for S3."",
    ""The application needs to add extra metadata to label the latest version when uploading to Amazon S3."": ""This choice is incorrect. Adding metadata does not affect the consistency model of Amazon S3. Metadata is used for storing additional information about the object but does not influence how S3 handles object versioning or consistency. The issue of reading an old version is related to the eventual consistency model, not metadata."",
    ""All S3 PUTS are eventually consistent, so the application may read the old object."": ""This choice is incorrect. While overwrite PUTS are eventually consistent, Amazon S3 provides read-after-write consistency for PUTS of new objects in all regions. This means that when a new object is uploaded, it is immediately available for read operations. Therefore, not all PUTS are eventually consistent, only overwrite PUTS are."",
    ""The application needs to explicitly specify latest version when retrieving the object."": ""This choice is incorrect. While versioning can be enabled in S3 to keep multiple versions of an object, the issue in the question is related to eventual consistency, not versioning. Even if versioning is enabled, the eventual consistency model still applies to overwrite PUTS. Specifying a version is only relevant if versioning is enabled and multiple versions exist.""
}
```"
"An application under development is required to store hundreds of video files. The data must be encrypted within the application prior to storage, with a unique key for each video file. How should the Developer code the application?","[""Use the KMS Encrypt API to encrypt the data. Store the encrypted data key and data."", ""Use a cryptography library to generate an encryption key for the application. Use the encryption key to encrypt the data. Store the encrypted data."", ""Use the KMS GenerateDataKey API to get a data key. Encrypt the data with the data key. Store the encrypted data key and data."", ""Upload the data to an S3 bucket using server side-encryption with an AWS KMS key.""]","[""Use the KMS GenerateDataKey API to get a data key. Encrypt the data with the data key. Store the encrypted data key and data.""]","```json
{
    ""Use the KMS Encrypt API to encrypt the data. Store the encrypted data key and data."": ""This choice is incorrect because the KMS Encrypt API is used for encrypting small amounts of data directly, typically up to 4 KB. It is not suitable for encrypting large files such as video files. Instead, AWS recommends using the GenerateDataKey API to obtain a data key for encrypting larger data, which is the correct approach for this scenario."",
    ""Use a cryptography library to generate an encryption key for the application. Use the encryption key to encrypt the data. Store the encrypted data."": ""This choice is incorrect because while it is possible to use a cryptography library to generate encryption keys, it does not leverage AWS KMS for key management, which provides additional security and management features. Using KMS ensures that keys are securely managed, rotated, and audited, which is a best practice in AWS environments."",
    ""Use the KMS GenerateDataKey API to get a data key. Encrypt the data with the data key. Store the encrypted data key and data."": ""This choice is correct because the KMS GenerateDataKey API is designed for scenarios where you need to encrypt large amounts of data. It provides a data key that can be used to encrypt the data, and the encrypted data key can be stored alongside the encrypted data. This approach leverages AWS KMS for secure key management and is a best practice for encrypting large files like video files."",
    ""Upload the data to an S3 bucket using server side-encryption with an AWS KMS key."": ""This choice is incorrect for the given scenario because it describes server-side encryption, where AWS manages the encryption and decryption process. The question specifies that the data must be encrypted within the application prior to storage, which requires client-side encryption. Server-side encryption with S3 does not meet the requirement of encrypting data within the application.""
}
```"
"A developer is testing an application that invokes an AWS Lambda function asynchronously. During the testing phase, the Lambda function fails to process after two retries. How can the developer troubleshoot the failure?","[""Configure AWS CloudTrail logging to investigate the invocation failures."", ""Configure Dead Letter Queues by sending events to Amazon SQS for investigatio."", ""Configure Amazon Simple Workflow Service to process any direct unprocessed events."", ""Configure AWS Config to process any direct unprocessed events.""]","[""Configure Dead Letter Queues by sending events to Amazon SQS for investigatio.""]","```json
{
    ""Configure AWS CloudTrail logging to investigate the invocation failures."": ""Incorrect. AWS CloudTrail is used to log API calls made within an AWS account, which can help track who made changes to resources. However, it is not designed to directly troubleshoot Lambda invocation failures or provide insights into why a Lambda function failed. It does not capture the payload or the execution details of a Lambda function."",
    ""Configure Dead Letter Queues by sending events to Amazon SQS for investigatio."": ""Correct. Dead Letter Queues (DLQs) are a best practice for handling failed asynchronous invocations in AWS Lambda. When a Lambda function fails after retries, the event can be sent to an Amazon SQS queue or an Amazon SNS topic for further analysis. This allows developers to inspect the payload that caused the failure and troubleshoot the issue. DLQs provide a reliable way to capture and analyze failed events, which is essential for debugging and improving application resilience."",
    ""Configure Amazon Simple Workflow Service to process any direct unprocessed events."": ""Incorrect. Amazon Simple Workflow Service (SWF) is used for building applications that coordinate work across distributed components, often for complex workflows. It is not used for handling failed Lambda invocations or for processing unprocessed events directly from Lambda. SWF is more suited for orchestrating tasks and managing state rather than troubleshooting Lambda failures."",
    ""Configure AWS Config to process any direct unprocessed events."": ""Incorrect. AWS Config is a service that enables assessment, auditing, and evaluation of the configurations of AWS resources. It is used for compliance and security monitoring, not for processing or troubleshooting unprocessed events from AWS Lambda. AWS Config does not handle event processing or provide insights into Lambda function execution failures.""
}
```"
A developer is setting up Amazon API Gateway for their company's products. The API will be used by registered developers to query and update their environments. The company wants to limit the amount of requests end users can send for both cost and security reasons. Management wants to offer registered developers the option of buying larger packages that allow for more requests. How can the developer accomplish this with the LEAST amount of overhead management?,"[""Enable throttling for the API Gateway stage. Set a value for both the rate and burst capacity. If a registered user chooses a larger package, create a stage for them, adjust the values, and share the new URL with them."", ""Set up Amazon CloudWatch API logging in API Gateway. Create a filter based on the user and requestTime fields and create an alarm on this filter. Write an AWS Lambda function to analyze the values and requester information, and respond accordingly. Set up the function as the target for the alarm. If a registered user chooses a larger package, update the Lambda code with the values."", ""Enable Amazon CloudWatch metrics for the API Gateway stage. Set up CloudWatch alarms based off the Count metric and the ApiName, Method, Resource, and Stage dimensions to alerts when request rates pass the threshold. Set the alarm action to `Deny`. If a registered user chooses a larger package, create a user-specific alarm and adjust the values."", ""Set up a default usage plan, specify values for the rate and burst capacity, and associate it with a stage. If a registered user chooses a larger package, create a custom plan with the appropriate values and associate the plan with the user.""]","[""Set up a default usage plan, specify values for the rate and burst capacity, and associate it with a stage. If a registered user chooses a larger package, create a custom plan with the appropriate values and associate the plan with the user.""]","```json
{
    ""Enable throttling for the API Gateway stage. Set a value for both the rate and burst capacity. If a registered user chooses a larger package, create a stage for them, adjust the values, and share the new URL with them."": ""This approach is incorrect because it involves creating separate stages for each user who wants a different package, which is inefficient and increases management overhead. Each stage would require its own URL, complicating the management and usage of the API. AWS best practices recommend using usage plans to manage throttling and quota limits per user or API key, which is more scalable and manageable."",
    ""Set up Amazon CloudWatch API logging in API Gateway. Create a filter based on the user and requestTime fields and create an alarm on this filter. Write an AWS Lambda function to analyze the values and requester information, and respond accordingly. Set up the function as the target for the alarm. If a registered user chooses a larger package, update the Lambda code with the values."": ""This method is incorrect due to its complexity and high management overhead. It involves setting up CloudWatch logging, creating filters and alarms, and writing a Lambda function to handle request limits, which is not the intended use of these services. This approach is not efficient for managing API request limits and does not leverage the built-in capabilities of API Gateway usage plans."",
    ""Enable Amazon CloudWatch metrics for the API Gateway stage. Set up CloudWatch alarms based off the Count metric and the ApiName, Method, Resource, and Stage dimensions to alerts when request rates pass the threshold. Set the alarm action to `Deny`. If a registered user chooses a larger package, create a user-specific alarm and adjust the values."": ""This option is incorrect because it relies on CloudWatch alarms to manage request limits, which is not the most efficient or intended use of CloudWatch. While CloudWatch can monitor API Gateway metrics, using it to enforce request limits with alarms and deny actions is cumbersome and not as straightforward as using API Gateway's built-in usage plans. Usage plans are specifically designed to handle such scenarios with less overhead."",
    ""Set up a default usage plan, specify values for the rate and burst capacity, and associate it with a stage. If a registered user chooses a larger package, create a custom plan with the appropriate values and associate the plan with the user."": ""This is the correct answer because API Gateway usage plans are specifically designed to manage request throttling and quotas for different users or API keys. They allow developers to set default limits and create custom plans for users who need higher limits, all while keeping management overhead low. Usage plans can be easily associated with API keys, making it simple to manage different request limits for different users. This approach aligns with AWS best practices and is supported by AWS documentation.""
}
```"
A developer is refactoring a monolithic application. The application takes a POST request and performs several operations. Some of the operations are in parallel while others run sequentially. These operations have been refactored into individual AWS Lambda functions. The POST request will be processed by Amazon API Gateway. How should the developer invoke the Lambda functions in the same sequence using API Gateway?,"[""Use Amazon SQS to invoke the Lambda functions."", ""Use an AWS Step Functions activity to run the Lambda functions."", ""Use Amazon SNS to trigger the Lambda functions."", ""Use an AWS Step Functions state machine to orchestrate the Lambda functions.""]","[""Use an AWS Step Functions state machine to orchestrate the Lambda functions.""]","```json
{
    ""Use Amazon SQS to invoke the Lambda functions."": ""Amazon SQS is a message queuing service that enables decoupling and scaling of microservices, distributed systems, and serverless applications. However, it is not designed for orchestrating complex workflows with sequential and parallel execution of tasks. SQS can trigger Lambda functions, but it does not provide the orchestration capabilities needed to manage the sequence and dependencies between multiple Lambda functions as described in the question."",
    ""Use an AWS Step Functions activity to run the Lambda functions."": ""AWS Step Functions activities are used to integrate external tasks into a Step Functions workflow, where the tasks are performed by workers outside of AWS Step Functions. This is not suitable for orchestrating Lambda functions directly. Instead, AWS Step Functions state machines should be used to define and execute the sequence and parallel execution of Lambda functions."",
    ""Use Amazon SNS to trigger the Lambda functions."": ""Amazon SNS is a fully managed pub/sub messaging service that can trigger Lambda functions. However, SNS is designed for broadcasting messages to multiple subscribers and does not provide orchestration capabilities to manage the sequence or parallel execution of tasks. It is not suitable for controlling the flow of execution between multiple Lambda functions as required in the scenario."",
    ""Use an AWS Step Functions state machine to orchestrate the Lambda functions."": ""AWS Step Functions is a serverless orchestration service that allows you to coordinate multiple AWS services into serverless workflows. It is ideal for managing complex workflows with both sequential and parallel execution of tasks. By using a state machine, developers can define the order of execution, handle errors, and manage retries for the Lambda functions. This makes it the correct choice for orchestrating the Lambda functions in the specified sequence.""
}
```"
"A company is adding stored value (or gift card) capability to its highly popular casual gaming website. Users need to be able to trade this value for other users' items on the platform. This would require both users' records be updated as a single transaction, or both users' records to be completely rolled back. Which AWS database options can provide the transactional capability required for this new feature? (Choose TWO)","[""Amazon DynamoDB with operations made with the ConsistentRead parameter set to true."", ""Amazon ElastiCache for Memcached with operations made within a transaction block."", ""Amazon Aurora MySQL with operations made within a transaction block."", ""Amazon DynamoDB with reads and writes made using Transact* operations."", ""Amazon Redshift with operations made within a transaction block.""]","[""Amazon Aurora MySQL with operations made within a transaction block."", ""Amazon DynamoDB with reads and writes made using Transact* operations.""]","```json
{
    ""Amazon DynamoDB with operations made with the ConsistentRead parameter set to true."": ""This choice is incorrect. The ConsistentRead parameter in DynamoDB ensures that reads are strongly consistent, but it does not provide transactional capabilities. For transactions, DynamoDB offers TransactWriteItems and TransactGetItems, which allow multiple items to be updated atomically."",
    ""Amazon ElastiCache for Memcached with operations made within a transaction block."": ""This choice is incorrect. Amazon ElastiCache for Memcached does not support transactions. Memcached is an in-memory key-value store that is used for caching, and it does not have the capability to perform atomic transactions across multiple operations."",
    ""Amazon Aurora MySQL with operations made within a transaction block."": ""This choice is correct. Amazon Aurora MySQL supports ACID transactions, which means you can perform operations within a transaction block to ensure atomicity, consistency, isolation, and durability. This is ideal for scenarios where multiple records need to be updated as a single unit of work."",
    ""Amazon DynamoDB with reads and writes made using Transact* operations."": ""This choice is correct. Amazon DynamoDB supports transactional operations through TransactWriteItems and TransactGetItems. These operations allow multiple items to be read or written atomically, ensuring that all operations succeed or fail together, which is essential for the use case described."",
    ""Amazon Redshift with operations made within a transaction block."": ""This choice is incorrect. While Amazon Redshift does support transactions, it is primarily a data warehousing service optimized for analytic queries on large datasets. It is not typically used for transactional workloads involving frequent updates to individual records, as would be required for the described use case.""
}
```"
A developer is creating an AWS Lambda function that generates a new file each time it runs. Each new file must be checked into an AWS CodeCommit repository hosted in the same AWS account. How should the developer accomplish this?,"[""When the Lambda function starts, use the Git CLI to clone the repository. Check the new file into the cloned repository and push the change."", ""After the new file is created in Lambda, use cURL to invoke the CodeCommit API. Send the file to the repository."", ""Use an AWS SDK to instantiate a CodeCommit client. Invoke the put_file method to add the file to the repository."", ""Upload the new to an Amazon S3 bucket. Create an AWS Step Function to accept S3 events. In the Step Function, add the new file to the repository.""]","[""When the Lambda function starts, use the Git CLI to clone the repository. Check the new file into the cloned repository and push the change.""]","```json
{
    ""When the Lambda function starts, use the Git CLI to clone the repository. Check the new file into the cloned repository and push the change."": ""This choice is correct because using the Git CLI within a Lambda function to clone a repository, make changes, and push them back is a straightforward approach to interact with a CodeCommit repository. This method leverages Git's native capabilities to manage repositories, which is a common practice for developers familiar with Git workflows. However, it requires the Lambda environment to have the necessary Git CLI tools and credentials configured, which can be managed through AWS IAM roles and Lambda layers."",
    ""After the new file is created in Lambda, use cURL to invoke the CodeCommit API. Send the file to the repository."": ""This choice is incorrect because using cURL to interact with the CodeCommit API is not a standard or efficient way to manage files in a repository. CodeCommit is designed to work with Git, and using the Git CLI or AWS SDKs is the recommended approach. While technically possible, using cURL would require manually handling authentication and API requests, which is more complex and error-prone compared to using Git or SDKs."",
    ""Use an AWS SDK to instantiate a CodeCommit client. Invoke the put_file method to add the file to the repository."": ""This choice is incorrect because, as of the current AWS SDK capabilities, there is no direct 'put_file' method in the AWS SDK for CodeCommit that allows adding files directly to a repository. CodeCommit is primarily designed to be used with Git commands. The AWS SDKs can be used for repository management tasks like creating repositories or branches, but not for directly adding files in the way described."",
    ""Upload the new to an Amazon S3 bucket. Create an AWS Step Function to accept S3 events. In the Step Function, add the new file to the repository."": ""This choice is incorrect because it introduces unnecessary complexity and does not directly address the requirement of adding files to a CodeCommit repository. While S3 and Step Functions can be used for orchestrating workflows and processing events, they are not needed for the task of committing files to a CodeCommit repository. The question specifically asks for checking files into CodeCommit, which is best handled using Git operations.""
}
```"
A developer must ensure that the IAM credentials used by an application in Amazon EC2 are not misused or compromised. What should the developer use to keep user credentials secure?,"[""Environment variables."", ""AWS credentials file."", ""Instance profile credentials."", ""Command line options.""]","[""Command line options.""]","```json
{
    ""Environment variables."": ""Environment variables are not recommended for storing IAM credentials because they can be easily exposed or leaked, especially if the environment is not properly secured. They are not a secure method for managing sensitive information like IAM credentials, as they can be accessed by any process running on the instance."",
    ""AWS credentials file."": ""The AWS credentials file is a common way to store credentials on a local machine, but it is not suitable for use on EC2 instances because it involves storing credentials in plaintext on the file system. This poses a security risk if the instance is compromised. AWS best practices recommend avoiding hardcoding credentials in files."",
    ""Instance profile credentials."": ""Instance profile credentials are the recommended method for managing IAM credentials on EC2 instances. They allow the instance to assume an IAM role with temporary credentials that are automatically rotated, reducing the risk of credential compromise. This aligns with AWS best practices for security and is the preferred method for applications running on EC2."",
    ""Command line options."": ""Using command line options to pass IAM credentials is not a secure practice. It can expose credentials in shell history or process lists, making them susceptible to being compromised. This method is not recommended by AWS for managing credentials securely.""
}
```"
"A company has an application where reading objects from Amazon S3 is based on the type of user. The user types are registered user and guest user. The company has 25,000 users and is growing. Information is pulled from an S3 bucket depending on the user type. Which approaches are recommended to provide access to both user types? (Choose TWO)","[""Provide a different access key and secret access key in the application code for registered users and guest users to provide read access to the objects."", ""Use S3 bucket policies to restrict read access to specific IAM users."", ""Use Amazon Cognito to provide access using authenticated and unauthenticated roles."", ""Create a new IAM user for each user and grant read access."", ""Use the AWS IAM service and let the application assume the different roles using the AWS Security Token Service (AWS STS) AssumeRole action depending on the type of user and provide read access to Amazon S3 using the assumed role.""]","[""Provide a different access key and secret access key in the application code for registered users and guest users to provide read access to the objects."", ""Use S3 bucket policies to restrict read access to specific IAM users.""]","```json
{
    ""Provide a different access key and secret access key in the application code for registered users and guest users to provide read access to the objects."": ""This choice is incorrect. Hardcoding access keys and secret access keys in application code is a security risk and not a best practice. It can lead to accidental exposure of credentials, and managing such keys for a large number of users (25,000 and growing) is not scalable. AWS recommends using IAM roles and policies to manage access securely."",
    ""Use S3 bucket policies to restrict read access to specific IAM users."": ""This choice is incorrect. While S3 bucket policies can be used to control access to S3 buckets, they are not suitable for managing access for a large number of individual users, especially in a scenario where user types (registered and guest) need differentiated access. Bucket policies are better suited for broader access control rather than individual user management."",
    ""Use Amazon Cognito to provide access using authenticated and unauthenticated roles."": ""This choice is correct. Amazon Cognito is designed to manage user authentication and authorization, providing a scalable solution for handling both authenticated (registered users) and unauthenticated (guest users) access. Cognito can assign different IAM roles to users based on their authentication status, allowing for differentiated access to AWS resources like S3."",
    ""Create a new IAM user for each user and grant read access."": ""This choice is incorrect. Creating a new IAM user for each of the 25,000 users is not scalable and would be difficult to manage. IAM users are better suited for managing access for AWS accounts or services rather than individual application users. Using IAM roles and services like Cognito is more appropriate for this use case."",
    ""Use the AWS IAM service and let the application assume the different roles using the AWS Security Token Service (AWS STS) AssumeRole action depending on the type of user and provide read access to Amazon S3 using the assumed role."": ""This choice is correct. Using AWS STS to assume roles is a best practice for providing temporary, limited-privilege credentials to access AWS resources. The application can assume different roles based on the user type (registered or guest), allowing for secure and scalable access management to S3.""
}
```"
"A company has 25,000 employees and is growing. The company is creating an application that will be accessible to its employees only. A developer is using Amazon S3 to store images and Amazon RDS to store application data. The company requires that all employee information remain in the legacy Security Assertion Markup Language (SAML) employee directory only and is not interested in mirroring any employee information on AWS. How can the developer provide authorized access for the employees who will be using this application so each employee can access their own application data only?","[""Use Amazon VPC and keep all resources inside the VPC, and use a VPC link for the S3 bucket with the bucket policy."", ""Use Amazon Cognito user pools, federate with the SAML provider, and use user pool groups with an IAM policy."", ""Use an Amazon Cognito identity pool, federate with the SAML provider, and use an IAM condition key with a value for the `cognito-identity.amazonaws.com:sub` variable to grant access to the employees."", ""Create a unique IAM role for each employee and have each employee assume the role to access the application so they can access their personal data only.""]","[""Use an Amazon Cognito identity pool, federate with the SAML provider, and use an IAM condition key with a value for the `cognito-identity.amazonaws.com:sub` variable to grant access to the employees.""]","```json
{
    ""Use Amazon VPC and keep all resources inside the VPC, and use a VPC link for the S3 bucket with the bucket policy."": ""This option is incorrect because using a VPC and VPC link does not address the requirement of federating with a SAML provider for authentication. VPCs are used for network isolation and security, not for identity management or authentication. A VPC link is typically used for connecting a VPC to other AWS services, but it does not provide a mechanism for authenticating users based on SAML assertions."",
    ""Use Amazon Cognito user pools, federate with the SAML provider, and use user pool groups with an IAM policy."": ""This option is incorrect because Amazon Cognito user pools are primarily used for user authentication and management within AWS, but they do not directly support federation with SAML providers for access control. While user pools can federate with external identity providers, they are not designed to handle SAML-based authentication directly in the context of IAM policy enforcement for resource access."",
    ""Use an Amazon Cognito identity pool, federate with the SAML provider, and use an IAM condition key with a value for the `cognito-identity.amazonaws.com:sub` variable to grant access to the employees."": ""This option is correct because Amazon Cognito identity pools allow for federated authentication with SAML providers, enabling users to authenticate using their existing credentials. By federating with a SAML provider, the application can authenticate users without storing their credentials in AWS. The use of the IAM condition key `cognito-identity.amazonaws.com:sub` allows for fine-grained access control, ensuring that employees can access only their own data. This approach aligns with AWS best practices for secure and scalable identity management."",
    ""Create a unique IAM role for each employee and have each employee assume the role to access the application so they can access their personal data only."": ""This option is incorrect because creating a unique IAM role for each of the 25,000 employees is not scalable or manageable. IAM roles are not designed for individual user management at this scale, and this approach would lead to significant administrative overhead. Additionally, this does not leverage the existing SAML-based identity provider, which is a key requirement in the scenario.""
}
```"
A company has developed a new serverless application using AWS Lambda functions that will be deployed using the AWS Serverless Application Model (AWS SAM) CLI. Which step should the developer complete prior to deploying the application?,"[""Compress the application to a `.zip` file and upload it into AWS Lambda."", ""Test the new AWS Lambda function by first tracing it in AWS X-Ray."", ""Bundle the serverless application using a SAM package."", ""Create the application environment using the `eb create my-env` command.""]","[""Bundle the serverless application using a SAM package.""]","```json
{
    ""Compress the application to a `.zip` file and upload it into AWS Lambda."": ""This choice is incorrect because when using AWS SAM, the deployment process is abstracted and managed by SAM itself, which handles packaging and deployment. Manually compressing and uploading a `.zip` file is a step typically associated with deploying Lambda functions directly through the AWS Management Console or CLI, not when using SAM."",
    ""Test the new AWS Lambda function by first tracing it in AWS X-Ray."": ""This choice is incorrect because while testing and tracing with AWS X-Ray is a good practice for monitoring and debugging, it is not a prerequisite step for deploying an application using AWS SAM. X-Ray is used for analyzing and debugging production or test environments, not for the initial deployment process."",
    ""Bundle the serverless application using a SAM package."": ""This choice is correct because AWS SAM requires the application to be packaged before deployment. The `sam package` command packages the application and uploads the artifacts to an S3 bucket, preparing it for deployment with `sam deploy`. This is a necessary step in the SAM deployment workflow as per AWS best practices."",
    ""Create the application environment using the `eb create my-env` command."": ""This choice is incorrect because the command `eb create my-env` is used with AWS Elastic Beanstalk, not AWS SAM. Elastic Beanstalk is a different service for deploying and managing applications in the AWS Cloud, and it is not related to the serverless application model used by AWS SAM.""
}
```"
"An application needs to encrypt data that is written to Amazon S3 where the keys are managed in an on-premises data center, and the encryption is handled by S3. Which type of encryption should be used?","[""Use server-side encryption with Amazon S3-managed keys."", ""Use server-side encryption with AWS KMS-managed keys."", ""Use client-side encryption with customer master keys."", ""Use server-side encryption with customer-provided keys.""]","[""Use client-side encryption with customer master keys.""]","```json
{
    ""Use server-side encryption with Amazon S3-managed keys."": ""This option is incorrect because server-side encryption with Amazon S3-managed keys (SSE-S3) does not involve customer-managed keys. The keys are fully managed by AWS and not by the customer, which does not meet the requirement of managing keys in an on-premises data center."",
    ""Use server-side encryption with AWS KMS-managed keys."": ""This option is incorrect because server-side encryption with AWS KMS-managed keys (SSE-KMS) involves AWS Key Management Service, where AWS manages the keys, although customers have more control over them compared to SSE-S3. However, the question specifies that the keys are managed in an on-premises data center, which SSE-KMS does not support."",
    ""Use client-side encryption with customer master keys."": ""This option is correct because client-side encryption allows the customer to manage their encryption keys outside of AWS, including in an on-premises data center. The customer encrypts the data before uploading it to S3, and S3 handles the storage of the encrypted data. This meets the requirement of managing keys on-premises while allowing S3 to handle the storage of encrypted data."",
    ""Use server-side encryption with customer-provided keys."": ""This option is incorrect because server-side encryption with customer-provided keys (SSE-C) involves the customer providing the encryption keys to AWS for each request, but the keys are not managed on-premises in the way described in the question. The encryption is still handled by S3, but the key management requirement is not fully met since the keys are not persistently managed on-premises.""
}
```"
"A development team is working on a mobile app that allows users to upload pictures to Amazon S3. The team expects the app will be used by hundreds of thousands of users during a single event simultaneously. Once the pictures are uploaded, the backend service will scan and parse the pictures for inappropriate content. Which approach is the MOST resilient way to achieve this goal, which also smooths out temporary volume spikes for the backend service?","[""Develop an AWS Lambda function to check the upload folder in the S3 bucket. If new uploaded pictures are detected, the Lambda function will scan and parse them."", ""Once a picture is uploaded to Amazon S3, publish the event to an Amazon SQS queue. Use the queue as an event source to trigger an AWS Lambda function. In the Lambda function, scan and parse the picture."", ""When the user uploads a picture, invoke an API hosted in Amazon API Gateway. The API will invoke an AWS Lambda function to scan and parse the picture."", ""Create a state machine in AWS Step Functions to check the upload folder in the S3 bucket. If a new picture is detected, invoke an AWS Lambda function to scan and parse it.""]","[""Once a picture is uploaded to Amazon S3, publish the event to an Amazon SQS queue. Use the queue as an event source to trigger an AWS Lambda function. In the Lambda function, scan and parse the picture.""]","```json
{
    ""Develop an AWS Lambda function to check the upload folder in the S3 bucket. If new uploaded pictures are detected, the Lambda function will scan and parse them."": ""This approach is incorrect because it involves polling the S3 bucket, which is inefficient and can lead to unnecessary costs and delays. AWS Lambda does not natively support polling S3; instead, S3 can trigger Lambda functions directly upon object creation. This method does not handle volume spikes effectively as it lacks a buffering mechanism like SQS."",
    ""Once a picture is uploaded to Amazon S3, publish the event to an Amazon SQS queue. Use the queue as an event source to trigger an AWS Lambda function. In the Lambda function, scan and parse the picture."": ""This approach is correct because it uses Amazon SQS as a buffer to handle temporary spikes in upload volume, ensuring that the backend service can process images at a steady rate. SQS decouples the upload and processing stages, providing resilience and scalability. This method aligns with AWS best practices for handling high-throughput workloads and smoothing out processing spikes."",
    ""When the user uploads a picture, invoke an API hosted in Amazon API Gateway. The API will invoke an AWS Lambda function to scan and parse the picture."": ""This approach is incorrect because it directly invokes the Lambda function without any buffering mechanism, which can lead to throttling or failures during high-volume events. API Gateway is not designed to handle large, bursty workloads without additional buffering, making it less resilient for this use case."",
    ""Create a state machine in AWS Step Functions to check the upload folder in the S3 bucket. If a new picture is detected, invoke an AWS Lambda function to scan and parse it."": ""This approach is incorrect because AWS Step Functions is not designed to poll S3 buckets. While Step Functions can orchestrate complex workflows, it is not suitable for detecting new S3 objects. Additionally, this method lacks a buffering mechanism to handle volume spikes, making it less resilient compared to using SQS.""
}
```"
A development team wants to run their container workloads on Amazon ECS. Each application container needs to share data with another container to collect logs and metrics. What should the developer team do to meet these requirements?,"[""Create two pod specifications. Make one to include the application container and the other to include the other container. Link the two pods together."", ""Create two task definitions. Make one to include the application container and the other to include the other container. Mount a shared volume between the two tasks."", ""Create one task definition. Specify both containers in the definition. Mount a shared volume between those two containers."", ""Create a single pod specification. Include both containers in the specification. Mount a persistent volume to both containers.""]","[""Create two pod specifications. Make one to include the application container and the other to include the other container. Link the two pods together.""]","```json
{
    ""Create two pod specifications. Make one to include the application container and the other to include the other container. Link the two pods together."": ""This choice is incorrect because it refers to Kubernetes concepts (pods) rather than Amazon ECS concepts. In ECS, the equivalent of a pod is a task definition, and the linking of containers is done within a single task definition. ECS does not use pods or pod specifications."",
    ""Create two task definitions. Make one to include the application container and the other to include the other container. Mount a shared volume between the two tasks."": ""This choice is incorrect because ECS does not support mounting a shared volume between two separate tasks. In ECS, containers that need to share data should be defined within the same task definition where they can share a volume."",
    ""Create one task definition. Specify both containers in the definition. Mount a shared volume between those two containers."": ""This choice is correct. In Amazon ECS, to share data between containers, both containers should be defined within the same task definition. This allows them to share a volume mounted within the task, which is the recommended approach for sharing data like logs and metrics between containers."",
    ""Create a single pod specification. Include both containers in the specification. Mount a persistent volume to both containers."": ""This choice is incorrect because it refers to Kubernetes terminology (pods and persistent volumes) rather than Amazon ECS. In ECS, the equivalent of a pod is a task definition, and persistent volumes are not directly applicable. Instead, ECS uses task definitions and volumes defined within them for data sharing.""
}
```"
"An ecommerce startup is preparing for an annual sales event. As the traffic to the company's application increases, the development team wants to be notified when the Amazon EC2 instance's CPU utilization exceeds 80%. Which solution will meet this requirement?","[""Create a custom Amazon CloudWatch alarm that sends a notification to an Amazon SNS topic when the CPU utilization exceeds 80%."", ""Create a custom AWS Cloud Trail alarm that sends a notification to an Amazon SNS topic when the CPU utilization exceeds 80%."", ""Create a cron job on the EC2 instance that executes the `–describe-instance-information` command on the host instance every 15 minutes and sends the results to an Amazon SNS topic."", ""Create an AWS Lambda function that queries the AWS CloudTrail logs for the CPUUtilization metric every 15 minutes and sends a notification to an Amazon SNS topic when the CPU utilization exceeds 80%.""]","[""Create a custom Amazon CloudWatch alarm that sends a notification to an Amazon SNS topic when the CPU utilization exceeds 80%.""]","```json
{
    ""Create a custom Amazon CloudWatch alarm that sends a notification to an Amazon SNS topic when the CPU utilization exceeds 80%."": ""This choice is correct because Amazon CloudWatch is designed to monitor AWS resources and applications, and it can create alarms based on metrics such as CPU utilization. By setting a CloudWatch alarm to trigger when CPU utilization exceeds 80%, and configuring it to send a notification to an Amazon SNS topic, the team can receive alerts in real-time. This approach is efficient, scalable, and aligns with AWS best practices for monitoring EC2 instances."",
    ""Create a custom AWS Cloud Trail alarm that sends a notification to an Amazon SNS topic when the CPU utilization exceeds 80%."": ""This choice is incorrect because AWS CloudTrail is a service that provides logging and monitoring of account activity related to actions across your AWS infrastructure. It is not used for monitoring resource metrics like CPU utilization. CloudTrail is more suited for auditing and compliance purposes rather than real-time performance monitoring."",
    ""Create a cron job on the EC2 instance that executes the `–describe-instance-information` command on the host instance every 15 minutes and sends the results to an Amazon SNS topic."": ""This choice is incorrect because using a cron job to monitor CPU utilization is not efficient or scalable. It requires additional management overhead and does not provide real-time alerts. Furthermore, the `describe-instance-information` command is not typically used for retrieving CPU utilization metrics. CloudWatch provides a more integrated and automated solution for this requirement."",
    ""Create an AWS Lambda function that queries the AWS CloudTrail logs for the CPUUtilization metric every 15 minutes and sends a notification to an Amazon SNS topic when the CPU utilization exceeds 80%."": ""This choice is incorrect because AWS CloudTrail does not log CPU utilization metrics. CloudTrail is used for logging API calls and account activity. Using a Lambda function to query CloudTrail for CPU metrics is not feasible. Instead, CloudWatch should be used for monitoring and triggering alerts based on CPU utilization.""
}
```"
An application running on Amazon EC2 opens connections to an Amazon RDS SQL Server database. The developer does not want to store the user name and password for the database in the code. The developer would also like to automatically rotate the credentials. What is the MOST secure way to store and access the database credentials?,"[""Create an IAM role that has permissions to access the database. Attach the role to the EC2 instance."", ""Use AWS Secrets Manager to store the credentials. Retrieve the credentials from Secrets Manager as needed."", ""Store the credentials in an encrypted text file in an Amazon S3 bucket. Configure the EC2 instance's user data to download the credentials from Amazon S3 as the instance boots."", ""Store the user name and password credentials directly in the source code. No further action is needed because the source code is stored in a private repository.""]","[""Use AWS Secrets Manager to store the credentials. Retrieve the credentials from Secrets Manager as needed.""]","```json
{
    ""Create an IAM role that has permissions to access the database. Attach the role to the EC2 instance."": ""This choice is incorrect because IAM roles are used to grant permissions to AWS services, not to store or manage database credentials. IAM roles can be used to allow an EC2 instance to access AWS services like RDS, but they do not handle the storage or rotation of database credentials. The question specifically asks for a solution to store and rotate credentials securely, which IAM roles do not provide."",
    ""Use AWS Secrets Manager to store the credentials. Retrieve the credentials from Secrets Manager as needed."": ""This choice is correct because AWS Secrets Manager is specifically designed to store, manage, and automatically rotate secrets such as database credentials. It provides a secure and scalable way to manage secrets and integrates with RDS to automatically rotate credentials, which aligns with the developer's requirement to not store credentials in code and to automatically rotate them. This is a best practice for managing sensitive information securely."",
    ""Store the credentials in an encrypted text file in an Amazon S3 bucket. Configure the EC2 instance's user data to download the credentials from Amazon S3 as the instance boots."": ""This choice is incorrect because while storing credentials in an encrypted S3 bucket can provide some level of security, it does not offer automatic credential rotation, which is a requirement in the question. Additionally, managing encryption keys and ensuring secure access to the S3 bucket adds complexity and potential security risks compared to using AWS Secrets Manager."",
    ""Store the user name and password credentials directly in the source code. No further action is needed because the source code is stored in a private repository."": ""This choice is incorrect because storing credentials directly in source code is a security risk, even if the repository is private. It violates best practices for credential management, as it exposes sensitive information to anyone with access to the codebase and does not allow for automatic rotation of credentials. AWS recommends using services like Secrets Manager to securely manage and rotate secrets.""
}
```"
"A developer is updating an application deployed on AWS Elastic Beanstalk. The new version is incompatible with the old version. To successfully deploy the update, a full cutover to the new, updated version must be performed on all instances at one time, with the ability to roll back changes in case of a deployment failure in the new version. How can this be performed with the LEAST amount of downtime?","[""Use the Elastic Beanstalk All at once deployment policy to update all instances simultaneously."", ""Perform an Elastic Beanstalk Rolling with additional batch deployment."", ""Deploy the new version in a new Elastic Beanstalk environment and swap environment URLs."", ""Perform an Elastic Beanstalk Rolling deployment.""]","[""Deploy the new version in a new Elastic Beanstalk environment and swap environment URLs.""]","```json
{
    ""Use the Elastic Beanstalk All at once deployment policy to update all instances simultaneously."": ""This option is incorrect because while it updates all instances at once, it does not provide a rollback mechanism in case of failure. This method can lead to downtime if the new version fails, as all instances would be running the incompatible version simultaneously."",
    ""Perform an Elastic Beanstalk Rolling with additional batch deployment."": ""This option is incorrect because a rolling deployment updates instances in batches, which can lead to a mix of old and new versions running simultaneously. This is not suitable for incompatible versions as it can cause application errors. Additionally, it does not inherently provide a quick rollback mechanism."",
    ""Deploy the new version in a new Elastic Beanstalk environment and swap environment URLs."": ""This option is correct because it allows the new version to be deployed in a separate environment, ensuring that the current environment remains unaffected during the deployment. Once the new environment is verified to be working correctly, the environment URLs can be swapped, minimizing downtime. If issues arise, the swap can be reversed, providing an effective rollback mechanism. This approach aligns with AWS best practices for minimizing downtime and ensuring a smooth transition between application versions."",
    ""Perform an Elastic Beanstalk Rolling deployment."": ""This option is incorrect because a rolling deployment updates instances in phases, which can result in a mix of old and new versions running simultaneously. This is problematic for incompatible versions and does not provide an immediate rollback option, potentially leading to application errors and increased downtime.""
}
```"
"A developer is writing a web application that must share secure documents with end users. The documents are stored in a private Amazon S3 bucket. The application must allow only authenticated users to download specific documents when requested, and only for a duration of 15 minutes. How can the developer meet these requirements?","[""Copy the documents to a separate S3 bucket that has a lifecycle policy for deletion after 15 minutes."", ""Create a presigned S3 URL using the AWS SDK with an expiration time of 15 minutes."", ""Use server-side encryption with AWS KMS managed keys (SSE-KMS) and download the documents using HTTPS."", ""Modify the S3 bucket policy to only allow specific users to download the documents. Revert the change after 15 minutes.""]","[""Create a presigned S3 URL using the AWS SDK with an expiration time of 15 minutes.""]","```json
{
    ""Copy the documents to a separate S3 bucket that has a lifecycle policy for deletion after 15 minutes."": ""This choice is incorrect because copying documents to a separate S3 bucket with a lifecycle policy does not inherently restrict access to authenticated users or control access duration. Lifecycle policies manage the deletion of objects but do not provide temporary access control. Additionally, setting a lifecycle policy for such a short duration is impractical and not a typical use case for lifecycle policies."",
    ""Create a presigned S3 URL using the AWS SDK with an expiration time of 15 minutes."": ""This choice is correct because presigned URLs are specifically designed to grant temporary access to objects in S3. By creating a presigned URL with an expiration time of 15 minutes, the developer can ensure that only authenticated users can access the document for a limited time. This approach leverages AWS SDKs to generate URLs that include temporary credentials, aligning with best practices for secure, time-limited access to S3 objects."",
    ""Use server-side encryption with AWS KMS managed keys (SSE-KMS) and download the documents using HTTPS."": ""This choice is incorrect because while SSE-KMS provides encryption at rest and HTTPS ensures data is encrypted in transit, neither of these options inherently restrict access to authenticated users or control the duration of access. Encryption and secure transmission are important for data protection but do not address the requirement for temporary, authenticated access."",
    ""Modify the S3 bucket policy to only allow specific users to download the documents. Revert the change after 15 minutes."": ""This choice is incorrect because modifying bucket policies to allow specific users and then reverting the changes is not a practical or scalable solution for temporary access. Bucket policies are intended for more permanent access control configurations and do not provide a mechanism for time-limited access. This approach would also require continuous monitoring and manual intervention, which is error-prone and inefficient.""
}
```"
"A company is developing a report executed by AWS Step Functions, Amazon CloudWatch shows errors in the Step Functions task state machine. To troubleshoot each task, the state input needs to be included along with the error message in the state output. Which coding practice can preserve both the original input and the error for the state?","[""Use ResultPath in a Catch statement to include the error with the original input."", ""Use InputPath in a Catch statement and set the value to null."", ""Use Error Equals in a Retry statement to include the error with the original input."", ""Use OutputPath in a Retry statement and set the value to $.""]","[""Use ResultPath in a Catch statement to include the error with the original input.""]","```json
{
    ""Use ResultPath in a Catch statement to include the error with the original input."": ""This choice is correct because ResultPath in a Catch block allows you to specify where to inject the error information into the state output. By using ResultPath, you can merge the error details with the original input, preserving both for troubleshooting. This is a best practice for handling errors in AWS Step Functions, as it provides comprehensive context for debugging."",
    ""Use InputPath in a Catch statement and set the value to null."": ""This choice is incorrect because InputPath is used to filter the JSON input to a state, not to handle errors. Setting InputPath to null would effectively remove the input data, which is contrary to the requirement of preserving the original input along with the error information."",
    ""Use Error Equals in a Retry statement to include the error with the original input."": ""This choice is incorrect because Error Equals in a Retry statement is used to specify which errors to retry, not to include error information in the state output. Retry logic is about handling transient errors by retrying the operation, not about preserving input and error data together."",
    ""Use OutputPath in a Retry statement and set the value to $."": ""This choice is incorrect because OutputPath is used to filter the JSON output of a state, not to handle errors or preserve input. Setting OutputPath to $ would pass the entire state output as is, but it does not specifically address the need to include error information with the original input. Additionally, OutputPath is not typically used in Retry statements.""
}
```"
A developer receives the following error message when trying to launch or terminate an Amazon EC2 instance using a boto3 script. What should the developer do to correct this error message?,"[""Assign an IAM role to the EC2 instance to allow necessary API calls on behalf of the client."", ""Implement an exponential backoff algorithm for optimizing the number of API requests made to Amazon EC2."", ""Increase the overall network bandwidth to handle higher API request rates."", ""Upgrade to the latest AWS CLI version so that boto3 can handle higher request rates.""]","[""Implement an exponential backoff algorithm for optimizing the number of API requests made to Amazon EC2.""]","```json
{
    ""Assign an IAM role to the EC2 instance to allow necessary API calls on behalf of the client."": ""This choice is incorrect because the error message mentioned in the question is related to API request limits, not permissions. Assigning an IAM role is relevant when the issue is about permissions or authorization to perform certain actions, not when dealing with request throttling or rate limits."",
    ""Implement an exponential backoff algorithm for optimizing the number of API requests made to Amazon EC2."": ""This choice is correct because it addresses the issue of API request limits. AWS services, including EC2, have request rate limits, and when these limits are exceeded, AWS recommends implementing an exponential backoff algorithm. This algorithm helps to retry requests with increasing wait times, reducing the likelihood of hitting rate limits again. This is a best practice for handling throttling errors as per AWS documentation."",
    ""Increase the overall network bandwidth to handle higher API request rates."": ""This choice is incorrect because network bandwidth is unrelated to API request limits. The issue at hand is about the number of API requests being made, not the data transfer rate or network capacity. Increasing bandwidth would not resolve the problem of exceeding API request limits."",
    ""Upgrade to the latest AWS CLI version so that boto3 can handle higher request rates."": ""This choice is incorrect because upgrading the AWS CLI or boto3 does not affect the API request limits imposed by AWS services. The limits are set by AWS and are independent of the version of the tools being used. Therefore, upgrading would not resolve the issue of hitting request limits.""
}
```"
Given the following AWS CloudFormation template. What is the MOST efficient way to reference the new Amazon S3 bucket from another AWS CloudFormation template?,"[""Add an Export declaration to the Outputs section of the original template and use ImportValue in other templates."", ""Add Exported: true to the Contentbucket in the original template and use ImportResource in other templates."", ""Create a custom AWS CloudFormation resource that gets the bucket name from the ContentBucket resource of the first stack."", ""Use `Fn::Include` to include the existing template in other templates and use the ContentBucket resource directly.""]","[""Create a custom AWS CloudFormation resource that gets the bucket name from the ContentBucket resource of the first stack.""]","```json
{
    ""Add an Export declaration to the Outputs section of the original template and use ImportValue in other templates."": ""This is the correct approach. By adding an Export declaration to the Outputs section of the original template, you make the resource's value available to other stacks. The ImportValue function can then be used in other templates to reference this exported value. This method is efficient and follows AWS best practices for cross-stack references, allowing for modular and reusable CloudFormation templates."",
    ""Add Exported: true to the Contentbucket in the original template and use ImportResource in other templates."": ""This is incorrect because there is no such property as 'Exported: true' in AWS CloudFormation. Additionally, 'ImportResource' is not a valid function in CloudFormation. The correct way to export and import values between stacks is by using the Export and ImportValue functions."",
    ""Create a custom AWS CloudFormation resource that gets the bucket name from the ContentBucket resource of the first stack."": ""This is incorrect because creating a custom resource for this purpose is unnecessarily complex and inefficient. Custom resources are typically used for operations that are not natively supported by CloudFormation. For referencing resources between stacks, using Export and ImportValue is the recommended and most efficient approach."",
    ""Use `Fn::Include` to include the existing template in other templates and use the ContentBucket resource directly."": ""This is incorrect because Fn::Include is not a valid CloudFormation intrinsic function. CloudFormation does not support including templates in this manner. To reference resources across templates, you should use the Export and ImportValue functions to maintain modularity and reusability.""
}
```"
A developer is using AWS CodeDeploy to deploy an application running on Amazon EC2. The developer wants to change the file permissions for a specific deployment file. Which lifecycle event should a developer use to meet this requirement?,"[""AfterInstall."", ""DownloadBundle."", ""BeforeInstall."", ""ValidateService.""]","[""AfterInstall.""]","```json
{
    ""AfterInstall."": ""The 'AfterInstall' lifecycle event is correct because it occurs after the application revision has been installed on the instance. This is the appropriate time to change file permissions, as the files are in place and ready for configuration. According to AWS CodeDeploy documentation, the 'AfterInstall' event is used for tasks such as configuring your application, which includes setting file permissions. This ensures that any changes made do not interfere with the installation process itself."",
    ""DownloadBundle."": ""The 'DownloadBundle' lifecycle event is incorrect because it occurs before the application revision is installed on the instance. During this phase, the application files are being downloaded to the instance, but they are not yet in a state where permissions can be effectively modified. Modifying permissions at this stage could lead to errors or incomplete installations."",
    ""BeforeInstall."": ""The 'BeforeInstall' lifecycle event is incorrect because it occurs before the application revision is installed. This phase is typically used for pre-installation tasks such as backing up the current version or stopping services. Changing file permissions at this stage might not be effective as the files are not yet in their final location or state."",
    ""ValidateService."": ""The 'ValidateService' lifecycle event is incorrect because it occurs after the application has been installed and started. This phase is used to verify that the deployment was successful and that the application is running as expected. Changing file permissions during this phase is too late, as the application might already be running and relying on specific permissions set during the 'AfterInstall' phase.""
}
```"
A developer is using Amazon DynamoDB to store application data. The developer wants to further improve application performance by reducing response times for read and write operations. Which DynamoDB feature should be used to meet these requirements?,"[""Amazon DynamoDB Streams."", ""Amazon DynamoDB Accelerator."", ""Amazon DynamoDB global tables."", ""Amazon DynamoDB transactions.""]","[""Amazon DynamoDB Accelerator.""]","```json
{
    ""Amazon DynamoDB Streams."": ""Amazon DynamoDB Streams is used to capture changes to items in a DynamoDB table and can be used for real-time processing of data changes. However, it does not directly improve the performance of read and write operations in terms of reducing response times. It is more about enabling event-driven architectures and data replication, not enhancing read/write performance."",
    ""Amazon DynamoDB Accelerator."": ""Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory caching service for DynamoDB that can reduce response times from milliseconds to microseconds, even at millions of requests per second. It is specifically designed to improve the performance of read operations by caching frequently accessed data, thus reducing the load on the DynamoDB table itself. This makes it the correct choice for improving application performance by reducing response times."",
    ""Amazon DynamoDB global tables."": ""Amazon DynamoDB global tables provide a fully managed, multi-region, and multi-master database solution that allows for low-latency access to data globally. While they improve data availability and reduce latency for cross-region access, they do not inherently reduce response times for read and write operations within a single region or application context. Therefore, they are not the best choice for the specific requirement of reducing response times."",
    ""Amazon DynamoDB transactions."": ""Amazon DynamoDB transactions provide developers with ACID (Atomicity, Consistency, Isolation, Durability) capabilities to ensure that a series of operations on multiple items are completed successfully. While transactions ensure data integrity and consistency, they can actually increase response times due to the overhead of ensuring transactional guarantees. Thus, they are not suitable for the requirement of reducing response times for read and write operations.""
}
```"
A developer is creating a script to automate the deployment process for a serverless application. The developer wants to use an existing AWS Serverless Application Model (AWS SAM) template for the application. What should the developer use for the project? (Choose TWO),"[""Call `aws cloudformation package` to create the deployment package. Call `aws cloudformation deploy` to deploy the package afterward."", ""Call `sam package` to create the deployment package. Call `sam deploy` to deploy the package afterward."", ""Call `aws s3 cp` to upload the AWS SAM template to Amazon S3. Call `aws lambda update-function-code` to create the application."", ""Create a `ZIP` package locally and call `aws serverlessrepo create-application` to create the application."", ""Create a `ZIP` package and upload it to Amazon S3. Call `aws cloudformation create-stack` to create the application.""]","[""Call `aws cloudformation package` to create the deployment package. Call `aws cloudformation deploy` to deploy the package afterward."", ""Call `sam package` to create the deployment package. Call `sam deploy` to deploy the package afterward.""]","```json
{
    ""Call `aws cloudformation package` to create the deployment package. Call `aws cloudformation deploy` to deploy the package afterward."": ""This choice is correct because AWS CloudFormation can be used to deploy serverless applications by packaging the application code and dependencies into an S3 bucket and then deploying the packaged template. The `aws cloudformation package` command uploads local artifacts to S3 and produces a modified template that CloudFormation can use. The `aws cloudformation deploy` command then deploys the application using the packaged template. This method is a standard practice for deploying serverless applications using CloudFormation."",
    ""Call `sam package` to create the deployment package. Call `sam deploy` to deploy the package afterward."": ""This choice is correct because AWS SAM (Serverless Application Model) is specifically designed for building serverless applications. The `sam package` command packages the application by uploading local artifacts to S3 and generating a CloudFormation template. The `sam deploy` command deploys the application using this template. SAM provides a simplified syntax for expressing serverless resources, making it easier to manage serverless applications. This is a best practice for deploying serverless applications using SAM."",
    ""Call `aws s3 cp` to upload the AWS SAM template to Amazon S3. Call `aws lambda update-function-code` to create the application."": ""This choice is incorrect because while `aws s3 cp` can upload files to S3, it does not handle the packaging of dependencies or the creation of a CloudFormation stack. `aws lambda update-function-code` is used to update existing Lambda functions, not to deploy new serverless applications using SAM templates. This approach does not utilize the capabilities of SAM or CloudFormation for deploying serverless applications."",
    ""Create a `ZIP` package locally and call `aws serverlessrepo create-application` to create the application."": ""This choice is incorrect because `aws serverlessrepo create-application` is used to publish applications to the AWS Serverless Application Repository, not to deploy applications. Creating a ZIP package locally is part of preparing code for deployment, but this method does not involve using SAM or CloudFormation to automate the deployment process. This approach is not aligned with the use of SAM templates for deployment."",
    ""Create a `ZIP` package and upload it to Amazon S3. Call `aws cloudformation create-stack` to create the application."": ""This choice is incorrect because while uploading a ZIP package to S3 is part of the deployment process, `aws cloudformation create-stack` requires a properly packaged template that references the S3 location of the artifacts. Without using `aws cloudformation package` or `sam package`, the template will not be correctly configured to deploy a serverless application. This method lacks the automation and simplification provided by SAM or CloudFormation package commands.""
}
```"
A development team is designing a mobile app that requires multi-factor authentication. Which steps should be taken to achieve this? (Choose TWO),"[""Use Amazon Cognito to create a user pool and create users in the user pool."", ""Send multi-factor authentication text codes to users with the Amazon SNS Publish API call in the app code."", ""Enable multi-factor authentication for the Amazon Cognito user pool."", ""Use AWS IAM to create IAM users."", ""Enable multi-factor authentication for the users created in AWS IAM.""]","[""Use Amazon Cognito to create a user pool and create users in the user pool."", ""Enable multi-factor authentication for the Amazon Cognito user pool.""]","```json
{
    ""Use Amazon Cognito to create a user pool and create users in the user pool."": ""This choice is correct because Amazon Cognito is a service designed to handle user authentication and authorization, including the creation of user pools. A user pool is a user directory in Amazon Cognito that provides options for sign-up and sign-in, including support for multi-factor authentication (MFA). This aligns with best practices for managing user authentication in mobile apps, as it abstracts the complexity of user management and security."",
    ""Send multi-factor authentication text codes to users with the Amazon SNS Publish API call in the app code."": ""This choice is incorrect because while Amazon SNS can be used to send SMS messages, it is not the recommended method for implementing MFA in a mobile app. Amazon Cognito natively supports MFA and can handle the sending of verification codes without the need for direct integration with SNS. Using Cognito's built-in MFA features is more secure and simplifies implementation."",
    ""Enable multi-factor authentication for the Amazon Cognito user pool."": ""This choice is correct because enabling MFA in an Amazon Cognito user pool is a straightforward way to add an additional layer of security to user authentication. Cognito provides built-in support for MFA, including options for SMS-based codes or TOTP applications, which are essential for securing user accounts in a mobile app."",
    ""Use AWS IAM to create IAM users."": ""This choice is incorrect because AWS IAM is intended for managing permissions and access for AWS resources, not for end-user authentication in applications. IAM users are meant for managing AWS resources and not for application-level user management. For mobile app authentication, Amazon Cognito is the appropriate service."",
    ""Enable multi-factor authentication for the users created in AWS IAM."": ""This choice is incorrect because, although MFA can be enabled for IAM users, it is not applicable for end-user authentication in a mobile app context. IAM is used for managing AWS resources and not for handling user authentication for applications. For application users, Amazon Cognito should be used, which provides built-in support for MFA.""
}
```"
"Two containerized microservices are hosted on Amazon EC2 ECS. The first microservice reads an Amazon RDS Aurora database instance, and the second microservice reads an Amazon DynamoDB table. How can each microservice be granted the minimum privileges?","[""Set ECS_ENABLE_TASK_IAM_ROLE to false on EC2 instance boot in ECS agent configuration file. Run the first microservice with an IAM role for ECS tasks with read-only access for the Aurora database. Run the second microservice with an IAM role for ECS tasks with read-only access to DynamoDB."", ""Set ECS_ENABLE_TASK_IAM_ROLE to false on EC2 instance boot in the ECS agent configuration file. Grant the instance profile role read-only access to the Aurora database and DynamoDB."", ""Set ECS_ENABLE_TASK_IAM_ROLE to true on EC2 instance boot in the ECS agent configuration file. Run the first microservice with an IAM role for ECS tasks with read-only access for the Aurora database. Run the secondmicroservice with an IAM role for ECS tasks with read-only access to DynamoDB."", ""Set ECS_ENABLE_TASK_IAM_ROLE to true on EC2 instance boot in the ECS agent configuration file. Grant the instance profile role read-only access to the Aurora database and DynamoDB.""]","[""Set ECS_ENABLE_TASK_IAM_ROLE to true on EC2 instance boot in the ECS agent configuration file. Run the first microservice with an IAM role for ECS tasks with read-only access for the Aurora database. Run the secondmicroservice with an IAM role for ECS tasks with read-only access to DynamoDB.""]","{""Set ECS_ENABLE_TASK_IAM_ROLE to false on EC2 instance boot in ECS agent configuration file. Run the first microservice with an IAM role for ECS tasks with read-only access for the Aurora database. Run the second microservice with an IAM role for ECS tasks with read-only access to DynamoDB."": ""Incorrect. Setting ECS_ENABLE_TASK_IAM_ROLE to false means that ECS tasks cannot assume IAM roles, which is necessary for granting specific permissions to each microservice. Without task roles, the microservices cannot have the minimum privileges required for accessing the Aurora database and DynamoDB."", ""Set ECS_ENABLE_TASK_IAM_ROLE to false on EC2 instance boot in the ECS agent configuration file. Grant the instance profile role read-only access to the Aurora database and DynamoDB."": ""Incorrect. By setting ECS_ENABLE_TASK_IAM_ROLE to false, the tasks cannot use IAM roles, and granting permissions at the instance level means all tasks on the instance have the same permissions, which violates the principle of least privilege. This approach is less secure as it grants broader access than necessary."", ""Set ECS_ENABLE_TASK_IAM_ROLE to true on EC2 instance boot in the ECS agent configuration file. Run the first microservice with an IAM role for ECS tasks with read-only access for the Aurora database. Run the second microservice with an IAM role for ECS tasks with read-only access to DynamoDB."": ""Correct. Setting ECS_ENABLE_TASK_IAM_ROLE to true allows ECS tasks to assume IAM roles, enabling fine-grained permissions for each microservice. This follows the principle of least privilege by granting each microservice only the permissions it needs. This is a best practice for securing applications running in ECS."", ""Set ECS_ENABLE_TASK_IAM_ROLE to true on EC2 instance boot in the ECS agent configuration file. Grant the instance profile role read-only access to the Aurora database and DynamoDB."": ""Incorrect. Although ECS_ENABLE_TASK_IAM_ROLE is set to true, granting permissions at the instance profile level means all tasks on the instance have the same permissions, which is not ideal for security. It does not leverage the ability to assign specific roles to individual tasks, which is a best practice for minimizing privileges.""}"
A developer has written an AWS Lambda function using Java as the runtime environment. The developer wants to isolate a performance bottleneck in the code. Which steps should be taken to reveal the bottleneck?,"[""Use the Amazon CloudWatch API to write timestamps to a custom CloudWatch metric. Use the CloudWatch console to analyze the resulting data."", ""Use the AWS X-Ray API to write trace data into X-Ray from strategic places within the code. Use the Amazon CloudWatch console to analyze the resulting data."", ""Use the AWS X-Ray API to write trace data into X-Ray from strategic places within the code. Use the X-Ray console to analyze the resulting data."", ""Use the Amazon CloudWatch API to write timestamps to a custom CloudWatch metric. Use the AWS X-Ray console to analyze the resulting data.""]","[""Use the AWS X-Ray API to write trace data into X-Ray from strategic places within the code. Use the X-Ray console to analyze the resulting data.""]","```json
{
    ""Use the Amazon CloudWatch API to write timestamps to a custom CloudWatch metric. Use the CloudWatch console to analyze the resulting data."": ""This choice is incorrect. While Amazon CloudWatch can be used to monitor metrics and logs, it is not specifically designed to trace and isolate performance bottlenecks in code execution. CloudWatch metrics are more suited for monitoring resource utilization and application performance at a high level, rather than providing detailed insights into the execution path of code."",
    ""Use the AWS X-Ray API to write trace data into X-Ray from strategic places within the code. Use the Amazon CloudWatch console to analyze the resulting data."": ""This choice is incorrect. Although AWS X-Ray is the correct tool for tracing and analyzing performance bottlenecks in code, the analysis should be done using the X-Ray console, not the CloudWatch console. X-Ray provides a service map and detailed trace data that are specifically designed for analyzing application performance and identifying bottlenecks."",
    ""Use the AWS X-Ray API to write trace data into X-Ray from strategic places within the code. Use the X-Ray console to analyze the resulting data."": ""This choice is correct. AWS X-Ray is designed to trace requests as they travel through your application, providing insights into the performance of your code. By instrumenting your code with X-Ray, you can capture detailed trace data and use the X-Ray console to visualize and analyze this data, helping to identify performance bottlenecks. This approach is a best practice for diagnosing performance issues in distributed applications."",
    ""Use the Amazon CloudWatch API to write timestamps to a custom CloudWatch metric. Use the AWS X-Ray console to analyze the resulting data."": ""This choice is incorrect. While CloudWatch metrics can provide some level of monitoring, they do not offer the detailed tracing capabilities needed to isolate performance bottlenecks in code. Additionally, the X-Ray console is not used to analyze CloudWatch metrics; it is specifically designed for analyzing trace data collected by X-Ray.""
}
```"
"A developer added a new feature to an application running on an Amazon EC2 instance that uses Amazon SQS. After deployment, the developer noticed a significant increase in Amazon SQS costs. When monitoring the Amazon SQS metrics on Amazon CloudWatch, the developer found that on average one message per minute is posted on this queue. What can be done to reduce Amazon SQS costs for this application?","[""Increase the Amazon SQS queue polling timeout."", ""Scale down the Amazon SQS queue to the appropriate size for low traffic demand."", ""Configure push delivery via Amazon SNS instead of polling the Amazon SQS queue."", ""Use an Amazon SQS first-in, first-out (FIFO) queue instead of a standard queue.""]","[""Increase the Amazon SQS queue polling timeout.""]","```json
{
    ""Increase the Amazon SQS queue polling timeout."": ""This choice is correct because increasing the polling timeout, also known as long polling, reduces the number of empty responses and thus the number of requests made to Amazon SQS. Long polling waits for a message to arrive in the queue before returning a response, which can significantly reduce costs when the message arrival rate is low. This is a best practice for reducing costs in low traffic scenarios as it minimizes the number of API calls made to the SQS service."",
    ""Scale down the Amazon SQS queue to the appropriate size for low traffic demand."": ""This choice is incorrect because Amazon SQS is a fully managed service that automatically scales to handle the demand. There is no concept of 'scaling down' an SQS queue as it does not have a fixed size or capacity that can be adjusted. Costs are based on the number of requests and data transferred, not on the size of the queue."",
    ""Configure push delivery via Amazon SNS instead of polling the Amazon SQS queue."": ""This choice is incorrect because Amazon SNS and SQS serve different purposes. SNS is a pub/sub messaging service, while SQS is a message queueing service. Switching to SNS would change the architecture and behavior of the application. Additionally, SNS does not support direct push to SQS queues; it would require setting up an SNS topic and subscriptions, which may not reduce costs and could complicate the architecture."",
    ""Use an Amazon SQS first-in, first-out (FIFO) queue instead of a standard queue."": ""This choice is incorrect because switching to a FIFO queue does not inherently reduce costs. FIFO queues are designed to ensure that messages are processed in the exact order they are sent and are typically more expensive than standard queues due to the additional ordering guarantees. This would not address the issue of reducing costs for low traffic scenarios.""
}
```"
"A developer is building an application using an Amazon API Gateway REST API backend by an AWS Lambda function that interacts with an Amazon DynamoDB table. During testing, the developer observes high latency when making requests to the API. How can the developer evaluate the end-to-end latency and identify performance bottlenecks?","[""Enable AWS CloudTrail logging and use the logs to map each latency and bottleneck."", ""Enable and configure AWS X-Ray tracing on API Gateway and the Lambda function. Use X-Ray to trace and analyze user requests."", ""Enable Amazon CloudWatch Logs for the Lambda function. Enable execution logs for API Gateway to view and analyze user request logs."", ""Enable VPC Flow Logs to capture and analyze network traffic within the VPC.""]","[""Enable and configure AWS X-Ray tracing on API Gateway and the Lambda function. Use X-Ray to trace and analyze user requests.""]","```json
{
    ""Enable AWS CloudTrail logging and use the logs to map each latency and bottleneck."": ""Incorrect. AWS CloudTrail is primarily used for auditing and logging API calls made on your AWS account. It does not provide detailed insights into the latency or performance bottlenecks of your application. CloudTrail logs are more suited for security and compliance purposes rather than performance analysis."",
    ""Enable and configure AWS X-Ray tracing on API Gateway and the Lambda function. Use X-Ray to trace and analyze user requests."": ""Correct. AWS X-Ray is designed to trace and analyze requests as they travel through your application, including API Gateway and Lambda. It provides end-to-end visibility into the performance of your application, allowing you to identify latency issues and bottlenecks. X-Ray can trace requests from API Gateway through to Lambda and DynamoDB, providing a comprehensive view of where delays may be occurring."",
    ""Enable Amazon CloudWatch Logs for the Lambda function. Enable execution logs for API Gateway to view and analyze user request logs."": ""Incorrect. While CloudWatch Logs can provide logs of Lambda function executions and API Gateway requests, they do not offer the detailed tracing capabilities needed to evaluate end-to-end latency and identify specific bottlenecks. CloudWatch Logs are useful for debugging and monitoring but lack the comprehensive tracing features of X-Ray."",
    ""Enable VPC Flow Logs to capture and analyze network traffic within the VPC."": ""Incorrect. VPC Flow Logs are used to capture information about the IP traffic going to and from network interfaces in your VPC. They are useful for network traffic analysis and security monitoring but do not provide insights into application-level latency or performance issues related to API Gateway, Lambda, or DynamoDB.""
}
```"
"An IAM role is attached to an Amazon EC2 instance that explicitly denies access to all Amazon S3 API actions. The EC2 instance credentials file specifies the IAM access key and secret access key, which allow full administrative access. Given that multiple modes of IAM access are present for this EC2 instance, which of the following is correct?","[""The EC2 instance will only be able to list the S3 buckets."", ""The EC2 instance will only be able to list the contents of one S3 bucket at a time."", ""The EC2 instance will be able to perform all actions on any S3 bucket."", ""The EC2 instance will not be able to perform any S3 action on any S3 bucket.""]","[""The EC2 instance will not be able to perform any S3 action on any S3 bucket.""]","```json
{
    ""The EC2 instance will only be able to list the S3 buckets."": ""This choice is incorrect because the IAM role attached to the EC2 instance explicitly denies access to all S3 API actions. In AWS, explicit denies take precedence over any allows. Even if the EC2 instance credentials file specifies keys that allow full administrative access, the explicit deny from the IAM role will prevent any S3 actions, including listing buckets."",
    ""The EC2 instance will only be able to list the contents of one S3 bucket at a time."": ""This choice is incorrect because the explicit deny in the IAM role prevents any S3 API actions, not just listing contents. The presence of an explicit deny means that no S3 actions, including listing contents of any bucket, can be performed regardless of the credentials specified in the EC2 instance credentials file."",
    ""The EC2 instance will be able to perform all actions on any S3 bucket."": ""This choice is incorrect because it overlooks the fact that an explicit deny in IAM policies takes precedence over any allows. Even though the EC2 instance credentials file has keys for full administrative access, the explicit deny in the IAM role will override this, preventing all S3 actions."",
    ""The EC2 instance will not be able to perform any S3 action on any S3 bucket."": ""This choice is correct because in AWS IAM, explicit denies take precedence over any allows. The IAM role attached to the EC2 instance explicitly denies all S3 API actions, which means that no matter what permissions are granted through other means (such as the credentials file), the EC2 instance will be unable to perform any S3 actions. This is a key concept in IAM policy evaluation and is supported by AWS best practices and documentation.""
}
```"
"A development team uses AWS Elastic Beanstalk for application deployment. The team has configured the application version lifecycle policy to limit the number of application versions to 25. However, even with the lifecycle policy, the source bundle is deleted from the Amazon S3 source bucket. What should a developer do in the Elastic Beanstalk application version lifecycle settings to retain the source code in the S3 bucket?","[""Change the Set the application versions limit by total count setting to zero."", ""Disable the Lifecycle policy setting."", ""Change the Set the application version limit by age setting to zero."", ""Set Retention to Retain source bundle in S3.""]","[""Set Retention to Retain source bundle in S3.""]","```json
{
    ""Change the Set the application versions limit by total count setting to zero."": ""This choice is incorrect because setting the application versions limit by total count to zero would mean there is no limit on the number of versions stored, which does not directly address the retention of the source bundle in S3. The lifecycle policy would still manage the deletion of versions based on other criteria, and this setting does not ensure the source bundle is retained."",
    ""Disable the Lifecycle policy setting."": ""This choice is incorrect because disabling the lifecycle policy would prevent any automatic cleanup of old application versions, which could lead to excessive storage usage and potential costs. It does not specifically address the retention of the source bundle in S3, which is controlled by a different setting."",
    ""Change the Set the application version limit by age setting to zero."": ""This choice is incorrect because setting the application version limit by age to zero would imply no age-based deletion of versions, but it does not ensure the source bundle is retained in S3. The retention of the source bundle is managed by a specific setting in Elastic Beanstalk."",
    ""Set Retention to Retain source bundle in S3."": ""This choice is correct because setting the retention policy to 'Retain source bundle in S3' ensures that the source bundle is not deleted from the S3 bucket, even if the application version is deleted. This setting is specifically designed to manage the retention of source bundles independently of the version lifecycle policy. According to AWS best practices, this allows developers to maintain a backup of their source code for future reference or redeployment.""
}
```"
"A developer has built a market application that stores pricing data in Amazon DynamoDB with Amazon ElastiCache in front. The prices of items in the market change frequently. Sellers have begun complaining that, after they update the price of an item, the price does not actually change in the product listing. What could be causing this issue?","[""The cache is not being invalidated when the price of the item is changed."", ""The price of the item is being retrieved using a write-through ElastiCache cluster."", ""The DynamoDB table was provisioned with insufficient read capacity."", ""The DynamoDB table was provisioned with insufficient write capacity.""]","[""The cache is not being invalidated when the price of the item is changed.""]","```json
{
    ""The cache is not being invalidated when the price of the item is changed."": ""This choice is correct because when data in the database changes, the cache must be invalidated or updated to ensure that the application retrieves the most current data. In this scenario, if the cache is not invalidated after a price update, the application will continue to serve stale data from the cache instead of the updated data from DynamoDB. This is a common issue when using caching layers like Amazon ElastiCache in front of a database, and it requires implementing a cache invalidation strategy to keep the cache in sync with the database."",
    ""The price of the item is being retrieved using a write-through ElastiCache cluster."": ""This choice is incorrect because a write-through cache automatically writes data to the cache whenever there is a write operation to the database. However, the issue described is about reading stale data, not writing. The problem is not with how data is written to the cache, but rather with how the cache is updated or invalidated when the underlying data changes."",
    ""The DynamoDB table was provisioned with insufficient read capacity."": ""This choice is incorrect because insufficient read capacity would lead to throttling and potentially slower response times, but it would not cause stale data to be served. The issue described is about serving outdated data after an update, which is related to caching, not read capacity."",
    ""The DynamoDB table was provisioned with insufficient write capacity."": ""This choice is incorrect because insufficient write capacity would lead to throttling of write operations, which could delay updates to the database. However, the issue described involves serving stale data, which is a caching problem. Even if writes are delayed, once they succeed, the cache should be invalidated to reflect the new data.""
}
```"
A developer is provided with an HTTPS clone URL for an AWS CodeCommit repository. What needs to be configured before cloning this repository?,"[""Use AWS KMS to set up public and private keys for use with AWS CodeCommit."", ""Set up the Git credential helper to use an AWS credential profile, and enable the helper to send the path to the repositories."", ""Use AWS Certificate Manager to provision public and private SSL/TLS certificates."", ""Generate encryption keys using AWS CloudHSM, then export the key for use with AWS CodeCommit.""]","[""Set up the Git credential helper to use an AWS credential profile, and enable the helper to send the path to the repositories.""]","```json
{
    ""Use AWS KMS to set up public and private keys for use with AWS CodeCommit."": ""This choice is incorrect because AWS KMS is used for managing encryption keys for data encryption, not for managing Git credentials. AWS CodeCommit does not require KMS keys for authentication when using HTTPS. Instead, it uses IAM credentials or Git credentials."",
    ""Set up the Git credential helper to use an AWS credential profile, and enable the helper to send the path to the repositories."": ""This choice is correct because when using HTTPS with AWS CodeCommit, you can configure the Git credential helper to use an AWS credential profile. This allows Git to automatically use your AWS credentials stored in the profile to authenticate with CodeCommit, which is a recommended best practice for secure access."",
    ""Use AWS Certificate Manager to provision public and private SSL/TLS certificates."": ""This choice is incorrect because AWS Certificate Manager is used for managing SSL/TLS certificates for securing network traffic, not for Git authentication. CodeCommit uses HTTPS for secure communication, but the authentication mechanism relies on IAM or Git credentials, not SSL/TLS certificates."",
    ""Generate encryption keys using AWS CloudHSM, then export the key for use with AWS CodeCommit."": ""This choice is incorrect because AWS CloudHSM is used for hardware-based key management and cryptographic operations, not for managing Git credentials. CodeCommit does not require CloudHSM keys for authentication when cloning repositories.""
}
```"
What is required to trace Lambda-based applications with AWS X-Ray?,"[""Send logs from the Lambda application to an S3 bucket; trigger a Lambda function from the bucket to send data to AWS X-Ray."", ""Trigger a Lambda function from the application logs in Amazon CloudWatch to submit tracing data to AWS X-Ray."", ""Use an IAM execution role to give the Lambda function permissions and enable tracing."", ""Update and add AWS X-Ray daemon code to relevant parts of the Lambda function to set up the trace.""]","[""Use an IAM execution role to give the Lambda function permissions and enable tracing.""]","```json
{
    ""Send logs from the Lambda application to an S3 bucket; trigger a Lambda function from the bucket to send data to AWS X-Ray."": ""This choice is incorrect because AWS X-Ray does not require sending logs to an S3 bucket or triggering another Lambda function to collect trace data. AWS X-Ray is designed to work directly with AWS Lambda by enabling tracing in the Lambda function settings and using an IAM role with the necessary permissions."",
    ""Trigger a Lambda function from the application logs in Amazon CloudWatch to submit tracing data to AWS X-Ray."": ""This choice is incorrect because AWS X-Ray does not require triggering a Lambda function from CloudWatch logs to submit tracing data. Instead, AWS X-Ray integrates directly with AWS Lambda, and tracing can be enabled in the Lambda configuration with the appropriate IAM permissions."",
    ""Use an IAM execution role to give the Lambda function permissions and enable tracing."": ""This choice is correct because AWS X-Ray requires that the Lambda function has the necessary permissions to send trace data to X-Ray. This is achieved by attaching an IAM execution role with the 'AWSXrayWriteOnlyAccess' policy to the Lambda function. Additionally, tracing must be enabled in the Lambda function's configuration settings. This setup allows AWS X-Ray to automatically capture trace data from the Lambda execution."",
    ""Update and add AWS X-Ray daemon code to relevant parts of the Lambda function to set up the trace."": ""This choice is incorrect because AWS Lambda does not require the addition of AWS X-Ray daemon code within the function code itself. AWS Lambda automatically integrates with AWS X-Ray when tracing is enabled in the function's configuration, and the necessary IAM permissions are provided. The X-Ray daemon is not needed within the Lambda execution environment.""
}
```"
"A development team is building a new application that will run on Amazon EC2 and use Amazon DynamoDB as a storage layer. The developers all have assigned IAM user accounts in the same IAM group. The developers currently can launch EC2 instances, and they need to be able to launch EC2 instances with an instance role allowing access to Amazon DynamoDB. Which AWS IAM changes are needed when creating an instance role to provide this functionality?","[""Create an IAM permission policy attached to the role that allows access to DynamoDB. Add a trust policy to the role that allows DynamoDB to assume the role. Attach a permissions policy to the development group in AWS IAM that allows developers to use the iam:GetRole and iam:PassRole permissions for the role."", ""Create an IAM permissions policy attached to the role that allows access to DynamoDB. Add a trust policy to the role that allows Amazon EC2 to assume the role. Attach a permissions policy to the development group in AWS IAM that allows developers to use the iam:PassRole permission for the role."", ""Create an IAM permission policy attached to the role that allows access to Amazon EC2. Add a trust policy to the role that allows DynamoDB to assume the role. Attach a permissions policy to the development group in AWS IAM that allows developers to use the iam:PassRole permission for the role."", ""Create an IAM permissions policy attached to the role that allows access to DynamoDB. Add a trust policy to the role that allows Amazon EC2 to assume the role. Attach a permissions policy to the development group in AWS IAM that allows developers to use the iam:GetRole permission for the role.""]","[""Create an IAM permissions policy attached to the role that allows access to DynamoDB. Add a trust policy to the role that allows Amazon EC2 to assume the role. Attach a permissions policy to the development group in AWS IAM that allows developers to use the iam:PassRole permission for the role.""]","```json
{
    ""Create an IAM permission policy attached to the role that allows access to DynamoDB. Add a trust policy to the role that allows DynamoDB to assume the role. Attach a permissions policy to the development group in AWS IAM that allows developers to use the iam:GetRole and iam:PassRole permissions for the role."": ""Incorrect. The trust policy should allow Amazon EC2 to assume the role, not DynamoDB. The role is meant to be assumed by EC2 instances to access DynamoDB, not the other way around. Additionally, the iam:GetRole permission is not necessary for this scenario."",
    ""Create an IAM permissions policy attached to the role that allows access to DynamoDB. Add a trust policy to the role that allows Amazon EC2 to assume the role. Attach a permissions policy to the development group in AWS IAM that allows developers to use the iam:PassRole permission for the role."": ""Correct. This choice correctly sets up an IAM role with a permissions policy that allows access to DynamoDB and a trust policy that allows Amazon EC2 to assume the role. The iam:PassRole permission is necessary for developers to attach the role to EC2 instances, enabling the instances to access DynamoDB."",
    ""Create an IAM permission policy attached to the role that allows access to Amazon EC2. Add a trust policy to the role that allows DynamoDB to assume the role. Attach a permissions policy to the development group in AWS IAM that allows developers to use the iam:PassRole permission for the role."": ""Incorrect. The permission policy should allow access to DynamoDB, not Amazon EC2. The trust policy should allow Amazon EC2 to assume the role, not DynamoDB. The role is intended for EC2 instances to access DynamoDB."",
    ""Create an IAM permissions policy attached to the role that allows access to DynamoDB. Add a trust policy to the role that allows Amazon EC2 to assume the role. Attach a permissions policy to the development group in AWS IAM that allows developers to use the iam:GetRole permission for the role."": ""Incorrect. While the trust policy correctly allows Amazon EC2 to assume the role, the developers need the iam:PassRole permission, not just iam:GetRole, to attach the role to EC2 instances. The iam:GetRole permission alone is insufficient for this task.""
}
```"
"A developer converted an existing program to an AWS Lambda function in the console. The program runs properly on a local laptop, but shows an `Unable to import module` error when tested in the Lambda console. Which of the following can fix the error?","[""Install the missing module and specify the current directory as the target. Create a `ZIP` file to include all files under the current directory, and upload the `ZIP` file."", ""Install the missing module in a lib directory. Create a `ZIP` file to include all files under the lib directory, and upload the `ZIP` file as dependency file."", ""In the Lambda code, invoke a Linux command to install the missing modules under the `/usr/lib directory`."", ""In the Lambda console, create a `LB_LIBRARY_PATH` environment and specify the value for the system library plan.""]","[""Install the missing module and specify the current directory as the target. Create a `ZIP` file to include all files under the current directory, and upload the `ZIP` file.""]","```json
{
    ""Install the missing module and specify the current directory as the target. Create a `ZIP` file to include all files under the current directory, and upload the `ZIP` file."": ""This choice is correct because AWS Lambda requires all dependencies to be packaged with the function code in a deployment package. By installing the missing module in the current directory and creating a ZIP file that includes all files, the developer ensures that all necessary dependencies are available to the Lambda function when it is executed. This is a common practice when deploying Lambda functions that rely on external libraries or modules, as Lambda does not have access to the local environment's packages."",
    ""Install the missing module in a lib directory. Create a `ZIP` file to include all files under the lib directory, and upload the `ZIP` file as dependency file."": ""This choice is incorrect because simply placing the module in a 'lib' directory and uploading it separately as a dependency file does not ensure that the Lambda function will have access to it. Lambda functions require all dependencies to be included in the same deployment package as the function code, unless using Lambda Layers, which is not mentioned in this option."",
    ""In the Lambda code, invoke a Linux command to install the missing modules under the `/usr/lib directory`."": ""This choice is incorrect because Lambda functions do not have permission to modify the `/usr/lib` directory or install packages at runtime. Lambda functions run in a restricted environment where such operations are not allowed. Dependencies must be packaged with the function code before deployment."",
    ""In the Lambda console, create a `LB_LIBRARY_PATH` environment and specify the value for the system library plan."": ""This choice is incorrect because setting an `LB_LIBRARY_PATH` environment variable does not resolve the issue of missing modules. The problem is that the modules are not included in the deployment package, and setting environment variables does not change the fact that the required modules are not present in the Lambda execution environment.""
}
```"
A front-end web application is using Amazon Cognito user pools to handle the user authentication flow. A developer is integrating Amazon DynamoDB into the application using the AWS SDK for JavaScript. How would the developer securely call the API without exposing the access or secret keys?,"[""Configure Amazon Cognito identity pools and exchange the JSON Web Token (JWT) for temporary credentials."", ""Run the web application in an Amazon EC2 instance with the instance profile configured."", ""Hardcore the credentials, use Amazon S3 to host the web application, and enable server-side encryption."", ""Use Amazon Cognito user pool JSON Web Tokens (JWITs) to access the DynamoDB APIs.""]","[""Configure Amazon Cognito identity pools and exchange the JSON Web Token (JWT) for temporary credentials.""]","```json
{
    ""Configure Amazon Cognito identity pools and exchange the JSON Web Token (JWT) for temporary credentials."": ""This is the correct choice. Amazon Cognito identity pools allow you to obtain temporary AWS credentials for users who have been authenticated via a user pool. This is a secure method because it avoids hardcoding credentials in the application and uses AWS IAM roles to define permissions, adhering to the principle of least privilege. This approach is recommended in scenarios where you need to securely access AWS resources from a client-side application, as it leverages AWS STS to provide temporary, limited-privilege credentials."",
    ""Run the web application in an Amazon EC2 instance with the instance profile configured."": ""This choice is incorrect in the context of a front-end web application. Instance profiles are used to provide applications running on EC2 instances with AWS credentials, but they are not suitable for client-side web applications. The question specifies a front-end web application, which typically runs in the user's browser, not on an EC2 instance. Therefore, this approach does not apply to the scenario described."",
    ""Hardcode the credentials, use Amazon S3 to host the web application, and enable server-side encryption."": ""This choice is incorrect because hardcoding AWS credentials in a client-side application is a security risk. Even if the application is hosted on Amazon S3 with server-side encryption, the credentials could be exposed to users who inspect the application's code. AWS best practices strongly advise against hardcoding credentials in client-side applications due to the risk of credential leakage."",
    ""Use Amazon Cognito user pool JSON Web Tokens (JWITs) to access the DynamoDB APIs."": ""This choice is incorrect because Amazon Cognito user pool JWTs are used for authentication, not for directly accessing AWS services like DynamoDB. While JWTs can be used to authenticate users, they need to be exchanged for temporary AWS credentials via an identity pool to securely access AWS resources. Directly using JWTs to access DynamoDB APIs is not supported.""
}
```"
"A developer needs to manage AWS infrastructure as code and must be able to deploy multiple identical copies of the infrastructure, stage changes, and revert to previous versions. Which approach addresses these requirements?","[""Use cost allocation reports and AWS OpsWorks to deploy and manage the infrastructure."", ""Use Amazon CloudWatch metrics and alerts along with resource tagging to deploy and manage the infrastructure."", ""Use AWS Elastic Beanstalk and AWS CodeCommit to deploy and manage the infrastructure."", ""Use AWS CloudFormation and AWS CodeCommit to deploy and manage the infrastructure.""]","[""Use AWS CloudFormation and AWS CodeCommit to deploy and manage the infrastructure.""]","```json
{
    ""Use cost allocation reports and AWS OpsWorks to deploy and manage the infrastructure."": ""Incorrect. Cost allocation reports are used for tracking and managing AWS costs, not for deploying infrastructure. AWS OpsWorks is a configuration management service that can automate server configurations but does not inherently support infrastructure as code, versioning, or staging changes in the way CloudFormation does."",
    ""Use Amazon CloudWatch metrics and alerts along with resource tagging to deploy and manage the infrastructure."": ""Incorrect. Amazon CloudWatch is primarily used for monitoring and logging AWS resources, while resource tagging helps in organizing and managing resources. Neither of these services provides infrastructure as code capabilities, version control, or the ability to stage changes and revert to previous versions."",
    ""Use AWS Elastic Beanstalk and AWS CodeCommit to deploy and manage the infrastructure."": ""Incorrect. AWS Elastic Beanstalk is a platform as a service (PaaS) that simplifies application deployment but is not designed for managing infrastructure as code. AWS CodeCommit is a source control service but does not provide infrastructure management capabilities like AWS CloudFormation."",
    ""Use AWS CloudFormation and AWS CodeCommit to deploy and manage the infrastructure."": ""Correct. AWS CloudFormation allows you to define and provision AWS infrastructure as code, enabling you to deploy multiple identical copies, stage changes, and revert to previous versions. AWS CodeCommit is a fully managed source control service that integrates well with CloudFormation for version control of infrastructure templates. This combination supports best practices for infrastructure management.""
}
```"
A Developer needs to deploy an application running on AWS Fargate using Amazon ECS. The application has environment variables that must be passed to a container for the application to initialize. How should the environment variables be passed to the container?,"[""Define an array that includes the environment variables under the environment parameter within the service definition."", ""Define an array that includes the environment variables under the environment parameter within the task definition."", ""Define an array that includes the environment variables under the entryPoint parameter within the task definition."", ""Define an array that includes the environment variables under the entryPoint parameter within the service definition.""]","[""Define an array that includes the environment variables under the environment parameter within the task definition.""]","```json
{
    ""Define an array that includes the environment variables under the environment parameter within the service definition."": ""This choice is incorrect. In Amazon ECS, environment variables are specified in the task definition, not the service definition. The service definition is used to specify how many tasks to run and other service-level configurations, but it does not handle container-specific configurations like environment variables."",
    ""Define an array that includes the environment variables under the environment parameter within the task definition."": ""This choice is correct. In Amazon ECS, environment variables for containers are defined in the task definition under the 'containerDefinitions' section. The 'environment' parameter is used to pass environment variables to the container. This is a key part of configuring the container's runtime environment and is supported by AWS documentation as the correct method for passing environment variables to a container in ECS."",
    ""Define an array that includes the environment variables under the entryPoint parameter within the task definition."": ""This choice is incorrect. The 'entryPoint' parameter in a task definition is used to specify the command that is run when the container starts, not for passing environment variables. Environment variables should be specified under the 'environment' parameter within the task definition."",
    ""Define an array that includes the environment variables under the entryPoint parameter within the service definition."": ""This choice is incorrect. Similar to the previous explanation, the 'entryPoint' parameter is not used for environment variables, and the service definition does not handle container-specific configurations like environment variables. The correct place to define environment variables is within the task definition under the 'environment' parameter.""
}
```"
"A company's fleet of Amazon EC2 instances receives data from millions of users through an API. The servers batch the data, add an object for each user, and upload the objects to an S3 bucket to ensure high access rates. The object attributes are Customer ID, Server ID, TS-Server (TimeStamp and Server ID), the size of the object, and a timestamp. A Developer wants to find all the objects for a given user collected during a specified time range. After creating an S3 object created event, how can the Developer achieve this requirement?","[""Execute an AWS Lambda function in response to the S3 object creation events that creates an Amazon DynamoDB record for every object with the Customer ID as the partition key and the Server ID as the sort key. Retrieve all the records using the Customer ID and Server ID attributes."", ""Execute an AWS Lambda function in response to the S3 object creation events that creates an Amazon Redshift record for every object with the Customer ID as the partition key and TS-Server as the sort key. Retrieve all the records using the Customer ID and TS-Server attributes."", ""Execute an AWS Lambda function in response to the S3 object creation events that creates an Amazon DynamoDB record for every object with the Customer ID as the partition key and TS-Server as the sort key. Retrieve all the records using the Customer ID and TS-Server attributes."", ""Execute an AWS Lambda function in response to the S3 object creation events that creates an Amazon Redshift record for every object with the Customer ID as the partition key and the Server ID as the sort key. Retrieve all the records using the Customer ID and Server ID attributes.""]","[""Execute an AWS Lambda function in response to the S3 object creation events that creates an Amazon DynamoDB record for every object with the Customer ID as the partition key and TS-Server as the sort key. Retrieve all the records using the Customer ID and TS-Server attributes.""]","```json
{
    ""Execute an AWS Lambda function in response to the S3 object creation events that creates an Amazon DynamoDB record for every object with the Customer ID as the partition key and the Server ID as the sort key. Retrieve all the records using the Customer ID and Server ID attributes."": ""This choice is incorrect because using the Server ID as the sort key does not allow for efficient querying based on a time range. The requirement is to find objects for a given user within a specified time range, which is not facilitated by using Server ID as the sort key. Instead, the TS-Server attribute, which includes a timestamp, should be used for sorting to enable time-based queries."",
    ""Execute an AWS Lambda function in response to the S3 object creation events that creates an Amazon Redshift record for every object with the Customer ID as the partition key and TS-Server as the sort key. Retrieve all the records using the Customer ID and TS-Server attributes."": ""This choice is incorrect because Amazon Redshift is a data warehouse service optimized for complex analytical queries and not for transactional queries or real-time data processing. The requirement is to efficiently retrieve records based on a time range, which is better suited for Amazon DynamoDB with its fast, scalable NoSQL database capabilities. Additionally, using Redshift for this use case would be overkill and not cost-effective."",
    ""Execute an AWS Lambda function in response to the S3 object creation events that creates an Amazon DynamoDB record for every object with the Customer ID as the partition key and TS-Server as the sort key. Retrieve all the records using the Customer ID and TS-Server attributes."": ""This choice is correct because it uses Amazon DynamoDB, which is well-suited for handling high-velocity data and provides efficient querying capabilities. By using Customer ID as the partition key and TS-Server (which includes a timestamp) as the sort key, the developer can efficiently query the records for a specific user within a specified time range. This approach leverages DynamoDB's strengths in handling time-based queries and scalability, aligning with AWS best practices for this type of workload."",
    ""Execute an AWS Lambda function in response to the S3 object creation events that creates an Amazon Redshift record for every object with the Customer ID as the partition key and the Server ID as the sort key. Retrieve all the records using the Customer ID and Server ID attributes."": ""This choice is incorrect because, similar to the second choice, it uses Amazon Redshift, which is not suitable for the requirement of querying data based on a time range in a transactional manner. Additionally, using Server ID as the sort key does not facilitate efficient time-based queries. The use case is better served by Amazon DynamoDB, which can efficiently handle the required query patterns.""
}
```"
"A company is managing a NoSQL database on-premises to host a critical component of an application, which is starting to have scaling issues. The company wants to migrate the application to Amazon DynamoDB with the following considerations: Optimize frequent queries. Reduce read latencies. Plan for frequent queries on certain key attributes of the table. Which solution would help achieve these objectives?","[""Create global secondary indexes on keys that are frequently queried. Add the necessary attributes into the indexes."", ""Create local secondary indexes on keys that are frequently queried. DynamoDB will fetch needed attributes from the table."", ""Create DynamoDB global tables to speed up query responses. Use a scan to fetch data from the table."", ""Create an AWS Auto Scaling policy for the DynamoDB table.""]","[""Create global secondary indexes on keys that are frequently queried. Add the necessary attributes into the indexes.""]","```json
{
    ""Create global secondary indexes on keys that are frequently queried. Add the necessary attributes into the indexes."": ""This choice is correct because Global Secondary Indexes (GSIs) in DynamoDB allow for querying on non-primary key attributes, which optimizes frequent queries and reduces read latencies. GSIs can be created on any attribute and can include additional attributes to be projected into the index, which means the query can be satisfied without accessing the main table, thus reducing latency. This aligns with the requirement to optimize frequent queries and reduce read latencies."",
    ""Create local secondary indexes on keys that are frequently queried. DynamoDB will fetch needed attributes from the table."": ""This choice is incorrect because Local Secondary Indexes (LSIs) are only available at the time of table creation and are limited to the same partition key as the base table, which may not suit the needs for optimizing queries on different attributes. Additionally, LSIs are less flexible than GSIs for handling diverse query patterns, especially when the query patterns are not known at the time of table creation."",
    ""Create DynamoDB global tables to speed up query responses. Use a scan to fetch data from the table."": ""This choice is incorrect because while DynamoDB Global Tables provide multi-region replication for high availability and low latency across regions, they do not inherently optimize query performance or reduce read latencies for specific queries. Using a scan operation is inefficient for frequent queries as it reads every item in the table, which can be slow and costly, especially for large datasets."",
    ""Create an AWS Auto Scaling policy for the DynamoDB table."": ""This choice is incorrect because Auto Scaling in DynamoDB helps manage throughput capacity to handle varying workloads but does not directly optimize query performance or reduce latencies for specific queries. Auto Scaling adjusts read and write capacity based on traffic patterns but does not address the need for efficient querying of specific attributes.""
}
```"
"A developer is writing an application that will process data delivered into an Amazon S3 bucket. The data is delivered approximately 10 times a day, and the developer expects the data will be processed in less than 1 minute, on average. How can the developer deploy and invoke the application with the lowest cost and lowest latency?","[""Deploy the application as an AWS Lambda function and invoke it with an Amazon CloudWatch alarm triggered by an S3 object upload."", ""Deploy the application as an AWS Lambda function and invoke it with an S3 event notification."", ""Deploy the application as an AWS Lambda function and invoke it with an Amazon CloudWatch scheduled event."", ""Deploy the application onto an Amazon EC2 instance and have it poll the S3 bucket for new objects.""]","[""Deploy the application as an AWS Lambda function and invoke it with an S3 event notification.""]","```json
{
    ""Deploy the application as an AWS Lambda function and invoke it with an Amazon CloudWatch alarm triggered by an S3 object upload."": ""This option is incorrect because Amazon CloudWatch Alarms are not designed to trigger Lambda functions based on S3 object uploads. CloudWatch Alarms are typically used to monitor metrics and trigger actions based on those metrics, not for event-driven architectures like S3 uploads. Using CloudWatch Alarms would introduce unnecessary complexity and latency."",
    ""Deploy the application as an AWS Lambda function and invoke it with an S3 event notification."": ""This option is correct because S3 event notifications are specifically designed to trigger AWS Lambda functions when objects are created in an S3 bucket. This provides a low-latency, cost-effective, and serverless way to process data as soon as it arrives. It aligns with AWS best practices for event-driven architectures, where Lambda functions are invoked in response to specific events, such as S3 object creation."",
    ""Deploy the application as an AWS Lambda function and invoke it with an Amazon CloudWatch scheduled event."": ""This option is incorrect because using a CloudWatch scheduled event would mean invoking the Lambda function at regular intervals, regardless of whether new data is available. This could lead to unnecessary invocations and increased costs, as the function would run even when there is no data to process. It is not the most efficient or cost-effective solution for processing data based on S3 uploads."",
    ""Deploy the application onto an Amazon EC2 instance and have it poll the S3 bucket for new objects."": ""This option is incorrect because deploying an application on an EC2 instance to poll an S3 bucket introduces higher costs and latency compared to using AWS Lambda. EC2 instances incur costs even when idle, and polling for new objects is less efficient and slower than using event-driven triggers like S3 event notifications. This approach does not leverage the serverless and on-demand capabilities of AWS Lambda.""
}
```"
A company is using Amazon API Gateway to manage its public-facing API. The CISO requires that the APIs be used by test account users only. What is the MOST secure way to restrict API access to users of this particular AWS account?,"[""Client-side SSL certificates for authentication."", ""API Gateway resource policies."", ""Cross-origin resource sharing (CORS)."", ""Usage plans.""]","[""API Gateway resource policies.""]","```json
{
    ""Client-side SSL certificates for authentication."": ""Client-side SSL certificates are used to authenticate clients to the server, ensuring secure communication between the client and the API Gateway. However, they do not inherently restrict access to users of a specific AWS account. This method focuses on securing the connection rather than restricting access based on account identity, making it incorrect for the requirement to restrict API access to a specific AWS account."",
    ""API Gateway resource policies."": ""API Gateway resource policies are the correct choice because they allow you to control access to your API based on the source IP address, VPC, or AWS account. By using resource policies, you can explicitly allow or deny access to your API from specific AWS accounts, making it the most secure way to restrict API access to users of a particular AWS account. This aligns with AWS best practices for securing APIs by using IAM policies and resource policies."",
    ""Cross-origin resource sharing (CORS)."": ""CORS is a security feature implemented by web browsers to allow or restrict resources requested from another domain. It is primarily used to control how web applications interact with resources from different origins. While CORS can help manage access from web browsers, it does not restrict API access based on AWS account identity, making it unsuitable for the requirement to restrict access to a specific AWS account."",
    ""Usage plans."": ""Usage plans in API Gateway are used to manage and throttle API usage by defining request limits and quotas for individual API keys. They are useful for controlling the rate of API consumption but do not inherently restrict access based on AWS account identity. Usage plans are more about managing API consumption rather than securing access based on account, which makes them incorrect for this requirement.""
}
```"
"A Developer is migrating existing applications to AWS. These applications use MongoDB as their primary data store, and they will be deployed to Amazon EC2 instances. Management requires that the Developer minimize changes to applications while using AWS services. Which solution should the Developer use to host MongoDB in AWS?","[""Install MongoDB on the same instance where the application is running."", ""Deploy Amazon DocumentDB in MongoDB compatibility mode."", ""Use Amazon API Gateway to translate API calls from MongoDB to Amazon DynamoDB."", ""Replicate the existing MongoDB workload to Amazon DynamoDB.""]","[""Deploy Amazon DocumentDB in MongoDB compatibility mode.""]","```json
{
    ""Install MongoDB on the same instance where the application is running."": ""This option is incorrect because installing MongoDB on the same EC2 instance as the application can lead to resource contention, affecting performance and scalability. It also does not leverage managed AWS services, which can simplify operations and improve reliability."",
    ""Deploy Amazon DocumentDB in MongoDB compatibility mode."": ""This option is correct because Amazon DocumentDB is a managed database service that is compatible with MongoDB, allowing the Developer to minimize changes to the application. It provides scalability, high availability, and automated backups, aligning with AWS best practices for managed services."",
    ""Use Amazon API Gateway to translate API calls from MongoDB to Amazon DynamoDB."": ""This option is incorrect because Amazon API Gateway is not designed to translate MongoDB API calls to DynamoDB. Additionally, DynamoDB is a different type of database (NoSQL key-value store) and would require significant changes to the application to accommodate its data model."",
    ""Replicate the existing MongoDB workload to Amazon DynamoDB."": ""This option is incorrect because replicating a MongoDB workload to DynamoDB would require significant changes to the application's data model and queries. DynamoDB is a NoSQL key-value store, which is fundamentally different from MongoDB's document-based model, making this a non-trivial migration.""
}
```"
A company requires that AWS Lambda functions written by Developers log errors so System Administrators can more effectively troubleshoot issues. What should the Developers implement to meet this need?,"[""Publish errors to a dedicated Amazon SQS queue."", ""Create an Amazon CloudWatch Events event trigger based on certain Lambda events."", ""Report errors through logging statements in Lambda function code."", ""Set up an Amazon SNS topic that sends logging statements upon failure.""]","[""Create an Amazon CloudWatch Events event trigger based on certain Lambda events.""]","```json
{
    ""Publish errors to a dedicated Amazon SQS queue."": ""This choice is incorrect because while SQS can be used to queue messages, it is not specifically designed for logging or monitoring errors. SQS is a message queuing service that decouples microservices, distributed systems, and serverless applications. It would require additional infrastructure to process and analyze these messages, which is not efficient for logging purposes."",
    ""Create an Amazon CloudWatch Events event trigger based on certain Lambda events."": ""This choice is incorrect as well. Although CloudWatch Events can trigger actions based on changes in your AWS environment, it is not specifically used for logging errors from Lambda functions. The primary purpose of CloudWatch Events is to respond to changes in your AWS resources, not to log or monitor application errors directly."",
    ""Report errors through logging statements in Lambda function code."": ""This choice is correct. Developers should use logging statements within their Lambda function code to log errors. AWS Lambda automatically integrates with Amazon CloudWatch Logs, allowing developers to view, search, and analyze logs. This is the standard and recommended practice for logging errors in AWS Lambda, as it provides a centralized and scalable solution for monitoring and troubleshooting."",
    ""Set up an Amazon SNS topic that sends logging statements upon failure."": ""This choice is incorrect because SNS is primarily used for sending notifications, not for logging. While SNS can notify administrators of errors, it does not provide a logging mechanism. Using SNS for logging would require additional setup and does not offer the same level of integration and ease of use as CloudWatch Logs for monitoring Lambda function errors.""
}
```"
A Developer is writing an application that runs on Amazon EC2 instances in an Auto Scaling group. The application data is stored in an Amazon DynamoDB table and records are constantly updated by all instances. An instance sometimes retrieves old data. The Developer wants to correct this by making sure the reads are strongly consistent. How can the Developer accomplish this?,"[""Set ConsistentRead to true when calling Getltem."", ""Create a new DynamoDB Accelerator (DAX) table."", ""Set Consistency to strong when calling UpdateTable."", ""Use the GetShardIterator command.""]","[""Set ConsistentRead to true when calling Getltem.""]","```json
{
    ""Set ConsistentRead to true when calling Getltem."": ""This choice is correct because Amazon DynamoDB provides two read consistency models: eventually consistent reads and strongly consistent reads. By default, reads are eventually consistent. To ensure that the reads are strongly consistent, the 'ConsistentRead' parameter must be set to true when calling the GetItem operation. This ensures that the read operation reflects all writes that received a successful response prior to the read."",
    ""Create a new DynamoDB Accelerator (DAX) table."": ""This choice is incorrect because DAX is a caching solution for DynamoDB that can improve read performance by reducing response times for eventually consistent reads. However, it does not provide strong consistency. DAX is designed to accelerate read-heavy and bursty workloads, but it does not change the consistency model of the underlying DynamoDB table."",
    ""Set Consistency to strong when calling UpdateTable."": ""This choice is incorrect because there is no 'Consistency' parameter in the UpdateTable operation. The consistency model in DynamoDB is controlled at the read operation level (such as GetItem or Query) by specifying the 'ConsistentRead' parameter, not at the table configuration level."",
    ""Use the GetShardIterator command."": ""This choice is incorrect because GetShardIterator is a command used with Amazon Kinesis Data Streams, not DynamoDB. It is used to get an iterator for reading data records from a shard in a Kinesis stream. It has no relation to read consistency in DynamoDB.""
}
```"
A Developer has an application that must accept a large amount of incoming data streams and process the data before sending it to many downstream users. Which serverless solution should the Developer use to meet these requirements?,"[""Amazon RDS MySQL stored procedure with AWS Lambda."", ""AWS Direct Connect with AWS Lambda."", ""Amazon Kinesis Data Streams with AWS Lambda."", ""Amazon EC2 bash script with AWS Lambda.""]","[""Amazon Kinesis Data Streams with AWS Lambda.""]","```json
{
    ""Amazon RDS MySQL stored procedure with AWS Lambda."": ""This choice is incorrect because Amazon RDS MySQL stored procedures are not designed for handling large amounts of incoming data streams. RDS is a relational database service, and while it can store and process data, it is not optimized for real-time data streaming and processing. AWS Lambda can be used to trigger actions based on database events, but this setup is not ideal for the described use case of processing large data streams and distributing them to downstream users."",
    ""AWS Direct Connect with AWS Lambda."": ""This choice is incorrect because AWS Direct Connect is a network service that provides a dedicated network connection from your premises to AWS. It is not related to processing data streams or integrating with AWS Lambda for such purposes. Direct Connect is used for high-bandwidth, low-latency connections to AWS, not for data stream processing."",
    ""Amazon Kinesis Data Streams with AWS Lambda."": ""This choice is correct because Amazon Kinesis Data Streams is a service specifically designed for real-time processing of streaming data at scale. It allows developers to continuously capture gigabytes of data per second from hundreds of thousands of sources such as website clickstreams, database event streams, and social media feeds. AWS Lambda can be used to process this data in real-time by triggering Lambda functions in response to new data records in the stream. This serverless architecture is ideal for the described use case of processing large incoming data streams and sending processed data to downstream users. It aligns with AWS best practices for building scalable, real-time data processing applications."",
    ""Amazon EC2 bash script with AWS Lambda."": ""This choice is incorrect because while Amazon EC2 can run bash scripts, it is not a serverless solution and requires management of the underlying infrastructure. EC2 instances are not inherently designed for handling large-scale real-time data streams in a serverless manner. AWS Lambda is serverless, but combining it with EC2 for this purpose does not leverage the full benefits of serverless architectures, such as automatic scaling and reduced operational overhead.""
}
```"
An application is experiencing performance issues based on increased demand. This increased demand is on read-only historical records pulled from an Amazon RDS-hosted database with custom views and queries. A Developer must improve performance without changing the database structure. Which approach will improve performance and MINIMIZE management overhead?,"[""Deploy Amazon DynamoDB, move all the data, and point to DynamoDB."", ""Deploy Amazon ElastiCache for Redis and cache the data for the application."", ""Deploy Memcached on Amazon EC2 and cache the data for the application."", ""Deploy Amazon DynamoDB Accelerator (DAX) on Amazon RDS to improve cache performance.""]","[""Deploy Amazon ElastiCache for Redis and cache the data for the application.""]","```json
{
    ""Deploy Amazon DynamoDB, move all the data, and point to DynamoDB."": ""This choice is incorrect because moving all data to DynamoDB would require significant changes to the application and database structure, which contradicts the requirement to not change the database structure. Additionally, DynamoDB is a NoSQL database, which may not support the custom views and queries used in the RDS-hosted database. This approach would also involve significant management overhead in terms of data migration and application refactoring."",
    ""Deploy Amazon ElastiCache for Redis and cache the data for the application."": ""This choice is correct because Amazon ElastiCache for Redis can be used to cache frequently accessed data, reducing the load on the RDS database and improving application performance. Redis supports complex data types and operations, making it suitable for caching the results of custom views and queries. This approach minimizes management overhead as ElastiCache is a fully managed service, and it does not require changes to the existing database structure. AWS best practices recommend using caching to improve read performance for applications with high read demand."",
    ""Deploy Memcached on Amazon EC2 and cache the data for the application."": ""This choice is incorrect because deploying Memcached on Amazon EC2 involves more management overhead compared to using a fully managed service like Amazon ElastiCache. Managing EC2 instances requires handling scaling, patching, and availability, which increases operational complexity. While Memcached can be used for caching, it lacks some of the advanced features of Redis, such as data persistence and support for complex data types, which may be beneficial for this use case."",
    ""Deploy Amazon DynamoDB Accelerator (DAX) on Amazon RDS to improve cache performance."": ""This choice is incorrect because Amazon DynamoDB Accelerator (DAX) is specifically designed to improve the performance of DynamoDB tables, not Amazon RDS databases. DAX cannot be deployed on RDS, as it is a caching solution for DynamoDB. This choice reflects a misunderstanding of the purpose and compatibility of DAX, which is not applicable to the RDS-hosted database in this scenario.""
}
```"
A Developer has an Amazon DynamoDB table that must be in provisioned mode to comply with user requirements. The application needs to support the following: Average item size: 10 KB Item reads each second: 10 strongly consistent Item writes each second: 2 transactional Which read and write capacity cost-effectively meets these requirements?,"[""Read 10; write 2."", ""Read 30; write 40."", ""Use on-demand scaling."", ""Read 300; write 400.""]","[""Read 30; write 40.""]","```json
{
    ""Read 10; write 2."": ""This choice is incorrect because it does not account for the need for strongly consistent reads and transactional writes. For strongly consistent reads, each read request requires twice the capacity units compared to eventually consistent reads. Therefore, 10 strongly consistent reads per second would require 20 read capacity units. Additionally, transactional writes require twice the write capacity units, so 2 transactional writes per second would require 4 write capacity units."",
    ""Read 30; write 40."": ""This choice is correct because it accurately calculates the necessary read and write capacity units for the given requirements. For 10 strongly consistent reads per second of 10 KB items, 20 read capacity units are needed (since each read of 10 KB requires 2 units, and strongly consistent reads double that requirement). For 2 transactional writes per second of 10 KB items, 4 write capacity units are needed (since each write of 10 KB requires 2 units, and transactional writes double that requirement). However, the choice provides a buffer to accommodate any additional load or spikes, which is a best practice for cost-effectiveness and performance."",
    ""Use on-demand scaling."": ""This choice is incorrect because the question specifies that the table must be in provisioned mode. On-demand mode is a different billing model that automatically scales capacity based on demand, but it does not meet the requirement for provisioned mode. Provisioned mode requires explicitly setting read and write capacity units."",
    ""Read 300; write 400."": ""This choice is incorrect because it significantly overestimates the required capacity, leading to unnecessary costs. The calculated needs are 20 read capacity units and 4 write capacity units for the specified workload, so allocating 300 read and 400 write units is excessive and not cost-effective. This choice does not align with best practices for cost management in AWS.""
}
```"
A company wants to containerize an existing three-tier web application and deploy it to Amazon ECS Fargate. The application is using session data to keep track of user activities. Which approach would provide the BEST user experience?,"[""Provision a Redis cluster in Amazon ElastiCache and save the session data in the cluster."", ""Create a session table in Amazon Redshift and save the session data in the database table."", ""Enable session stickiness in the existing Network Load Balancer and manage the session data in the container."", ""Use an Amazon S3 bucket as data store and save the session data in the bucket.""]","[""Enable session stickiness in the existing Network Load Balancer and manage the session data in the container.""]","```json
{
    ""Provision a Redis cluster in Amazon ElastiCache and save the session data in the cluster."": ""This choice is incorrect because while Redis in Amazon ElastiCache is a great option for caching and managing session data due to its low latency and high throughput, it does not inherently provide the best user experience in terms of session persistence across container restarts or failures. It requires additional configuration and management to ensure data consistency and availability."",
    ""Create a session table in Amazon Redshift and save the session data in the database table."": ""This choice is incorrect because Amazon Redshift is a data warehousing service designed for analytical queries and not optimized for handling high-frequency session data updates. Using Redshift for session data would lead to increased latency and poor performance, negatively impacting user experience."",
    ""Enable session stickiness in the existing Network Load Balancer and manage the session data in the container."": ""This choice is correct because enabling session stickiness (also known as session affinity) on a Network Load Balancer ensures that user requests are consistently routed to the same container instance, allowing session data to be managed within the container. This approach simplifies session management and improves user experience by maintaining session continuity even if the application is scaled across multiple containers."",
    ""Use an Amazon S3 bucket as data store and save the session data in the bucket."": ""This choice is incorrect because Amazon S3 is an object storage service optimized for storing and retrieving large amounts of data, not for frequent read/write operations required for session data. Using S3 for session data would result in high latency and poor performance, adversely affecting user experience.""
}
```"
"An application is using a single-node Amazon ElastiCache for Redis instance to improve read performance. Over time, demand for the application has increased exponentially, which has increased the load on the ElastiCache instance. It is critical that this cache layer handles the load and is resilient in case of node failures. What can the Developer do to address the load and resiliency requirements?","[""Add a read replica instance."", ""Migrate to a Memcached cluster."", ""Migrate to an Amazon Elasticsearch Service cluster."", ""Vertically scale the ElastiCache instance.""]","[""Add a read replica instance.""]","```json
{
    ""Add a read replica instance."": ""This is the correct choice because adding a read replica to an Amazon ElastiCache for Redis instance helps distribute the read load across multiple nodes, improving read performance and providing redundancy in case of node failures. Redis read replicas can be used to scale out read-heavy workloads and enhance resilience, as they can take over in case the primary node fails. This aligns with AWS best practices for scaling and resilience."",
    ""Migrate to a Memcached cluster."": ""This choice is incorrect because Memcached is a different caching engine that does not support replication or persistence, which are critical for handling increased load and ensuring resiliency. Migrating to Memcached would not address the need for failover and redundancy as effectively as Redis with read replicas."",
    ""Migrate to an Amazon Elasticsearch Service cluster."": ""This choice is incorrect because Amazon Elasticsearch Service is designed for search and analytics, not as a caching solution. Migrating to Elasticsearch would not address the caching needs of the application and would not improve read performance or resiliency in the context of caching."",
    ""Vertically scale the ElastiCache instance."": ""While vertically scaling the ElastiCache instance (increasing the instance size) can temporarily address performance issues, it does not inherently provide resiliency or failover capabilities. In the event of a node failure, a single larger instance would still be a single point of failure, which does not meet the resiliency requirement.""
}
```"
"A Developer is investigating an application's performance issues. The application consists of hundreds of microservices, and a single API call can potentially have a deep call stack. The Developer must isolate the component that is causing the issue. Which AWS service or feature should the Developer use to gather information about what is happening and isolate the fault?","[""AWS X-Ray."", ""VPC Flow Logs."", ""Amazon GuardDuty."", ""Amazon Macie.""]","[""AWS X-Ray.""]","{""AWS X-Ray."": ""AWS X-Ray is the correct choice because it is specifically designed for tracing and analyzing requests as they travel through an application. It helps developers understand how their application and its underlying services are performing by providing insights into the application's architecture, including a map of the application's components. X-Ray is particularly useful for applications with a microservices architecture, as it can trace requests across different services and identify bottlenecks or errors in the call stack. This aligns with the need to isolate the component causing performance issues in a deep call stack, as mentioned in the question."", ""VPC Flow Logs."": ""VPC Flow Logs is incorrect because it is used for capturing information about the IP traffic going to and from network interfaces in a VPC. While it can provide insights into network traffic and security, it does not offer the detailed application-level tracing and performance analysis needed to isolate issues within a microservices architecture. It does not provide the ability to trace API calls or understand the interactions between microservices."", ""Amazon GuardDuty."": ""Amazon GuardDuty is incorrect because it is a threat detection service that continuously monitors for malicious or unauthorized behavior to help protect AWS accounts and workloads. It is focused on security threats rather than application performance or tracing. It does not provide the capability to trace requests or analyze the performance of microservices, which is necessary for isolating performance issues."", ""Amazon Macie."": ""Amazon Macie is incorrect because it is a data security service that uses machine learning to automatically discover, classify, and protect sensitive data in AWS. It is not designed for application performance monitoring or tracing. Macie focuses on data security and compliance rather than diagnosing performance issues in applications, making it unsuitable for the scenario described in the question.""}"
A Company runs continuous integration/continuous delivery (CI/CD) pipelines for its application on AWS CodePipeline. A Developer must write unit tests and run them as part of the pipelines before staging the artifacts for testing. How should the Developer incorporate unit tests as part of CI/CD pipelines?,"[""Create a separate CodePipeline pipeline to run unit tests."", ""Update the AWS CodeBuild specification to include a phase for running unit tests."", ""Install the AWS CodeDeploy agent on an Amazon EC2 instance to run unit tests."", ""Create a testing branch in AWS CodeCommit to run unit tests.""]","[""Update the AWS CodeBuild specification to include a phase for running unit tests.""]","```json
{
    ""Create a separate CodePipeline pipeline to run unit tests."": ""This choice is incorrect because creating a separate CodePipeline just for running unit tests is not an efficient use of resources and does not align with best practices for CI/CD. Unit tests should be integrated into the existing pipeline to ensure they are part of the same build and test process. This approach would also complicate the management and orchestration of the CI/CD process."",
    ""Update the AWS CodeBuild specification to include a phase for running unit tests."": ""This choice is correct because AWS CodeBuild is designed to run build and test commands as part of a CI/CD pipeline. By updating the buildspec.yml file to include a phase for running unit tests, developers can ensure that tests are executed automatically as part of the build process. This aligns with best practices for CI/CD, where testing is integrated into the pipeline to catch issues early. AWS CodeBuild provides the flexibility to define multiple phases, including install, pre_build, build, and post_build, which can be used to run unit tests."",
    ""Install the AWS CodeDeploy agent on an Amazon EC2 instance to run unit tests."": ""This choice is incorrect because AWS CodeDeploy is primarily used for deploying applications to various compute services like EC2, Lambda, or on-premises servers. It is not designed to run unit tests. Unit tests should be part of the build process, which is handled by AWS CodeBuild, not CodeDeploy. Using CodeDeploy for running tests would be an inappropriate use of the service."",
    ""Create a testing branch in AWS CodeCommit to run unit tests."": ""This choice is incorrect because creating a separate branch for running unit tests is not a standard practice in CI/CD pipelines. Unit tests should be run as part of the main pipeline to ensure that every code change is tested before it is staged or deployed. Branches in CodeCommit are typically used for feature development, not for running tests separately from the main CI/CD process.""
}
```"
An application has the following requirements: Performance efficiency of seconds with up to a minute of latency. The data storage size may grow up to thousands of terabytes. Per-message sizes may vary between 100 KB and 100 MB. Data can be stored as key/value stores supporting eventual consistency. What is the MOST cost-effective AWS service to meet these requirements?,"[""Amazon DynamoDB."", ""Amazon S3."", ""Amazon RDS (with a MySQL engine)."", ""Amazon ElastiCache.""]","[""Amazon DynamoDB.""]","{""Amazon DynamoDB."": ""Amazon DynamoDB is the correct choice because it is a fully managed NoSQL database service that provides fast and predictable performance with seamless scalability. It supports key/value data models and can handle large amounts of data, making it suitable for applications with storage requirements in the thousands of terabytes. DynamoDB supports eventual consistency, which aligns with the application's requirements. Additionally, it can handle varying per-message sizes efficiently. DynamoDB's on-demand and provisioned capacity modes offer cost-effective solutions for different usage patterns, aligning with the application's need for cost-effectiveness."", ""Amazon S3."": ""Amazon S3 is a highly durable and scalable object storage service, but it is not optimized for key/value store operations with performance efficiency requirements of seconds. While S3 can store large amounts of data and handle varying object sizes, it is primarily used for object storage rather than as a database service. S3 does not natively support key/value store operations with eventual consistency in the same way that DynamoDB does, making it less suitable for this application's requirements."", ""Amazon RDS (with a MySQL engine)."": ""Amazon RDS with a MySQL engine is a relational database service that provides managed MySQL databases. While it can handle large data sizes, it is not as cost-effective as DynamoDB for key/value store use cases, especially when dealing with eventual consistency and varying message sizes. RDS is optimized for structured data and complex queries, which are not requirements for this application. Additionally, the performance efficiency requirement of seconds with up to a minute of latency may not be as easily achieved with RDS compared to DynamoDB's capabilities."", ""Amazon ElastiCache."": ""Amazon ElastiCache is a service that provides in-memory data stores, such as Redis or Memcached, which are optimized for low-latency data access. While ElastiCache can provide high performance, it is not designed for storing large datasets up to thousands of terabytes, as it is primarily used for caching frequently accessed data. Additionally, ElastiCache does not natively support eventual consistency or key/value store operations at the scale required by the application, making it an unsuitable choice for this use case.""}"
A Developer must allow guest users without logins to access an Amazon Cognito-enabled site to view files stored within an Amazon S3 bucket. How should the Developer meet these requirements?,"[""Create a blank user ID in a user pool, add to the user group, and grant access to AWS resources."", ""Create a new identity pool, enable access to authenticated identities, and grant access to AWS resources."", ""Create a new user pool, enable access to authenticated identifies, and grant access to AWS resources."", ""Create a new user pool, disable authentication access, and grant access to AWS resources.""]","[""Create a new identity pool, enable access to authenticated identities, and grant access to AWS resources.""]","```json
{
    ""Create a blank user ID in a user pool, add to the user group, and grant access to AWS resources."": ""This choice is incorrect because user pools in Amazon Cognito are used for authentication of users, not for providing unauthenticated access. Creating a blank user ID does not align with the requirement of allowing guest users without logins. User pools are meant for managing user sign-up and sign-in processes, not for guest access."",
    ""Create a new identity pool, enable access to authenticated identities, and grant access to AWS resources."": ""This choice is incorrect because the requirement is to allow guest users without logins, which means unauthenticated access. Enabling access to authenticated identities would require users to log in, which contradicts the requirement. Instead, the correct approach would be to enable unauthenticated identities in the identity pool to allow guest access."",
    ""Create a new user pool, enable access to authenticated identifies, and grant access to AWS resources."": ""This choice is incorrect because user pools are used for authentication purposes, not for providing unauthenticated access. Enabling access to authenticated identities in a user pool implies that users must log in, which does not meet the requirement of allowing guest users without logins."",
    ""Create a new user pool, disable authentication access, and grant access to AWS resources."": ""This choice is incorrect because user pools are inherently designed for managing user authentication. Disabling authentication access in a user pool does not provide a mechanism for granting access to AWS resources, especially for unauthenticated users. The correct approach involves using an identity pool with unauthenticated identities.""
}
```"
A Developer has written code for an application and wants to share it with other Developers on the team to receive feedback. The shared application code needs to be stored long-term with multiple versions and batch change tracking. Which AWS service should the Developer use?,"[""AWS CodeBuild."", ""Amazon S3."", ""AWS CodeCommit."", ""AWS Cloud9.""]","[""AWS CodeCommit.""]","```json
{
    ""AWS CodeBuild."": ""AWS CodeBuild is a fully managed build service in the cloud. It compiles source code, runs tests, and produces software packages that are ready to deploy. It is not used for storing application code or version control, which is why it is incorrect for this use case."",
    ""Amazon S3."": ""Amazon S3 is an object storage service that offers scalability, data availability, security, and performance. While it can store application code, it does not provide version control or batch change tracking in the way a source control system does, making it unsuitable for this scenario."",
    ""AWS CodeCommit."": ""AWS CodeCommit is a fully managed source control service that makes it easy for teams to host secure and scalable Git repositories. It supports long-term storage of code, multiple versions, and batch change tracking, which aligns perfectly with the Developer's needs in this scenario. This is why it is the correct choice."",
    ""AWS Cloud9."": ""AWS Cloud9 is a cloud-based integrated development environment (IDE) that lets you write, run, and debug your code with just a browser. It is not used for storing code long-term or for version control, which makes it incorrect for this use case.""
}
```"
"A Developer has discovered that an application responsible for processing messages in an Amazon SQS queue is routinely falling behind. The application is capable of processing multiple messages in one execution, but is only receiving one message at a time. What should the Developer do to increase the number of messages the application receives?","[""Call the `ChangeMessageVisibility` API for the queue and set `MaxNumberOfMessages` to a value greater than the default of `1`."", ""Call the `AddPermission` API to set `MaxNumberOfMessages` for the `ReceiveMessage` action to a value greater than the default of `1`."", ""Call the `ReceiveMessage` API to set `MaxNumberOfMessages` to a value greater than the default of `1`."", ""Call the `SetQueueAttributes` API for the queue and set `MaxNumberOfMessages` to a value greater than the default of `1`.""]","[""Call the `ReceiveMessage` API to set `MaxNumberOfMessages` to a value greater than the default of `1`.""]","```json
{
    ""Call the `ChangeMessageVisibility` API for the queue and set `MaxNumberOfMessages` to a value greater than the default of `1`."": ""This choice is incorrect because the `ChangeMessageVisibility` API is used to change the visibility timeout of a specific message in the queue, not to change the number of messages received. The `MaxNumberOfMessages` parameter is not applicable to this API. Visibility timeout controls how long a message is hidden from other consumers after being retrieved, which is unrelated to the number of messages being received."",
    ""Call the `AddPermission` API to set `MaxNumberOfMessages` for the `ReceiveMessage` action to a value greater than the default of `1`."": ""This choice is incorrect because the `AddPermission` API is used to add permissions to an SQS queue, allowing other AWS accounts or IAM users to perform actions on the queue. It does not modify the `MaxNumberOfMessages` parameter for the `ReceiveMessage` action. Permissions are about access control, not message retrieval settings."",
    ""Call the `ReceiveMessage` API to set `MaxNumberOfMessages` to a value greater than the default of `1`."": ""This choice is correct because the `ReceiveMessage` API is used to retrieve messages from an SQS queue. The `MaxNumberOfMessages` parameter specifies the maximum number of messages to return. By default, this is set to 1, but it can be increased up to 10. Increasing this value allows the application to receive multiple messages in a single call, which can help process messages more efficiently and reduce the backlog."",
    ""Call the `SetQueueAttributes` API for the queue and set `MaxNumberOfMessages` to a value greater than the default of `1`."": ""This choice is incorrect because the `SetQueueAttributes` API is used to change the attributes of an SQS queue, such as visibility timeout, message retention period, etc. However, `MaxNumberOfMessages` is not an attribute of the queue itself; it is a parameter of the `ReceiveMessage` API call. Therefore, this API cannot be used to change the number of messages received per call.""
}
```"
"A Developer registered an AWS Lambda function as a target for an Application Load Balancer (ALB) using a CLI command. However, the Lambda function is not being invoked when the client sends requests through the ALB. Why is the Lambda function not being invoked?","[""A Lambda function cannot be registered as a target for an ALB."", ""A Lambda function can be registered with an ALB using AWS Management Console only."", ""The permissions to invoke the Lambda function are missing."", ""Cross-zone is not enabled on the ALB.""]","[""The permissions to invoke the Lambda function are missing.""]","```json
{
    ""A Lambda function cannot be registered as a target for an ALB."": ""This choice is incorrect. AWS supports registering Lambda functions as targets for an Application Load Balancer. This feature allows ALB to route requests to Lambda functions, enabling serverless applications to be integrated with traditional load balancing methods."",
    ""A Lambda function can be registered with an ALB using AWS Management Console only."": ""This choice is incorrect. Lambda functions can be registered as targets for an ALB using both the AWS Management Console and the AWS CLI. The CLI provides a way to automate and script the registration process, which is useful for infrastructure as code practices."",
    ""The permissions to invoke the Lambda function are missing."": ""This choice is correct. For an ALB to invoke a Lambda function, the Lambda function's resource policy must grant the ALB permission to invoke it. This is a common requirement in AWS where permissions must be explicitly granted to allow services to interact with each other."",
    ""Cross-zone is not enabled on the ALB."": ""This choice is incorrect. Cross-zone load balancing is a feature that determines how requests are distributed across multiple availability zones, but it does not affect the ability of an ALB to invoke a Lambda function. The issue described is related to permissions, not load balancing configuration.""
}
```"
"A company provides APIs as a service and commits to a service level agreement (SLA) with all its users. To comply with each SLA, what should the company do?","[""Enable throttling limits for each method in Amazon API Gateway."", ""Create a usage plan for each user and request API keys to access the APIs."", ""Enable API rate limiting in Amazon Cognito for each user."", ""Enable default throttling limits for each stage after deploying the APIs.""]","[""Enable default throttling limits for each stage after deploying the APIs.""]","```json
{
    ""Enable throttling limits for each method in Amazon API Gateway."": ""This choice is incorrect because while enabling throttling limits for each method can help manage traffic and prevent abuse, it does not directly address the need to comply with an SLA across all users and stages. Throttling at the method level is more granular and might not provide the comprehensive control needed for SLA compliance across different stages."",
    ""Create a usage plan for each user and request API keys to access the APIs."": ""This choice is incorrect because while creating usage plans and requiring API keys can help manage and monitor access, it does not inherently ensure SLA compliance. Usage plans can limit request rates and burst limits per user, but they do not address the need for default throttling limits across all users and stages."",
    ""Enable API rate limiting in Amazon Cognito for each user."": ""This choice is incorrect because Amazon Cognito is primarily used for user authentication and authorization, not for API rate limiting. API rate limiting should be managed within the API Gateway itself, not through Cognito, which is not designed for this purpose."",
    ""Enable default throttling limits for each stage after deploying the APIs."": ""This choice is correct because enabling default throttling limits for each stage in Amazon API Gateway ensures that all users and methods are subject to consistent request rate limits, which is crucial for maintaining SLA compliance. By setting these limits at the stage level, the company can control the overall load and prevent any single user from overwhelming the system, thereby helping to meet SLA commitments. This approach aligns with AWS best practices for managing API traffic and ensuring service reliability.""
}
```"
A Developer is preparing a deployment package using AWS CloudFormation. The package consists of two separate templates: one for the infrastructure and one for the application. The application has to be inside the VPC that is created from the infrastructure template. How can the application stack refer to the VPC created from the infrastructure template?,"[""Use the Ref function to import the VPC into the application stack from the infrastructure template."", ""Use the export flag in the infrastructure template, and then use the `Fn::ImportValue` function in the application template."", ""Use the DependsOn attribute to specify that the application instance depends on the VPC in the application template."", ""Use the `Fn::GetAtt` function to include the attribute of the VPC in the application template.""]","[""Use the Ref function to import the VPC into the application stack from the infrastructure template.""]","```json
{
    ""Use the Ref function to import the VPC into the application stack from the infrastructure template."": ""This choice is incorrect. The Ref function is used to reference resources within the same stack, not across different stacks. To refer to resources in another stack, you need to use exports and imports, which is not possible with Ref alone."",
    ""Use the export flag in the infrastructure template, and then use the `Fn::ImportValue` function in the application template."": ""This choice is correct. In AWS CloudFormation, to share resources between stacks, you can export the resource from one stack using the Outputs section with the Export flag. The other stack can then use the `Fn::ImportValue` function to import the value. This is the recommended way to share resources like VPCs between CloudFormation stacks."",
    ""Use the DependsOn attribute to specify that the application instance depends on the VPC in the application template."": ""This choice is incorrect. The DependsOn attribute is used to specify resource creation order within the same stack, not across different stacks. It cannot be used to reference resources from another stack."",
    ""Use the `Fn::GetAtt` function to include the attribute of the VPC in the application template."": ""This choice is incorrect. The `Fn::GetAtt` function is used to retrieve attributes of resources within the same stack. It cannot be used to access resources or their attributes from a different stack.""
}
```"
"A Developer needs to create an application that supports Security Assertion Markup Language (SAML) and Facebook authentication. It must also allow access to AWS services, such as Amazon DynamoDB. Which AWS service or feature will meet these requirements with the LEAST amount of additional coding?","[""AWS AppSync."", ""Amazon Cognito identity pools."", ""Amazon Cognito user pools."", ""Amazon Lambda@Edge.""]","[""Amazon Cognito user pools.""]","```json
{
    ""AWS AppSync."": ""AWS AppSync is a service for building scalable GraphQL APIs, which can integrate with various data sources like DynamoDB. However, it does not natively support authentication mechanisms such as SAML or Facebook authentication. Therefore, it is not suitable for handling authentication with the least amount of additional coding."",
    ""Amazon Cognito identity pools."": ""Amazon Cognito identity pools provide temporary AWS credentials to access AWS services like DynamoDB. While they can integrate with external identity providers, they do not directly manage user authentication flows such as SAML or Facebook. They are more focused on providing AWS credentials after authentication has been handled, typically by user pools."",
    ""Amazon Cognito user pools."": ""Amazon Cognito user pools are the correct choice because they provide a user directory and are capable of handling user authentication, including support for SAML and social identity providers like Facebook. They can be configured to federate with these identity providers, allowing users to authenticate using SAML or Facebook with minimal additional coding. Once authenticated, user pools can be integrated with identity pools to provide access to AWS services like DynamoDB."",
    ""Amazon Lambda@Edge."": ""Lambda@Edge is a feature of Amazon CloudFront that allows you to run code closer to users of your application, improving performance and reducing latency. It is not designed for handling user authentication or integrating with identity providers like SAML or Facebook. Thus, it is not suitable for this use case.""
}
```"
"A Developer is trying to monitor an application's status by running a cron job that returns 1 if the service is up and 0 if the service is down. The Developer created code that uses an AWS CLI put-metric-alarm command to publish the custom metrics to Amazon CloudWatch and create an alarm. However, the Developer is unable to create an alarm as the custom metrics do not appear in the CloudWatch console. What is causing this issue?","[""Sending custom metrics using the CLI is not supported."", ""The Developer needs to use the put-metric-data command."", ""The Developer must use a unified CloudWatch agent to publish custom metrics."", ""The code is not running on an Amazon EC2 instance.""]","[""The Developer needs to use the put-metric-data command.""]","```json
{
    ""Sending custom metrics using the CLI is not supported."": ""This is incorrect. AWS CLI does support sending custom metrics to CloudWatch using the put-metric-data command. The issue here is not with the CLI's capability to send custom metrics but with using the wrong command."",
    ""The Developer needs to use the put-metric-data command."": ""This is correct. To publish custom metrics to Amazon CloudWatch, the Developer should use the put-metric-data command. This command allows you to send custom metrics to CloudWatch, which can then be used to create alarms. The put-metric-alarm command is used to create alarms based on existing metrics, not to publish new metrics."",
    ""The Developer must use a unified CloudWatch agent to publish custom metrics."": ""This is incorrect. While the CloudWatch agent can be used to collect and publish metrics, it is not necessary for publishing custom metrics via the AWS CLI. The put-metric-data command is sufficient for this purpose."",
    ""The code is not running on an Amazon EC2 instance."": ""This is incorrect. The ability to publish custom metrics to CloudWatch using the AWS CLI is not dependent on the code running on an EC2 instance. The CLI can be used from any environment where it is configured and has the necessary permissions.""
}
```"
A Developer has written an application that runs on Amazon EC2 instances and generates a value every minute. The Developer wants to monitor and graph the values generated over time without logging in to the instance each time. Which approach should the Developer use to achieve this goal?,"[""Use the Amazon CloudWatch metrics reported by default for all EC2 instances. View each value from the CloudWatch console."", ""Develop the application to store each value in a file on Amazon S3 every minute with the timestamp as the name."", ""Publish each generated value as a custom metric to Amazon CloudWatch using available AWS SDKs."", ""Store each value as a variable and add the variable to the list of EC2 metrics that should be reported to the Amazon CloudWatch console.""]","[""Publish each generated value as a custom metric to Amazon CloudWatch using available AWS SDKs.""]","```json
{
    ""Use the Amazon CloudWatch metrics reported by default for all EC2 instances. View each value from the CloudWatch console."": ""This choice is incorrect because the default CloudWatch metrics for EC2 instances do not include custom application-generated values. Default metrics are limited to instance-level metrics like CPU utilization, disk I/O, and network traffic. To monitor application-specific values, custom metrics must be published."",
    ""Develop the application to store each value in a file on Amazon S3 every minute with the timestamp as the name."": ""This choice is incorrect because while storing data in S3 is a valid approach for persistence, it does not provide real-time monitoring or graphing capabilities. S3 is a storage service and does not offer built-in tools for monitoring or visualizing data trends over time, which is the requirement here."",
    ""Publish each generated value as a custom metric to Amazon CloudWatch using available AWS SDKs."": ""This choice is correct because Amazon CloudWatch allows developers to publish custom metrics using AWS SDKs. This enables real-time monitoring and graphing of application-specific data. CloudWatch provides powerful visualization and alerting capabilities, making it the best choice for tracking and analyzing the values generated by the application."",
    ""Store each value as a variable and add the variable to the list of EC2 metrics that should be reported to the Amazon CloudWatch console."": ""This choice is incorrect because EC2 instance metrics are predefined and cannot be arbitrarily extended with application-specific variables. Custom metrics must be explicitly published to CloudWatch using the PutMetricData API or AWS SDKs, rather than being added to existing EC2 metrics.""
}
```"
"A Development team decides to adopt a continuous integration/continuous delivery (CI/CD) process using AWS CodePipeline and AWS CodeCommit for a new application. However, management wants a person to review and approve the code before it is deployed to production. How can the Development team add a manual approver to the CI/CD pipeline?","[""Use AWS SES to send an email to approvers when their action is required. Develop a simple application that allows approvers to accept or reject a build. Invoke an AWS Lambda function to advance the pipeline when a build is accepted."", ""If approved, add an approved tag when pushing changes to the CodeCommit repository. CodePipeline will proceed to build and deploy approved commits without interruption."", ""Add an approval step to CodeCommit. Commits will not be saved until approved."", ""Add an approval action to the pipeline. Configure the approval action to publish to an Amazon SNS topic when approval is required. The pipeline execution will stop and wait for an approval.""]","[""Add an approval action to the pipeline. Configure the approval action to publish to an Amazon SNS topic when approval is required. The pipeline execution will stop and wait for an approval.""]","```json
{
    ""Use AWS SES to send an email to approvers when their action is required. Develop a simple application that allows approvers to accept or reject a build. Invoke an AWS Lambda function to advance the pipeline when a build is accepted."": ""This choice is incorrect because it suggests building a custom solution for approval, which is unnecessary and more complex than required. AWS CodePipeline natively supports manual approval actions, which are specifically designed for this purpose. Using AWS SES and a custom application introduces additional complexity and potential points of failure, whereas CodePipeline's approval actions are integrated and managed within the service itself."",
    ""If approved, add an approved tag when pushing changes to the CodeCommit repository. CodePipeline will proceed to build and deploy approved commits without interruption."": ""This choice is incorrect because CodePipeline does not have a built-in mechanism to recognize tags in CodeCommit as approval signals. CodePipeline uses stages and actions, including manual approval actions, to manage the flow of changes through the pipeline. Tags in a repository are not a substitute for a formal approval process within the pipeline."",
    ""Add an approval step to CodeCommit. Commits will not be saved until approved."": ""This choice is incorrect because CodeCommit does not support approval steps for commits. CodeCommit is a source control service that allows for version control of code, but it does not have a feature to hold commits pending approval. Approval processes are managed in CodePipeline, where you can add manual approval actions to control the flow of changes."",
    ""Add an approval action to the pipeline. Configure the approval action to publish to an Amazon SNS topic when approval is required. The pipeline execution will stop and wait for an approval."": ""This choice is correct because AWS CodePipeline provides a built-in feature to add manual approval actions. These actions can be configured to send notifications via Amazon SNS when approval is required, allowing for human intervention before proceeding to the next stage. This is a best practice for implementing manual approvals in a CI/CD pipeline, as it leverages AWS's managed services to ensure a reliable and integrated approval process. This approach is supported by AWS documentation and is commonly used in real-world scenarios to ensure code quality and compliance before deployment.""
}
```"
A Developer is building a serverless application using AWS Lambda and must create a REST API using an HTTP GET method. What needs to be defined to meet this requirement? (Choose TWO),"[""A Lambda@Edge function."", ""An Amazon API Gateway with a Lambda function."", ""An exposed GET method in an Amazon API Gateway."", ""An exposed GET method in the Lambda function."", ""An exposed GET method in Amazon Route 53.""]","[""An Amazon API Gateway with a Lambda function."", ""An exposed GET method in an Amazon API Gateway.""]","```json
{
    ""A Lambda@Edge function."": ""A Lambda@Edge function is incorrect because Lambda@Edge is used for running code closer to users of a CloudFront distribution, not for creating REST APIs. It is primarily used for content delivery and caching, not for handling HTTP methods in a REST API."",
    ""An Amazon API Gateway with a Lambda function."": ""An Amazon API Gateway with a Lambda function is correct because API Gateway is a fully managed service that makes it easy to create, publish, maintain, monitor, and secure APIs. It acts as a 'front door' for applications to access data, business logic, or functionality from your backend services, such as AWS Lambda functions. This is a common pattern for serverless applications where API Gateway routes HTTP requests to Lambda functions."",
    ""An exposed GET method in an Amazon API Gateway."": ""An exposed GET method in an Amazon API Gateway is correct because in order to create a REST API, you need to define HTTP methods (such as GET) that the API will respond to. API Gateway allows you to expose these methods and map them to backend integrations like Lambda functions, enabling the serverless application to handle HTTP requests."",
    ""An exposed GET method in the Lambda function."": ""An exposed GET method in the Lambda function is incorrect because Lambda functions do not directly expose HTTP methods. Instead, they are invoked by triggers such as API Gateway, which handles the HTTP method exposure. Lambda functions execute code in response to these triggers."",
    ""An exposed GET method in Amazon Route 53."": ""An exposed GET method in Amazon Route 53 is incorrect because Route 53 is a scalable and highly available Domain Name System (DNS) web service. It is used for domain registration and routing traffic to AWS resources, not for exposing HTTP methods or creating REST APIs.""
}
```"
"A Developer is writing an application in AWS Lambda. To simplify testing and deployments, the Developer needs the database connection string to be easily changed without modifying the Lambda code. How can this requirement be met?","[""Store the connection string as a secret in AWS Secrets Manager."", ""Store the connection string in an IAM user account."", ""Store the connection string in AWS KMS."", ""Store the connection string as a Lambda layer.""]","[""Store the connection string as a secret in AWS Secrets Manager.""]","```json
{
    ""Store the connection string as a secret in AWS Secrets Manager."": ""This choice is correct because AWS Secrets Manager is designed to store, manage, and retrieve secrets such as database connection strings. It allows you to change the connection string without modifying the Lambda code, as the application can retrieve the latest secret value at runtime. This aligns with best practices for managing sensitive information and supports automatic rotation of secrets, enhancing security."",
    ""Store the connection string in an IAM user account."": ""This choice is incorrect because IAM user accounts are used for managing access permissions and credentials for AWS services, not for storing application-specific data like database connection strings. Using IAM for this purpose would be a misuse of the service and does not provide the flexibility or security features needed for managing secrets."",
    ""Store the connection string in AWS KMS."": ""This choice is incorrect because AWS Key Management Service (KMS) is used for managing cryptographic keys, not for storing secrets like connection strings. While KMS can encrypt and decrypt data, it is not intended to store the actual data itself. Secrets Manager or AWS Systems Manager Parameter Store would be more appropriate for storing secrets."",
    ""Store the connection string as a Lambda layer."": ""This choice is incorrect because Lambda layers are used to package libraries and dependencies that can be shared across multiple Lambda functions. They are not intended for storing configuration data or secrets. Using a Lambda layer for a connection string would require code changes to update the layer, which does not meet the requirement of changing the connection string without modifying the code.""
}
```"
"A company is launching an ecommerce website and will host the static data in Amazon S3. The company expects approximately 1,000 transactions per second (TPS) for GET and PUT requests in total. Logging must be enabled to track all requests and must be retained for auditing purposes. What is the MOST cost-effective solution?","[""Enable AWS CloudTrail logging for the S3 bucket-level action and create a lifecycle policy to move the data from the log bucket to Amazon S3 Glacier in 90 days."", ""Enable S3 server access logging and create a lifecycle policy to expire the data in 90 days."", ""Enable AWS CloudTrail logging for the S3 bucket-level action and create a lifecycle policy to expire the data in 90 days."", ""Enable S3 server access logging and create a lifecycle policy to move the data to Amazon S3 Glacier in 90 days.""]","[""Enable S3 server access logging and create a lifecycle policy to move the data to Amazon S3 Glacier in 90 days.""]","```json
{
    ""Enable AWS CloudTrail logging for the S3 bucket-level action and create a lifecycle policy to move the data from the log bucket to Amazon S3 Glacier in 90 days."": ""This option is incorrect because AWS CloudTrail is primarily used for logging API calls made on your AWS account, which includes S3 bucket-level actions, but it is more expensive compared to S3 server access logging for high-volume operations like 1,000 TPS. Also, CloudTrail logs are more detailed and might not be necessary for simple access logging, making it less cost-effective."",
    ""Enable S3 server access logging and create a lifecycle policy to expire the data in 90 days."": ""This option is incorrect because expiring the data in 90 days means deleting it, which does not meet the requirement of retaining logs for auditing purposes. Although S3 server access logging is cost-effective, expiring logs would not allow for future audits."",
    ""Enable AWS CloudTrail logging for the S3 bucket-level action and create a lifecycle policy to expire the data in 90 days."": ""This option is incorrect because, similar to the first option, using AWS CloudTrail for logging S3 access is more expensive than S3 server access logging for high-volume operations. Additionally, expiring the data in 90 days would mean deleting it, which does not align with the requirement to retain logs for auditing."",
    ""Enable S3 server access logging and create a lifecycle policy to move the data to Amazon S3 Glacier in 90 days."": ""This option is correct because S3 server access logging is a cost-effective way to log requests made to your S3 bucket. By creating a lifecycle policy to move the logs to Amazon S3 Glacier after 90 days, the company can retain the logs for auditing purposes at a lower storage cost, which is both cost-effective and compliant with the requirement to retain logs.""
}
```"
"A Developer decides to store highly secure data in Amazon S3 and wants to implement server-side encryption (SSE) with granular control of who can access the master key. Company policy requires that the master key be created, rotated, and disabled easily when needed, all for security reasons. Which solution should be used to meet these requirements?","[""SSE with Amazon S3 managed keys (SSE-S3)."", ""SSE with AWS KMS managed keys (SSE-KMS)."", ""SSE with AWS Secrets Manager."", ""SSE with customer-provided encryption keys.""]","[""SSE with AWS Secrets Manager.""]","```json
{
    ""SSE with Amazon S3 managed keys (SSE-S3)."": ""SSE-S3 is incorrect because it uses keys managed by Amazon S3, which does not provide granular control over the master key. It does not allow for easy creation, rotation, or disabling of keys as required by the company policy. SSE-S3 is suitable for general encryption needs but lacks the advanced key management features provided by AWS KMS."",
    ""SSE with AWS KMS managed keys (SSE-KMS)."": ""SSE-KMS is the correct choice because it integrates with AWS Key Management Service (KMS), which provides granular control over the encryption keys. KMS allows for the creation, rotation, and disabling of keys, meeting the company's security policy requirements. It also provides detailed access control and auditing capabilities, making it suitable for storing highly secure data."",
    ""SSE with AWS Secrets Manager."": ""SSE with AWS Secrets Manager is incorrect because AWS Secrets Manager is not used for server-side encryption of data in S3. Instead, it is designed for managing secrets like database credentials and API keys. It does not provide encryption for S3 objects, nor does it offer the key management features needed for this scenario."",
    ""SSE with customer-provided encryption keys."": ""SSE with customer-provided encryption keys (SSE-C) is incorrect because, while it allows customers to manage their own keys, it does not integrate with AWS KMS for key management. This means the customer is responsible for key rotation and disabling, which can be more complex and does not meet the requirement for easy management as specified in the question.""
}
```"
A Developer is migrating an on-premises application to AWS. The application currently takes user uploads and saves them to a local directory on the server. All uploads must be saved and made immediately available to all instances in an Auto Scaling group. Which approach will meet these requirements?,"[""Use Amazon EBS and configure the application AMI to use a snapshot of the same EBS instance on boot."", ""Use Amazon S3 and rearchitect the application so all uploads are placed in S3."", ""Use instance storage and share it between instances launched from the same Amazon Machine Image (AMI)."", ""Use Amazon EBS and file synchronization software to achieve eventual consistency among the Auto Scaling group.""]","[""Use Amazon S3 and rearchitect the application so all uploads are placed in S3.""]","```json
{
    ""Use Amazon EBS and configure the application AMI to use a snapshot of the same EBS instance on boot."": ""This option is incorrect because Amazon EBS volumes are attached to a single instance and cannot be shared across multiple instances. While you can create snapshots of EBS volumes, these snapshots are static and do not provide real-time data sharing across instances. This does not meet the requirement of making uploads immediately available to all instances in an Auto Scaling group."",
    ""Use Amazon S3 and rearchitect the application so all uploads are placed in S3."": ""This is the correct option because Amazon S3 is a highly durable and scalable object storage service that allows multiple instances to access the same data simultaneously. By rearchitecting the application to store uploads in S3, you ensure that all instances in the Auto Scaling group can immediately access the uploaded files. This approach aligns with AWS best practices for decoupling storage from compute resources, providing high availability and durability."",
    ""Use instance storage and share it between instances launched from the same Amazon Machine Image (AMI)."": ""This option is incorrect because instance storage is ephemeral and only available to the instance it is attached to. It cannot be shared between instances, and data stored in instance storage is lost when the instance is stopped or terminated. This does not meet the requirement of making uploads available to all instances in an Auto Scaling group."",
    ""Use Amazon EBS and file synchronization software to achieve eventual consistency among the Auto Scaling group."": ""This option is incorrect because while file synchronization software could theoretically be used to replicate data across instances, it introduces complexity and potential latency, which does not meet the requirement for immediate availability. Additionally, this approach is not as reliable or scalable as using Amazon S3, which is designed for such use cases.""
}
```"
A Developer implemented a static website hosted in Amazon S3 that makes web service requests hosted in Amazon API Gateway and AWS Lambda. The site is showing an error that reads: `No Access-Control-Allow-Origin` header is present on the requested resource. Origin `null` is therefore not allowed access.' What should the Developer do to resolve this issue?,"[""Enable cross-origin resource sharing (CORS) on the S3 bucket."", ""Enable cross-origin resource sharing (CORS) for the method in API Gateway."", ""Add the Access-Control-Request-Method header to the request."", ""Add the Access-Control-Request-Headers header to the request.""]","[""Enable cross-origin resource sharing (CORS) for the method in API Gateway.""]","```json
{
    ""Enable cross-origin resource sharing (CORS) on the S3 bucket."": ""This choice is incorrect because the error message relates to the API Gateway, not the S3 bucket. CORS on the S3 bucket would be relevant if the static website itself was being accessed cross-origin, but the issue here is with the API Gateway endpoint being accessed from the website hosted on S3."",
    ""Enable cross-origin resource sharing (CORS) for the method in API Gateway."": ""This choice is correct because the error message indicates a CORS issue with the API Gateway. CORS must be enabled on the API Gateway method to allow the S3-hosted website to make requests to it. This involves configuring the API Gateway to include the Access-Control-Allow-Origin header in its responses, which is a common requirement when a web application hosted on one domain needs to interact with resources on another domain."",
    ""Add the Access-Control-Request-Method header to the request."": ""This choice is incorrect because adding the Access-Control-Request-Method header to the request does not resolve the CORS issue. The problem is with the server not responding with the appropriate CORS headers, not with the request itself. The server (API Gateway) needs to be configured to handle CORS requests properly."",
    ""Add the Access-Control-Request-Headers header to the request."": ""This choice is incorrect because, similar to the previous option, adding the Access-Control-Request-Headers header to the request does not address the root cause of the CORS error. The issue is with the server-side configuration (API Gateway), which needs to include the Access-Control-Allow-Origin header in its responses to allow cross-origin requests.""
}
```"
A Developer is building an application that needs to store data in Amazon S3. Management requires that the data be encrypted before it is sent to Amazon S3 for storage. The encryption keys need to be managed by the Security team. Which approach should the Developer take to meet these requirements?,"[""Implement server-side encryption using customer-provided encryption keys (SSE-C)."", ""Implement server-side encryption by using a client-side master key."", ""Implement client-side encryption using an AWS KMS managed customer master key (CMK)."", ""Implement client-side encryption using Amazon S3 managed keys.""]","[""Implement client-side encryption using an AWS KMS managed customer master key (CMK).""]","```json
{
    ""Implement server-side encryption using customer-provided encryption keys (SSE-C)."": ""This option is incorrect because while SSE-C allows the use of customer-provided keys, the keys are not managed by AWS KMS or the Security team. The keys must be provided with each request, and AWS does not store them. This does not meet the requirement for the Security team to manage the encryption keys."",
    ""Implement server-side encryption by using a client-side master key."": ""This option is incorrect because it confuses server-side encryption with client-side encryption. Server-side encryption does not involve a client-side master key. Instead, server-side encryption options like SSE-S3, SSE-KMS, or SSE-C are used. This does not meet the requirement for the Security team to manage the encryption keys."",
    ""Implement client-side encryption using an AWS KMS managed customer master key (CMK)."": ""This option is correct because client-side encryption with AWS KMS allows the developer to encrypt data before sending it to S3, using a CMK managed by AWS KMS. The Security team can manage the CMK, which aligns with the requirement for the Security team to manage the encryption keys. This approach ensures that data is encrypted before it is sent to S3, meeting the management's encryption requirement."",
    ""Implement client-side encryption using Amazon S3 managed keys."": ""This option is incorrect because Amazon S3 managed keys (SSE-S3) are used for server-side encryption, not client-side encryption. In this scenario, the encryption keys are managed by AWS, not the Security team, which does not meet the requirement for the Security team to manage the encryption keys.""
}
```"
"A Developer has written an Amazon Kinesis Data Streams application. As usage grows and traffic increases over time, the application is regularly receiving `ProvisionedThroughputExceededException` error messages. Which steps should the Developer take to resolve the error? (Choose TWO)","[""Use Auto Scaling to scale the stream for better performance."", ""Increase the delay between the GetRecords call and the PutRecords call."", ""Increase the number of shards in the data stream."", ""Specify a shard iterator using the ShardIterator parameter."", ""Implement exponential backoff on the GetRecords call and the PutRecords call.""]","[""Increase the number of shards in the data stream."", ""Implement exponential backoff on the GetRecords call and the PutRecords call.""]","```json
{
    ""Use Auto Scaling to scale the stream for better performance."": ""Incorrect. Auto Scaling is not directly applicable to Kinesis Data Streams. Kinesis Data Streams do not support automatic scaling like some other AWS services. Instead, you must manually increase the number of shards to handle increased throughput."",
    ""Increase the delay between the GetRecords call and the PutRecords call."": ""Incorrect. Increasing the delay between GetRecords and PutRecords does not address the root cause of the ProvisionedThroughputExceededException. This error occurs when the request rate for the stream exceeds the provisioned throughput, which is better addressed by increasing shard count or implementing backoff strategies."",
    ""Increase the number of shards in the data stream."": ""Correct. Increasing the number of shards in a Kinesis Data Stream is a direct method to increase the stream's capacity to handle more read and write operations. Each shard provides a fixed amount of read and write capacity, so adding more shards allows the stream to handle higher throughput."",
    ""Specify a shard iterator using the ShardIterator parameter."": ""Incorrect. Specifying a shard iterator is necessary for reading data from a shard but does not resolve throughput exceptions. The error is related to exceeding the provisioned throughput, not the method of accessing the data."",
    ""Implement exponential backoff on the GetRecords call and the PutRecords call."": ""Correct. Implementing exponential backoff is a best practice for handling transient errors like ProvisionedThroughputExceededException. This approach helps to reduce the request rate temporarily, allowing the system to recover and eventually succeed without overwhelming the stream.""
}
```"
"A Developer is publishing critical log data to a log group in Amazon CloudWatch Logs, which was created 2 months ago. The Developer must encrypt the log data using an AWS KMS customer master key (CMK) so future data can be encrypted to comply with the company's security policy. How can the Developer meet this requirement?","[""Use the CloudWatch Logs console and enable the encrypt feature on the log group."", ""Use the AWS CLI `create-log-group` command and specify the key Amazon Resource Name (ARN)."", ""Use the KMS console and associate the CMK with the log group."", ""Use the AWS CLI `associate-kms-key` command and specify the key Amazon Resource Name (ARN)""]","[""Use the AWS CLI `associate-kms-key` command and specify the key Amazon Resource Name (ARN)""]","```json
{
    ""Use the CloudWatch Logs console and enable the encrypt feature on the log group."": ""Incorrect. As of the current AWS capabilities, there is no direct option in the CloudWatch Logs console to enable encryption on an existing log group. Encryption must be configured using the AWS CLI or SDKs by associating a KMS key with the log group."",
    ""Use the AWS CLI `create-log-group` command and specify the key Amazon Resource Name (ARN)."": ""Incorrect. The `create-log-group` command is used to create a new log group, not to modify an existing one. Since the log group was created 2 months ago, this command cannot be used to apply encryption to it. Instead, the `associate-kms-key` command should be used to associate a KMS key with an existing log group."",
    ""Use the KMS console and associate the CMK with the log group."": ""Incorrect. The KMS console is used to manage keys but does not provide functionality to directly associate a CMK with a CloudWatch Logs log group. This association must be done through the AWS CLI or SDKs."",
    ""Use the AWS CLI `associate-kms-key` command and specify the key Amazon Resource Name (ARN)"": ""Correct. The `associate-kms-key` command is specifically designed to associate a KMS key with an existing CloudWatch Logs log group, enabling encryption for future log data. This is the recommended approach for encrypting log data in an existing log group according to AWS best practices.""
}
```"
A Developer has code running on Amazon EC2 instances that needs read-only access to an Amazon DynamoDB table. What is the MOST secure approach the Developer should take to accomplish this task?,"[""Create a user access key for each EC2 instance with read-only access to DynamoDB. Place the keys in the code. Redeploy the code as keys rotate."", ""Use an IAM role with an AmazonDynamoDBReadOnlyAccess policy applied to the EC2 instances."", ""Run all code with only AWS account root user access keys to ensure maximum access to services."", ""Use an IAM role with Administrator access applied to the EC2 instance.""]","[""Use an IAM role with an AmazonDynamoDBReadOnlyAccess policy applied to the EC2 instances.""]","```json
{
    ""Create a user access key for each EC2 instance with read-only access to DynamoDB. Place the keys in the code. Redeploy the code as keys rotate."": ""This approach is incorrect because embedding access keys in code is a security risk. It exposes the keys to potential leaks and requires manual rotation and redeployment of code, which is not efficient or secure. AWS best practices recommend using IAM roles for applications running on EC2 to access AWS services securely without the need for access keys."",
    ""Use an IAM role with an AmazonDynamoDBReadOnlyAccess policy applied to the EC2 instances."": ""This is the correct approach as it aligns with AWS best practices. By attaching an IAM role with the AmazonDynamoDBReadOnlyAccess policy to the EC2 instances, the application can securely access DynamoDB without embedding credentials in the code. The IAM role provides temporary security credentials that are automatically rotated, enhancing security and simplifying management."",
    ""Run all code with only AWS account root user access keys to ensure maximum access to services."": ""This is incorrect and highly discouraged. Using the root user for everyday tasks is a significant security risk. The root user has unrestricted access to all resources, and its credentials should be used sparingly and only for tasks that require root access. AWS best practices recommend creating IAM roles and users with the least privilege necessary for specific tasks."",
    ""Use an IAM role with Administrator access applied to the EC2 instance."": ""This approach is incorrect because it violates the principle of least privilege. Granting Administrator access provides more permissions than necessary, increasing the risk of accidental or malicious actions. It's best to grant only the permissions needed for the task, such as the AmazonDynamoDBReadOnlyAccess policy for read-only access to DynamoDB.""
}
```"
"A Developer migrated a web application to AWS. As part of the migration, the Developer implemented an automated continuous integration/continuous improvement (CI/CD) process using a blue/green deployment. The deployment provisions new Amazon EC2 instances in an Auto Scaling group behind a new Application Load Balancer. After the migration was completed, the Developer began receiving complaints from users getting booted out of the system. The system also requires users to log in after every new deployment. How can these issues be resolved?","[""Use rolling updates instead of a blue/green deployment."", ""Externalize the user sessions to Amazon ElastiCache."", ""Turn on sticky sessions in the Application Load Balancer."", ""Use multicast to replicate session information.""]","[""Externalize the user sessions to Amazon ElastiCache.""]","```json
{
    ""Use rolling updates instead of a blue/green deployment."": ""This choice is incorrect because the issue is not with the deployment strategy itself but with how user sessions are managed. Rolling updates would still result in users being logged out if sessions are stored on the instances, as new instances would not have access to the session data from the old instances."",
    ""Externalize the user sessions to Amazon ElastiCache."": ""This choice is correct because externalizing user sessions to a centralized cache like Amazon ElastiCache allows session data to be stored independently of the EC2 instances. This means that even when new instances are provisioned during a blue/green deployment, they can access the same session data, preventing users from being logged out. This is a best practice for managing session state in a scalable and distributed environment."",
    ""Turn on sticky sessions in the Application Load Balancer."": ""This choice is incorrect because while sticky sessions can help maintain session state by directing users to the same instance, they do not solve the problem of users being logged out during deployments. Sticky sessions are not ideal for blue/green deployments as they can lead to uneven load distribution and do not address the root cause of session data loss during instance replacement."",
    ""Use multicast to replicate session information."": ""This choice is incorrect because multicast is not a supported or practical method for replicating session information across instances in AWS. AWS does not natively support multicast, and using it would not be a scalable or reliable solution for session management in a cloud environment.""
}
```"
A Developer wants to insert a record into an Amazon DynamoDB table as soon as a new file is added to an Amazon S3 bucket. Which set of steps would be necessary to achieve this?,"[""Create an event with Amazon CloudWatch Events that will monitor the S3 bucket and then insert the records into DynamoDB."", ""Configure an S3 event to invoke a Lambda function that inserts records into DynamoDB."", ""Create a Lambda function that will poll the S3 bucket and then insert the records into DynamoDB."", ""Create a cron job that will run at a scheduled time and insert the records into DynamoDB.""]","[""Configure an S3 event to invoke a Lambda function that inserts records into DynamoDB.""]","```json
{
    ""Create an event with Amazon CloudWatch Events that will monitor the S3 bucket and then insert the records into DynamoDB."": ""This choice is incorrect because Amazon CloudWatch Events (now known as Amazon EventBridge) is not used to directly monitor S3 bucket events. Instead, S3 has its own event notification system that can trigger actions such as invoking a Lambda function. CloudWatch Events/EventBridge is typically used for scheduling tasks or responding to changes in AWS resources, but it is not the direct solution for S3 event notifications."",
    ""Configure an S3 event to invoke a Lambda function that inserts records into DynamoDB."": ""This choice is correct because Amazon S3 can be configured to trigger an event notification when a new file is added to a bucket. This event can directly invoke an AWS Lambda function, which can then execute code to insert a record into a DynamoDB table. This is a common pattern for event-driven architectures in AWS, leveraging S3 event notifications and Lambda's serverless compute capabilities. This approach is efficient and aligns with AWS best practices for decoupled and scalable architectures."",
    ""Create a Lambda function that will poll the S3 bucket and then insert the records into DynamoDB."": ""This choice is incorrect because Lambda functions are not designed to poll S3 buckets. Polling would require continuous checking, which is inefficient and not cost-effective. Instead, S3 event notifications should be used to automatically trigger a Lambda function when an event occurs, such as the addition of a new file. This event-driven approach is more efficient and aligns with AWS best practices."",
    ""Create a cron job that will run at a scheduled time and insert the records into DynamoDB."": ""This choice is incorrect because a cron job implies a scheduled task that runs at fixed intervals, which is not suitable for real-time processing of S3 events. The requirement is to insert a record as soon as a new file is added, which is best achieved through S3 event notifications triggering a Lambda function, rather than waiting for a scheduled cron job.""
}
```"
A company has implemented AWS CodeDeploy as part of its cloud native CI/CD stack. The company enables automatic rollbacks while deploying a new version of a popular web application from in-place to Amazon EC2. What occurs if the deployment of the new version fails due to code regression?,"[""The last known good deployment is automatically restored using the snapshot stored in Amazon S3."", ""CodeDeploy switches the Amazon Route 53 alias records back to the known good green deployment and terminates the failed blue deployment."", ""A new deployment of the last known version of the application is deployed with a new deployment ID."", ""AWS CodePipeline promotes the most recent deployment with a SUCCEEDED status to production.""]","[""CodeDeploy switches the Amazon Route 53 alias records back to the known good green deployment and terminates the failed blue deployment.""]","```json
{
    ""The last known good deployment is automatically restored using the snapshot stored in Amazon S3."": ""This choice is incorrect because AWS CodeDeploy does not use Amazon S3 snapshots for rollbacks. Rollbacks in CodeDeploy involve redeploying the last known good application revision, not restoring from a snapshot. Snapshots are typically associated with Amazon EBS volumes, not application deployments."",
    ""CodeDeploy switches the Amazon Route 53 alias records back to the known good green deployment and terminates the failed blue deployment."": ""This choice is incorrect because it describes a blue/green deployment scenario, not an in-place deployment. In an in-place deployment, CodeDeploy does not switch Route 53 records; it simply redeploys the last known good version on the same instances."",
    ""A new deployment of the last known version of the application is deployed with a new deployment ID."": ""This choice is correct because when an in-place deployment fails and automatic rollback is enabled, CodeDeploy redeploys the last known good version of the application with a new deployment ID. This ensures that the application reverts to a stable state."",
    ""AWS CodePipeline promotes the most recent deployment with a SUCCEEDED status to production."": ""This choice is incorrect because AWS CodePipeline does not automatically promote deployments based on their status. CodePipeline orchestrates the CI/CD process, but the rollback behavior described is specific to CodeDeploy, which handles rollbacks independently of CodePipeline.""
}
```"
"A Developer uses Amazon S3 buckets for static website hosting. The Developer creates one S3 bucket for the code and another S3 bucket for the assets, such as image and video files. Access is denied when a user attempts to access the assets bucket from the code bucket, with the website application showing a `403` error. How should the Developer solve this issue?","[""Create an IAM role and apply it to the assets bucket for the code bucket to be granted access."", ""Edit the bucket policy of the assets bucket to allow access from the code bucket."", ""Edit the bucket policy of the assets bucket to open access to all principals."", ""Change the code bucket to use AWS Lambda functions instead of static website hosting.""]","[""Edit the bucket policy of the assets bucket to allow access from the code bucket.""]","```json
{
    ""Create an IAM role and apply it to the assets bucket for the code bucket to be granted access."": ""This choice is incorrect because IAM roles are used to grant permissions to AWS services or users, not to other S3 buckets. The issue here is about cross-bucket access, which is best managed through bucket policies rather than IAM roles. IAM roles would be more appropriate if the access involved AWS services like EC2 or Lambda accessing the S3 bucket."",
    ""Edit the bucket policy of the assets bucket to allow access from the code bucket."": ""This choice is correct because S3 bucket policies are the appropriate way to manage access between S3 buckets. By editing the bucket policy of the assets bucket, the developer can specify conditions that allow requests from the code bucket, thus resolving the 403 error. This approach aligns with AWS best practices for managing access control between resources."",
    ""Edit the bucket policy of the assets bucket to open access to all principals."": ""This choice is incorrect because opening access to all principals would make the assets bucket publicly accessible, which is not a best practice due to security concerns. It would expose the assets to unauthorized access, which is not the intended solution for allowing access specifically from the code bucket."",
    ""Change the code bucket to use AWS Lambda functions instead of static website hosting."": ""This choice is incorrect because the issue is related to S3 bucket access policies, not the hosting method. Changing the hosting method to AWS Lambda does not address the cross-bucket access issue and is irrelevant to solving the 403 error. The problem is about permissions, not the execution environment.""
}
```"
A company has implemented AWS CodePipeline to automate its release pipelines. The Development team is writing an AWS Lambda function what will send notifications for state changes of each of the actions in the stages. Which steps must be taken to associate the Lambda function with the event source?,"[""Create a trigger that invokes the Lambda function from the Lambda console by selecting CodePipeline as the event source."", ""Create an event trigger and specify the Lambda function from the CodePipeline console."", ""Create an Amazon CloudWatch alarm that monitors status changes in Code Pipeline and triggers the Lambda function."", ""Create an Amazon CloudWatch Events rule that uses CodePipeline as an event source.""]","[""Create an event trigger and specify the Lambda function from the CodePipeline console.""]","```json
{
    ""Create a trigger that invokes the Lambda function from the Lambda console by selecting CodePipeline as the event source."": ""Incorrect. The Lambda console does not allow you to directly select CodePipeline as an event source. Instead, you would use Amazon EventBridge (formerly CloudWatch Events) to create a rule that triggers the Lambda function based on CodePipeline events."",
    ""Create an event trigger and specify the Lambda function from the CodePipeline console."": ""Incorrect. The CodePipeline console does not provide a direct option to create event triggers for Lambda functions. Instead, you should use Amazon EventBridge to create a rule that listens for CodePipeline state changes and triggers the Lambda function."",
    ""Create an Amazon CloudWatch alarm that monitors status changes in Code Pipeline and triggers the Lambda function."": ""Incorrect. CloudWatch Alarms are used for monitoring metrics and triggering actions based on thresholds. CodePipeline state changes are not metrics that can be monitored by CloudWatch Alarms. Instead, Amazon EventBridge should be used to handle event-driven actions based on state changes."",
    ""Create an Amazon CloudWatch Events rule that uses CodePipeline as an event source."": ""Correct. Amazon EventBridge (formerly CloudWatch Events) is the service used to create rules that react to changes in AWS resources, such as state changes in CodePipeline. By creating a rule with CodePipeline as the event source, you can trigger a Lambda function to send notifications when specific pipeline events occur. This is the recommended approach for event-driven architectures in AWS.""
}
```"
A Developer has built an application running on AWS Lambda using AWS Serverless Application Model (AWS SAM). What is the correct order of execution to successfully deploy the application?,"[""1. Build the SAM template in Amazon EC2. 2. Package the SAM template to Amazon EBS storage. 3. Deploy the SAM template from Amazon EBS."", ""1. Build the SAM template locally. 2. Package the SAM template onto Amazon S3. 3. Deploy the SAM template from Amazon S3."", ""1. Build the SAM template locally. 2. Deploy the SAM template from Amazon S3. 3. Package the SAM template for use."", ""1. Build the SAM template locally. 2. Package the SAM template from AWS CodeCommit. 3. Deploy the SAM template to CodeCommit.""]","[""1. Build the SAM template locally. 2. Package the SAM template onto Amazon S3. 3. Deploy the SAM template from Amazon S3.""]","```json
{
    ""1. Build the SAM template in Amazon EC2. 2. Package the SAM template to Amazon EBS storage. 3. Deploy the SAM template from Amazon EBS."": ""This choice is incorrect because AWS SAM is typically used with Amazon S3 for packaging and deployment, not Amazon EC2 or Amazon EBS. SAM templates are built locally and then packaged to Amazon S3, which is a scalable storage service that integrates well with AWS CloudFormation for deployment. Amazon EBS is primarily used for block storage with EC2 instances, not for storing and deploying SAM templates."",
    ""1. Build the SAM template locally. 2. Package the SAM template onto Amazon S3. 3. Deploy the SAM template from Amazon S3."": ""This choice is correct. The AWS Serverless Application Model (SAM) is designed to simplify building and deploying serverless applications. The correct process involves building the SAM template locally, packaging it to Amazon S3, and then deploying it from S3 using AWS CloudFormation. This workflow leverages S3's integration with CloudFormation, allowing for efficient and scalable deployment of serverless applications. This is a best practice as outlined in AWS documentation."",
    ""1. Build the SAM template locally. 2. Deploy the SAM template from Amazon S3. 3. Package the SAM template for use."": ""This choice is incorrect because it suggests deploying the SAM template before packaging it, which is not possible. The correct order is to first package the SAM template onto Amazon S3, and then deploy it from there. Packaging is a necessary step to prepare the template and its associated resources for deployment."",
    ""1. Build the SAM template locally. 2. Package the SAM template from AWS CodeCommit. 3. Deploy the SAM template to CodeCommit."": ""This choice is incorrect because AWS CodeCommit is a source control service and is not used for packaging or deploying SAM templates. The correct process involves packaging the SAM template onto Amazon S3, not AWS CodeCommit, and deploying it from S3 using AWS CloudFormation. CodeCommit is used for version control of code, not for deployment.""
}
```"
A company wants to migrate an imaging service to Amazon EC2 while following security best practices. The images are sourced and read from a non-public Amazon S3 bucket. What should a Developer do to meet these requirements?,"[""Create an IAM user with read-only permissions for the S3 bucket. Temporarily store the user credentials in the Amazon EBS volume of the EC2 instance."", ""Create an IAM user with read-only permissions for the S3 bucket. Temporarily store the user credentials in the user data of the EC2 instance."", ""Create an EC2 service role with read-only permissions for the S3 bucket. Attach the role to the EC2 instance."", ""Create an S3 service role with read-only permissions for the S3 bucket. Attach the role to the EC2 instance.""]","[""Create an EC2 service role with read-only permissions for the S3 bucket. Attach the role to the EC2 instance.""]","```json
{
    ""Create an IAM user with read-only permissions for the S3 bucket. Temporarily store the user credentials in the Amazon EBS volume of the EC2 instance."": ""This choice is incorrect because storing IAM user credentials on an Amazon EBS volume is not secure. If the instance is compromised, the credentials could be exposed. AWS best practices recommend using IAM roles for EC2 instances to provide temporary credentials, which are automatically rotated and managed by AWS, reducing the risk of credential exposure."",
    ""Create an IAM user with read-only permissions for the S3 bucket. Temporarily store the user credentials in the user data of the EC2 instance."": ""This choice is incorrect because storing IAM user credentials in the user data of an EC2 instance is insecure. User data can be accessed by anyone with access to the instance, potentially exposing the credentials. AWS best practices recommend using IAM roles for EC2 instances, which provide secure, temporary access to AWS resources without the need to manage credentials manually."",
    ""Create an EC2 service role with read-only permissions for the S3 bucket. Attach the role to the EC2 instance."": ""This choice is correct because using an IAM role for EC2 instances is a security best practice. The role provides temporary credentials that are automatically rotated and managed by AWS, ensuring secure access to the S3 bucket. This approach eliminates the need to hard-code or manually manage credentials, reducing the risk of credential exposure. According to AWS documentation, IAM roles are the recommended way to grant permissions to applications running on EC2 instances."",
    ""Create an S3 service role with read-only permissions for the S3 bucket. Attach the role to the EC2 instance."": ""This choice is incorrect because S3 service roles are not designed to be attached to EC2 instances. Instead, EC2 instances should use IAM roles specifically created for them, known as instance profiles. These roles provide the necessary permissions for the instance to interact with other AWS services, such as S3, in a secure manner.""
}
```"
A Development team wants to immediately build and deploy an application whenever there is a change to the source code. Which approaches could be used to trigger the deployment? (Choose TWO),"[""Store the source code in an Amazon S3 bucket. Configure AWS CodePipeline to start whenever a file in the bucket changes."", ""Store the source code in an encrypted Amazon EBS volume. Configure AWS CodePipeline to start whenever a file in the volume changes."", ""Store the source code in an AWS CodeCommit repository. Configure AWS CodePipeline to start whenever a change is committed to the repository."", ""Store the source code in an Amazon S3 bucket. Configure AWS CodePipeline to start every 15 minutes."", ""Store the source code in an Amazon EC2 instance's ephemeral storage. Configure the instance to start AWS CodePipeline whenever there are changes to the source code.""]","[""Store the source code in an encrypted Amazon EBS volume. Configure AWS CodePipeline to start whenever a file in the volume changes."", ""Store the source code in an AWS CodeCommit repository. Configure AWS CodePipeline to start whenever a change is committed to the repository.""]","```json
{
    ""Store the source code in an Amazon S3 bucket. Configure AWS CodePipeline to start whenever a file in the bucket changes."": ""Incorrect. While Amazon S3 can store source code, AWS CodePipeline does not natively support triggering a pipeline based on changes to files in an S3 bucket. CodePipeline can be triggered by events in AWS CodeCommit, GitHub, or Bitbucket repositories, but not directly by S3 bucket changes."",
    ""Store the source code in an encrypted Amazon EBS volume. Configure AWS CodePipeline to start whenever a file in the volume changes."": ""Incorrect. AWS CodePipeline cannot be directly triggered by changes to files in an Amazon EBS volume. CodePipeline is designed to integrate with source repositories like AWS CodeCommit, GitHub, or Bitbucket, not EBS volumes."",
    ""Store the source code in an AWS CodeCommit repository. Configure AWS CodePipeline to start whenever a change is committed to the repository."": ""Correct. AWS CodeCommit is a fully managed source control service that integrates directly with AWS CodePipeline. You can configure CodePipeline to automatically start a pipeline when a change is committed to a CodeCommit repository, which is a common practice for continuous integration and deployment (CI/CD)."",
    ""Store the source code in an Amazon S3 bucket. Configure AWS CodePipeline to start every 15 minutes."": ""Incorrect. While you can schedule AWS CodePipeline to run at regular intervals using AWS CloudWatch Events, this approach does not provide immediate deployment upon code changes. The question specifies the need for immediate deployment, which is better achieved through event-driven triggers like those provided by AWS CodeCommit."",
    ""Store the source code in an Amazon EC2 instance's ephemeral storage. Configure the instance to start AWS CodePipeline whenever there are changes to the source code."": ""Incorrect. Using ephemeral storage on an EC2 instance for source code is not a best practice because the data is lost when the instance is stopped or terminated. Additionally, there is no direct mechanism to trigger AWS CodePipeline from changes in EC2 instance storage. AWS CodePipeline is better integrated with managed source control services like AWS CodeCommit.""
}
```"
"An application ingests a large number of small messages and stores them in a database. The application uses AWS Lambda. A Development team is making changes to the application's processing logic. In testing, it is taking more than 15 minutes to process each message. The team is concerned the current backend may time out. Which changes should be made to the backend system to ensure each message is processed in the MOST scalable way?","[""Add the messages to an Amazon SQS queue. Set up and Amazon EC2 instance to poll the queue and process messages as they arrive."", ""Add the messages to an Amazon SQS queue. Set up Amazon EC2 instances in an Auto Scaling group to poll the queue and process the messages as they arrive."", ""Create a support ticket to increase the Lambda timeout to 60 minutes to allow for increased processing time."", ""Change the application to directly insert the body of the message into an Amazon RDS database.""]","[""Add the messages to an Amazon SQS queue. Set up Amazon EC2 instances in an Auto Scaling group to poll the queue and process the messages as they arrive.""]","```json
{
    ""Add the messages to an Amazon SQS queue. Set up and Amazon EC2 instance to poll the queue and process messages as they arrive."": ""This option is incorrect because using a single EC2 instance to process messages does not provide scalability. If the message processing time exceeds the capacity of a single instance, it will become a bottleneck. Additionally, it does not handle increased load efficiently, as there is no mechanism to automatically scale the number of instances based on the queue length or processing demand."",
    ""Add the messages to an Amazon SQS queue. Set up Amazon EC2 instances in an Auto Scaling group to poll the queue and process the messages as they arrive."": ""This option is correct because it leverages Amazon SQS for decoupling the message ingestion from processing, allowing for scalable and reliable message handling. Using an Auto Scaling group with EC2 instances ensures that the system can automatically adjust the number of instances based on the queue length or processing demand, providing scalability and fault tolerance. This approach aligns with AWS best practices for building scalable applications."",
    ""Create a support ticket to increase the Lambda timeout to 60 minutes to allow for increased processing time."": ""This option is incorrect because AWS Lambda has a maximum timeout limit of 15 minutes, and it cannot be increased beyond this limit. Additionally, this approach does not address scalability concerns, as a single Lambda function would still be processing messages sequentially, which is not efficient for high volumes of messages."",
    ""Change the application to directly insert the body of the message into an Amazon RDS database."": ""This option is incorrect because directly inserting messages into a database without a queuing mechanism does not provide scalability or fault tolerance. It also couples the message ingestion tightly with the database, which can lead to performance issues if the database becomes a bottleneck. Using a queue like Amazon SQS allows for decoupling and more efficient handling of message bursts.""
}
```"
"A Software Engineer developed an AWS Lambda function in Node.js to do some CPU-intensive data processing. With the default settings, the Lambda function takes about 5 minutes to complete. Which approach should a Developer take to increase the speed of completion?","[""Instead of using Node.js, rewrite the Lambda function using Python."", ""Instead of packaging the libraries in the `ZIP` file with the function, move them to a Lambda layer and use the layer with the function."", ""Allocate the maximum available CPU units to the function."", ""Increase the available memory to the function.""]","[""Increase the available memory to the function.""]","```json
{
    ""Instead of using Node.js, rewrite the Lambda function using Python."": ""This choice is incorrect because the programming language itself (Node.js vs Python) is unlikely to significantly affect the performance of a CPU-intensive task in AWS Lambda. Both Node.js and Python are interpreted languages and the performance difference for CPU-bound tasks is generally negligible. The execution speed is more dependent on the allocated resources rather than the language used."",
    ""Instead of packaging the libraries in the `ZIP` file with the function, move them to a Lambda layer and use the layer with the function."": ""This choice is incorrect because moving libraries to a Lambda layer does not inherently improve the execution speed of a Lambda function. Lambda layers are used to manage dependencies more efficiently and reduce the size of deployment packages, but they do not impact the runtime performance of the function itself."",
    ""Allocate the maximum available CPU units to the function."": ""This choice is incorrect because AWS Lambda does not allow direct allocation of CPU units. Instead, CPU allocation is indirectly controlled by the amount of memory allocated to the function. More memory results in more CPU power being allocated, which can improve the performance of CPU-intensive tasks."",
    ""Increase the available memory to the function."": ""This choice is correct because in AWS Lambda, increasing the memory allocation also increases the CPU allocation proportionally. For CPU-intensive tasks, increasing memory can lead to faster execution times because more CPU power is available. This is a best practice for optimizing the performance of Lambda functions, especially for tasks that require significant processing power.""
}
```"
"An online retail company has deployed a serverless application with AWS Lambda, Amazon API Gateway, Amazon S3, and Amazon DynamoDB using AWS CloudFormation. The company rolled out a new release with major upgrades to the Lambda function and deployed the release to production. Subsequently, the application stopped working. Which solution should bring the application back up as quickly as possible?","[""Redeploy the application on Amazon EC2 so the Lambda function can resolve dependencies."", ""Migrate DynamoDB to Amazon RDS and redeploy the Lambda function."", ""Roll back the Lambda function to the previous version."", ""Deploy the latest Lambda function in a different Region.""]","[""Roll back the Lambda function to the previous version.""]","```json
{
    ""Redeploy the application on Amazon EC2 so the Lambda function can resolve dependencies."": ""This choice is incorrect because deploying the application on Amazon EC2 is a significant architectural change and does not address the immediate issue of the Lambda function upgrade causing the application to stop working. Lambda functions are designed to handle dependencies through layers or by packaging dependencies with the function code. Redeploying on EC2 would be time-consuming and unnecessary for a serverless application."",
    ""Migrate DynamoDB to Amazon RDS and redeploy the Lambda function."": ""This choice is incorrect because the problem is related to the Lambda function upgrade, not the database. Migrating from DynamoDB to Amazon RDS is a complex task that would not resolve the immediate issue and could introduce additional complications. DynamoDB and RDS serve different use cases, and the choice of database should align with the application's requirements rather than being a reactionary measure."",
    ""Roll back the Lambda function to the previous version."": ""This choice is correct because rolling back to a previous version of the Lambda function is a quick and effective way to restore the application to a working state. AWS Lambda supports versioning, allowing developers to revert to a stable version if a new deployment causes issues. This is a best practice for managing changes and ensuring application stability."",
    ""Deploy the latest Lambda function in a different Region."": ""This choice is incorrect because deploying the Lambda function in a different region does not address the root cause of the application failure. The issue is with the function's code or configuration, not its geographic location. Additionally, deploying in a different region could lead to increased latency and other complications without resolving the underlying problem.""
}
```"
A Developer is writing an application that will run on Amazon EC2 instances in an Auto Scaling group. The Developer wants to externalize session state to support the application. Which services will meet these needs? (Choose TWO),"[""Amazon DynamoDB."", ""Amazon Cognito."", ""Amazon ElastiCache."", ""Amazon EBS."", ""Amazon SQS.""]","[""Amazon DynamoDB."", ""Amazon ElastiCache.""]","```json
{
    ""Amazon DynamoDB."": ""Amazon DynamoDB is a fully managed NoSQL database service that is highly scalable and provides low-latency data access. It is suitable for externalizing session state because it can handle high request rates and offers features like TTL (Time to Live) for automatic expiration of session data. This makes it a best practice for storing session state in distributed applications running on EC2 instances in an Auto Scaling group, as it ensures that session data is consistent and available across multiple instances."",
    ""Amazon Cognito."": ""Amazon Cognito is primarily used for user authentication and authorization, providing user sign-up, sign-in, and access control. While it can manage user sessions, it is not designed for storing arbitrary session state data for applications. Therefore, it is not suitable for externalizing session state for an application running on EC2 instances."",
    ""Amazon ElastiCache."": ""Amazon ElastiCache is a fully managed in-memory data store that supports Redis and Memcached. It is ideal for externalizing session state due to its high performance and low latency, which are critical for session management. ElastiCache can store session data in memory, allowing for quick access and updates, making it a best practice for applications requiring fast session state retrieval."",
    ""Amazon EBS."": ""Amazon EBS (Elastic Block Store) provides block storage for use with EC2 instances. It is not suitable for externalizing session state because it is designed for persistent block storage rather than high-speed, scalable session management. EBS volumes are attached to specific EC2 instances, which does not align with the need for a shared session state across multiple instances in an Auto Scaling group."",
    ""Amazon SQS."": ""Amazon SQS (Simple Queue Service) is a message queuing service used for decoupling and scaling microservices, distributed systems, and serverless applications. It is not designed for storing session state, as its primary function is to facilitate message passing between components rather than maintaining session data. Therefore, it is not suitable for externalizing session state in this context.""
}
```"
"A Developer has a legacy application that is hosted on-premises. Other applications hosted on AWS depend on the on-premises application for proper functioning. In case of any application errors, the Developer wants to be able to use Amazon CloudWatch to monitor and troubleshoot all applications from one place. How can the Developer accomplish this?","[""Install an AWS SDK on the on-premises server to automatically send logs to CloudWatch."", ""Download the CloudWatch agent to the on-premises server. Configure the agent to use IAM user credentials with permissions for CloudWatch."", ""Upload log files from the on-premises server to Amazon S3 and have CloudWatch read the files."", ""Upload log files from the on-premises server to an Amazon EC2 instance and have the instance forward the logs to CloudWatch.""]","[""Download the CloudWatch agent to the on-premises server. Configure the agent to use IAM user credentials with permissions for CloudWatch.""]","```json
{
    ""Install an AWS SDK on the on-premises server to automatically send logs to CloudWatch."": ""This choice is incorrect because AWS SDKs are primarily used for interacting with AWS services programmatically, such as making API calls. They do not automatically send logs to CloudWatch. The SDKs would require custom code to be written to send logs, which is not the most efficient or straightforward method for monitoring logs from an on-premises server."",
    ""Download the CloudWatch agent to the on-premises server. Configure the agent to use IAM user credentials with permissions for CloudWatch."": ""This choice is correct because the CloudWatch agent is specifically designed to collect metrics and logs from servers, including on-premises servers, and send them to Amazon CloudWatch. By configuring the agent with IAM user credentials that have the necessary permissions, the developer can ensure that logs are sent securely and reliably to CloudWatch, allowing for centralized monitoring and troubleshooting. This is a best practice for integrating on-premises resources with AWS monitoring services."",
    ""Upload log files from the on-premises server to Amazon S3 and have CloudWatch read the files."": ""This choice is incorrect because CloudWatch does not natively read log files from Amazon S3. While you could theoretically upload logs to S3 and then use a Lambda function to process and send them to CloudWatch, this adds unnecessary complexity and is not a direct feature of CloudWatch. The CloudWatch agent provides a more straightforward and efficient solution."",
    ""Upload log files from the on-premises server to an Amazon EC2 instance and have the instance forward the logs to CloudWatch."": ""This choice is incorrect because it introduces unnecessary complexity and cost. Using an EC2 instance as an intermediary to forward logs to CloudWatch is not efficient when the CloudWatch agent can be installed directly on the on-premises server. This approach would require additional infrastructure management and does not leverage AWS best practices for monitoring on-premises resources.""
}
```"
A company is developing an application that will be accessed through the Amazon API Gateway REST API. Registered users should be the only ones who can access certain resources of this API. The token being used should expire automatically and needs to be refreshed periodically. How can a Developer meet these requirements?,"[""Create an Amazon Cognito identity pool, configure the Amazon Cognito Authorizer in API Gateway, and use the temporary credentials generated by the identity pool."", ""Create and maintain a database record for each user with a corresponding token and use an AWS Lambda authorizer in API Gateway."", ""Create an Amazon Cognito user pool, configure the Cognito Authorizer in API Gateway, and use the identity or access token."", ""Create an IAM user for each API user, attach an invoke permissions policy to the API, and use an IAM authorizer in API Gateway.""]","[""Create an Amazon Cognito user pool, configure the Cognito Authorizer in API Gateway, and use the identity or access token.""]","```json
{
    ""Create an Amazon Cognito identity pool, configure the Amazon Cognito Authorizer in API Gateway, and use the temporary credentials generated by the identity pool."": ""This option is incorrect because Amazon Cognito Identity Pools are used to provide temporary AWS credentials to access AWS services. They are not designed to handle authentication with token expiration and refresh capabilities directly. Identity Pools are more suited for granting access to AWS resources rather than managing user authentication and token lifecycle for API access."",
    ""Create and maintain a database record for each user with a corresponding token and use an AWS Lambda authorizer in API Gateway."": ""This option is incorrect because while it is possible to implement a custom token management system with a database and Lambda authorizer, it is not the recommended approach due to complexity and maintenance overhead. Amazon Cognito User Pools provide a managed solution for user authentication, token issuance, expiration, and refresh, which is more efficient and secure."",
    ""Create an Amazon Cognito user pool, configure the Cognito Authorizer in API Gateway, and use the identity or access token."": ""This option is correct because Amazon Cognito User Pools are specifically designed for user authentication and can issue JWT tokens that include expiration and refresh capabilities. By configuring the Cognito Authorizer in API Gateway, you can easily manage access to API resources for authenticated users. This approach follows AWS best practices for managing user authentication and authorization in a scalable and secure manner."",
    ""Create an IAM user for each API user, attach an invoke permissions policy to the API, and use an IAM authorizer in API Gateway."": ""This option is incorrect because creating an IAM user for each API user is not scalable and goes against best practices for managing application users. IAM users are intended for managing AWS account access rather than application-level authentication. Additionally, IAM does not provide token expiration and refresh capabilities, which are required in this scenario.""
}
```"
A Developer is working on a serverless project based in Java. Initial testing shows a cold start takes about 8 seconds on average for AWS Lambda functions. What should the Developer do to reduce the cold start time? (Choose TWO),"[""Add the Spring Framework to the project and enable dependency injection."", ""Reduce the deployment package by including only needed modules from the AWS SDK for Java."", ""Increase the memory allocation setting for the Lambda function."", ""Increase the timeout setting for the Lambda function."", ""Change the Lambda invocation mode from synchronous to asynchronous.""]","[""Add the Spring Framework to the project and enable dependency injection."", ""Change the Lambda invocation mode from synchronous to asynchronous.""]","```json
{
    ""Add the Spring Framework to the project and enable dependency injection."": ""This choice is incorrect. Adding the Spring Framework and enabling dependency injection can actually increase the cold start time because it adds additional overhead and complexity to the initialization process. The Spring Framework is known for its extensive setup and configuration time, which is counterproductive for reducing cold start times in AWS Lambda functions."",
    ""Reduce the deployment package by including only needed modules from the AWS SDK for Java."": ""This choice is correct. Reducing the size of the deployment package can significantly decrease the cold start time of a Lambda function. By including only the necessary modules from the AWS SDK for Java, the function's initialization time is reduced because there is less code to load and execute. This is a best practice for optimizing Lambda performance."",
    ""Increase the memory allocation setting for the Lambda function."": ""This choice is correct. Increasing the memory allocation for a Lambda function can lead to faster execution times, including reduced cold start latency. AWS Lambda allocates CPU power linearly in proportion to the amount of memory configured, so more memory can result in faster initialization and execution, thus reducing cold start times."",
    ""Increase the timeout setting for the Lambda function."": ""This choice is incorrect. Increasing the timeout setting does not directly affect the cold start time. The timeout setting determines how long a Lambda function can run before it is forcibly terminated, but it does not influence the initialization time of the function."",
    ""Change the Lambda invocation mode from synchronous to asynchronous."": ""This choice is incorrect. Changing the invocation mode from synchronous to asynchronous does not reduce the cold start time. The invocation mode affects how the function is called and how the response is handled, but it does not impact the initialization time of the Lambda function itself.""
}
```"
"A Developer is leveraging a Border Gateway Protocol (BGP)-based AWS VPN connection to connect from on-premises to Amazon EC2 instances in the Developer's account. The Developer is able to access an EC2 instance in subnet A, but is unable to access an EC2 instance in subnet B in the same VPC. Which logs can the Developer use to verify whether the traffic is reaching subnet B?","[""VPN logs."", ""BGP logs"", ""VPC Flow Logs."", ""AWS CloudTrail logs.""]","[""VPC Flow Logs.""]","```json
{
    ""VPN logs."": ""VPN logs typically provide information about the VPN connection status and events related to the VPN tunnel itself, such as connection establishment and termination. They do not provide detailed information about traffic reaching specific subnets within a VPC. Therefore, they are not useful for determining whether traffic is reaching subnet B."",
    ""BGP logs"": ""BGP logs would provide information about the routing decisions and the status of BGP sessions. While they can help determine if routes are being advertised correctly, they do not provide visibility into the actual traffic flow to specific subnets within a VPC. Thus, they are not suitable for verifying traffic to subnet B."",
    ""VPC Flow Logs."": ""VPC Flow Logs capture information about the IP traffic going to and from network interfaces in a VPC. They are the correct choice for verifying whether traffic is reaching subnet B, as they provide detailed insights into the traffic flow, including source and destination IP addresses, ports, and whether the traffic was accepted or rejected. This makes them ideal for diagnosing connectivity issues within a VPC."",
    ""AWS CloudTrail logs."": ""AWS CloudTrail logs record API calls made within an AWS account, providing a history of actions taken by users, roles, or AWS services. While useful for auditing and compliance, they do not provide information about network traffic flows or connectivity issues to specific subnets, making them unsuitable for this scenario.""
}
```"
"A Developer has created a new AWS IAM user that has s3 putObject permission to write to a specific Amazon S3 bucket. This S3 bucket uses server-side encryption with AWS KMS managed keys (SSE-KMS) as the default encryption. Using the access key and secret key of the IAM user, the application received an access denied error when calling the PutObject API. How can this issue be resolved?","[""Update the policy of the IAM user to allow the `s3 Encrypt` action."", ""Update the bucket policy of the S3 bucket to allow the IAM user to upload objects."", ""Update the policy of the IAM user to allow the `kms:GenerateDataKey` action."", ""Update the ACL of the S3 bucket to allow the IAM user to upload objects.""]","[""Update the policy of the IAM user to allow the `kms:GenerateDataKey` action.""]","```json
{
    ""Update the policy of the IAM user to allow the `s3 Encrypt` action."": ""This choice is incorrect because there is no specific `s3:Encrypt` action in AWS IAM policies. The issue is related to the use of KMS for encryption, not a missing S3 action. The IAM user needs permission to use the KMS key for encryption, which is addressed by allowing `kms:GenerateDataKey`."",
    ""Update the bucket policy of the S3 bucket to allow the IAM user to upload objects."": ""This choice is incorrect because the IAM user already has `s3:PutObject` permission. The problem is not with the bucket policy but with the lack of permission to use the KMS key for encryption. Bucket policies control access to the bucket itself, but they do not grant permissions for KMS operations."",
    ""Update the policy of the IAM user to allow the `kms:GenerateDataKey` action."": ""This choice is correct because when using SSE-KMS, the IAM user needs permission to use the KMS key to encrypt data. Specifically, the `kms:GenerateDataKey` permission is required to generate a data encryption key for encrypting the object. Without this permission, the PutObject operation fails with an access denied error."",
    ""Update the ACL of the S3 bucket to allow the IAM user to upload objects."": ""This choice is incorrect because ACLs (Access Control Lists) are used to manage access at the object level, and they do not address the issue of permissions related to KMS keys. The problem is with the IAM user's permissions to use the KMS key, not with the ability to upload objects to the bucket.""
}
```"
A company has a web application that uses an Amazon Cognito user pool for authentication. The company wants to create a login page with the company logo. What should a Developer do to meet these requirements?,"[""Create a hosted user interface in Amazon Cognito and customize it with the company logo."", ""Create a login page with the company logo and upload it to Amazon Cognito."", ""Create a login page in Amazon API Gateway with the logo and save the link in Amazon Cognito."", ""Upload the logo to the Amazon Cognito app settings and point to the logo on a custom login page.""]","[""Upload the logo to the Amazon Cognito app settings and point to the logo on a custom login page.""]","```json
{
    ""Create a hosted user interface in Amazon Cognito and customize it with the company logo."": ""This choice is incorrect because while Amazon Cognito does offer a hosted UI that can be customized, the customization options are limited to changing the CSS and adding a logo. However, if the requirement is to have a completely custom login page with specific branding, using the hosted UI may not provide the necessary flexibility."",
    ""Create a login page with the company logo and upload it to Amazon Cognito."": ""This choice is incorrect because Amazon Cognito does not allow you to upload custom HTML pages directly to the service. Instead, Cognito provides a hosted UI that can be customized to some extent, or you can create a completely separate custom login page hosted elsewhere."",
    ""Create a login page in Amazon API Gateway with the logo and save the link in Amazon Cognito."": ""This choice is incorrect because Amazon API Gateway is not used for creating login pages. API Gateway is primarily used for creating, deploying, and managing APIs. While you can create a custom login page and host it elsewhere, API Gateway is not the service used for hosting static web pages."",
    ""Upload the logo to the Amazon Cognito app settings and point to the logo on a custom login page."": ""This choice is correct because Amazon Cognito allows you to customize the hosted UI by uploading a logo in the app client settings. You can also create a completely custom login page hosted elsewhere and integrate it with Cognito for authentication. This approach provides the flexibility to design the login page according to the company's branding requirements.""
}
```"
"A Developer is working on an AWS Lambda function that accesses Amazon DynamoDB. The Lambda function must retrieve an item and update some of its attributes, or create the item if it does not exist. The Lambda function has access to the primary key. Which IAM permissions should the Developer request for the Lambda function to achieve this functionality?","[""dynamodb:DeleteItem dynamodb:GetItem dynamodb:PutItem."", ""dynamodb:UpdateItem dynamodb:GetItem dynamodb:DescribeTable."", ""dynamodb:GetRecords dynamodb:PutItem dynamodb:UpdateTable."", ""dynamodb:UpdateItem dynamodb:GetItem dynamodb:PutItem.""]","[""dynamodb:UpdateItem dynamodb:GetItem dynamodb:PutItem.""]","```json
{
    ""dynamodb:DeleteItem dynamodb:GetItem dynamodb:PutItem."": ""This choice is incorrect because it includes the dynamodb:DeleteItem permission, which is not necessary for the described functionality. The task requires updating an item or creating it if it does not exist, which does not involve deleting any items. The correct permissions should focus on getting, updating, and putting items."",
    ""dynamodb:UpdateItem dynamodb:GetItem dynamodb:DescribeTable."": ""This choice is incorrect because it includes the dynamodb:DescribeTable permission, which is not needed for the described functionality. DescribeTable is used to retrieve information about a table's structure and status, which is not required for retrieving, updating, or creating items. The correct permissions should include dynamodb:PutItem instead of dynamodb:DescribeTable."",
    ""dynamodb:GetRecords dynamodb:PutItem dynamodb:UpdateTable."": ""This choice is incorrect because it includes dynamodb:GetRecords and dynamodb:UpdateTable, which are not relevant to the task. dynamodb:GetRecords is used with DynamoDB Streams, and dynamodb:UpdateTable is used to modify table settings, neither of which are needed for retrieving, updating, or creating items. The correct permissions should include dynamodb:GetItem and dynamodb:UpdateItem."",
    ""dynamodb:UpdateItem dynamodb:GetItem dynamodb:PutItem."": ""This choice is correct because it includes the necessary permissions for the described functionality. dynamodb:GetItem is needed to retrieve the item using its primary key, dynamodb:UpdateItem is needed to update attributes of an existing item, and dynamodb:PutItem is necessary to create a new item if it does not exist. These permissions align with AWS best practices for accessing and modifying items in a DynamoDB table.""
}
```"
A Developer is storing sensitive data generated by an application in Amazon S3. The Developer wants to encrypt the data at rest. A company policy requires an audit trail of when the master key was used and by whom. Which encryption option will meet these requirements?,"[""Server-side encryption with Amazon S3 managed keys (SSE-S3)."", ""Server-side encryption with AWS KMS managed keys (SSE-KMS)."", ""Server-side encryption with customer-provided keys (SSE-C)."", ""Server-side encryption with self-managed keys.""]","[""Server-side encryption with AWS KMS managed keys (SSE-KMS).""]","```json
{
    ""Server-side encryption with Amazon S3 managed keys (SSE-S3)."": ""This option uses keys managed by Amazon S3 for encryption, but it does not provide detailed audit trails of when the master key was used or by whom. SSE-S3 is a simpler encryption method that does not integrate with AWS CloudTrail for logging key usage, which is required by the company policy for auditing purposes."",
    ""Server-side encryption with AWS KMS managed keys (SSE-KMS)."": ""This option is correct because it uses AWS Key Management Service (KMS) to manage encryption keys, providing detailed audit trails through AWS CloudTrail. KMS allows you to track when keys are used and by whom, meeting the company policy requirement for an audit trail. SSE-KMS is a best practice for encrypting sensitive data in S3 when auditability is required."",
    ""Server-side encryption with customer-provided keys (SSE-C)."": ""While SSE-C allows customers to manage their own encryption keys, it does not inherently provide an audit trail for key usage. The responsibility for logging and auditing key usage falls on the customer, which does not meet the requirement for an integrated audit trail provided by AWS services."",
    ""Server-side encryption with self-managed keys."": ""This option involves managing your own encryption keys outside of AWS services, which does not provide an integrated audit trail of key usage within AWS. This approach requires significant effort to implement and maintain logging and auditing mechanisms, which does not align with the requirement for using AWS services to provide an audit trail.""
}
```"
"A company's website runs on an Amazon EC2 instance and uses Auto Scaling to scale the environment during peak times. Website users across the world are experiencing high latency due to static content on the EC2 instance, even during non-peak hours. Which combination of steps will resolve the latency issue? (Choose TWO)","[""Double the Auto Scaling group's maximum number of servers."", ""Host the application code on AWS Lambda."", ""Scale vertically by resizing the EC2 instances."", ""Create an Amazon CloudFront distribution to cache the static content."", ""Store the application's static content in Amazon S3.""]","[""Create an Amazon CloudFront distribution to cache the static content."", ""Store the application's static content in Amazon S3.""]","{""Double the Auto Scaling group's maximum number of servers."": ""This choice is incorrect because increasing the number of servers in the Auto Scaling group addresses scaling issues during peak times but does not directly address latency issues caused by static content delivery. The problem is related to content delivery across the globe, which is better solved by caching and content distribution solutions."", ""Host the application code on AWS Lambda."": ""This choice is incorrect because while AWS Lambda can help with scaling and managing application code, it does not address the specific issue of high latency due to static content delivery. The problem is related to static content, not dynamic application code execution."", ""Scale vertically by resizing the EC2 instances."": ""This choice is incorrect because scaling vertically by increasing the size of EC2 instances can improve performance but does not solve the latency issue caused by the geographic distribution of users and static content delivery. The issue is better addressed by using a CDN and storing static content in a more globally accessible manner."", ""Create an Amazon CloudFront distribution to cache the static content."": ""This choice is correct because Amazon CloudFront is a Content Delivery Network (CDN) that caches content at edge locations around the world, reducing latency for users by serving content from locations closer to them. This is a best practice for improving performance for global users accessing static content."", ""Store the application's static content in Amazon S3."": ""This choice is correct because Amazon S3 is a highly durable and available storage service that can be used to store static content. When combined with CloudFront, it allows for efficient and scalable delivery of static content to users worldwide, reducing latency and improving performance.""}"
"A company is developing a web application that allows its employees to upload a profile picture to a private Amazon S3 bucket. There is no size limit for the profile pictures, which should be displayed every time an employee logs in. For security reasons, the pictures cannot be publicly accessible. What is a viable long-term solution for this scenario?","[""Generate a presigned URL when a picture is uploaded. Save the URL in an Amazon DynamoDB table. Return the URL to the browser when the employee logs in."", ""Save the picture's S3 key in an Amazon DynamoDB table. Create an Amazon S3 VPC endpoint to allow the employees to download pictures once they log in."", ""Encode a picture using base64. Save the base64 string in an Amazon DB table. Allow the browser to retrieve the string and convert it to a picture."", ""Save the picture's S3 key in an Amazon DynamoDB table. Use a function to generate a presigned URL every time an employee logs in. Return the URL to the browser.""]","[""Save the picture's S3 key in an Amazon DynamoDB table. Create an Amazon S3 VPC endpoint to allow the employees to download pictures once they log in.""]","```json
{
    ""Generate a presigned URL when a picture is uploaded. Save the URL in an Amazon DynamoDB table. Return the URL to the browser when the employee logs in."": ""This approach is incorrect because presigned URLs have a limited expiration time. Saving a presigned URL in a DynamoDB table for long-term use is not viable, as the URL will eventually expire, making it unusable for accessing the picture when the employee logs in later. AWS best practices recommend generating presigned URLs on-demand to ensure they are valid when needed."",
    ""Save the picture's S3 key in an Amazon DynamoDB table. Create an Amazon S3 VPC endpoint to allow the employees to download pictures once they log in."": ""This approach is correct. By saving the S3 key in a DynamoDB table, the application can retrieve the key and use it to access the picture. Creating an Amazon S3 VPC endpoint allows secure, private connectivity between the VPC and S3 without exposing the data to the public internet. This setup aligns with AWS best practices for security and scalability, as it keeps the data private and accessible only within the VPC."",
    ""Encode a picture using base64. Save the base64 string in an Amazon DB table. Allow the browser to retrieve the string and convert it to a picture."": ""This approach is incorrect because storing images as base64 strings in a database is inefficient in terms of storage and retrieval performance. Base64 encoding increases the size of the data by approximately 33%, leading to increased storage costs and slower retrieval times. AWS best practices recommend storing binary data, such as images, directly in S3 rather than encoding them for database storage."",
    ""Save the picture's S3 key in an Amazon DynamoDB table. Use a function to generate a presigned URL every time an employee logs in. Return the URL to the browser."": ""This approach is viable but not as optimal as using an S3 VPC endpoint. Generating a presigned URL on-demand ensures that the URL is valid and secure for a limited time. However, this method requires additional compute resources to generate the URL each time an employee logs in. While it maintains security, using a VPC endpoint is more efficient for continuous access to private S3 resources without the need for presigned URLs.""
}
```"
A Developer is going to deploy an AWS Lambda function that requires significant CPU utilization. Which approach will MINIMIZE the average runtime of the function?,"[""Deploy the function into multiple AWS Regions."", ""Deploy the function into multiple Availability Zones."", ""Deploy the function using Lambda layers."", ""Deploy the function with its memory allocation set to the maximum amount.""]","[""Deploy the function with its memory allocation set to the maximum amount.""]","```json
{
    ""Deploy the function into multiple AWS Regions."": ""Deploying a Lambda function into multiple AWS Regions does not directly minimize the average runtime of the function. This approach is more about improving availability and reducing latency for users in different geographical locations. It does not impact the CPU utilization or execution speed of the function itself."",
    ""Deploy the function into multiple Availability Zones."": ""Deploying a Lambda function into multiple Availability Zones is primarily a strategy for improving fault tolerance and availability, not for reducing runtime. Lambda functions are inherently designed to be highly available and fault-tolerant across Availability Zones within a region, so this does not affect CPU utilization or execution speed."",
    ""Deploy the function using Lambda layers."": ""Lambda layers are used to manage and share code and dependencies across multiple functions. While they can help streamline deployment and reduce package size, they do not directly impact the CPU utilization or execution speed of a Lambda function. Therefore, using layers does not minimize the average runtime of a CPU-intensive function."",
    ""Deploy the function with its memory allocation set to the maximum amount."": ""In AWS Lambda, the amount of memory allocated to a function is directly proportional to the CPU power allocated. By increasing the memory allocation to the maximum, you also increase the CPU power, which can significantly reduce the execution time of CPU-intensive functions. This is a best practice for optimizing performance for compute-bound workloads, as documented in AWS Lambda best practices.""
}
```"
A company has a legacy application that was migrated to a fleet of Amazon EC2 instances. The application stores data in a MySQL database that is currently installed on a single EC2 instance. The company has decided to migrate the database from the EC2 instance to MySQL on Amazon RDS. What should the Developer do to update the application to support data storage in Amazon RDS?,"[""Update the database connection parameters in the application to point to the new RDS instance."", ""Add a script to the EC2 instance that implements an AWS SDK for requesting database credentials."", ""Create a new EC2 instance with an IAM role that allows access to the new RDS database."", ""Create an AWS Lambda function that will route traffic, from the EC2 instance to the RDS database.""]","[""Update the database connection parameters in the application to point to the new RDS instance.""]","```json
{
    ""Update the database connection parameters in the application to point to the new RDS instance."": ""This choice is correct because when migrating a database from an on-premises or EC2-hosted MySQL database to Amazon RDS, the primary change needed in the application is updating the database connection string to point to the new RDS endpoint. This involves changing the hostname, port, and possibly the database name in the application's configuration to match the new RDS instance. This is a common practice when migrating databases and aligns with AWS best practices for using managed services like RDS to offload database management tasks."",
    ""Add a script to the EC2 instance that implements an AWS SDK for requesting database credentials."": ""This choice is incorrect because while using AWS SDKs can be helpful for various AWS service interactions, simply migrating a database to RDS does not require the use of a script to request credentials. Typically, database credentials are stored in a secure manner, such as using AWS Secrets Manager, and are retrieved by the application at runtime. The primary task here is to update the connection parameters, not to implement a new method of credential retrieval."",
    ""Create a new EC2 instance with an IAM role that allows access to the new RDS database."": ""This choice is incorrect because creating a new EC2 instance is unnecessary for migrating a database to RDS. The existing application can continue to run on the current EC2 instances, and the focus should be on updating the database connection parameters. While IAM roles can be used to manage permissions, they are not directly relevant to connecting an application to an RDS database, which typically relies on database credentials rather than IAM roles."",
    ""Create an AWS Lambda function that will route traffic, from the EC2 instance to the RDS database."": ""This choice is incorrect because AWS Lambda is not needed to route traffic from an EC2 instance to an RDS database. The application running on EC2 can directly connect to the RDS instance using the updated connection parameters. Lambda functions are typically used for serverless compute tasks and event-driven architectures, not for routing database traffic from EC2 instances.""
}
```"
"A Developer has an e-commerce API hosted on Amazon ECS. Variable and spiking demand on the application is causing order processing to take too long. The application processes Amazon SQS queues. The ApproximateNumberOfMessagesVisible metric spikes at very high values throughout the day, which cause Amazon CloudWatch alarm breaches. Other ECS metrics for the API containers are well within limits. What can the Developer implement to improve performance while keeping costs low?","[""Target tracking scaling policy."", ""Docker Swarm."", ""Service scheduler."", ""Step scaling policy.""]","[""Step scaling policy.""]","```json
{
    ""Target tracking scaling policy."": ""Target tracking scaling policies are used to maintain a specific metric at a target value, such as keeping CPU utilization at a certain level. While useful for maintaining steady state performance, they may not be the best choice for handling sudden spikes in demand, as they adjust capacity more gradually. In this scenario, the ApproximateNumberOfMessagesVisible metric spikes significantly, which requires a more immediate scaling response, making this option less suitable."",
    ""Docker Swarm."": ""Docker Swarm is a container orchestration tool that is not directly related to AWS ECS scaling policies. It is used for managing Docker containers but does not provide the specific scaling capabilities needed to address the spiking demand and queue processing issues described in the scenario. Therefore, it is not applicable for improving performance in this AWS-specific context."",
    ""Service scheduler."": ""The service scheduler in ECS is responsible for placing tasks on instances, but it does not inherently provide scaling capabilities to address fluctuating demand. While it ensures tasks are running, it does not dynamically adjust the number of tasks based on queue metrics. Thus, it is not the correct solution for handling the spiking ApproximateNumberOfMessagesVisible metric."",
    ""Step scaling policy."": ""Step scaling policies are designed to handle changes in demand by adjusting the number of running instances or tasks based on CloudWatch alarms. They allow for more granular control by scaling in steps, which is ideal for scenarios with variable and spiking demand, like the one described. This approach can quickly add or remove capacity based on the ApproximateNumberOfMessagesVisible metric, effectively addressing the performance issue while keeping costs low. This aligns with AWS best practices for handling spiky workloads.""
}
```"
A Developer wants to build an application that will allow new users to register and create new user accounts. The application must also allow users with social media accounts to log in using their social media credentials. Which AWS service or feature can be used to meet these requirements?,"[""AWS IAM."", ""Amazon Cognito identity pools."", ""Amazon Cognito user pools."", ""AWS Directory Service.""]","[""Amazon Cognito user pools.""]","```json
{
    ""AWS IAM."": ""AWS Identity and Access Management (IAM) is primarily used for managing access to AWS resources and does not provide user registration or social media login capabilities. IAM is designed for managing permissions and roles for AWS services rather than handling user authentication for applications. Therefore, it is not suitable for building an application that requires user registration and social media login."",
    ""Amazon Cognito identity pools."": ""Amazon Cognito Identity Pools are used to provide temporary AWS credentials to users so they can access AWS services. While they can be used in conjunction with user pools to allow users to access AWS resources, they do not handle user registration or social media login directly. Identity pools are more focused on providing AWS resource access rather than user authentication."",
    ""Amazon Cognito user pools."": ""Amazon Cognito User Pools are the correct choice because they provide a user directory where users can sign up and sign in to your application. User pools support user registration, authentication, and account recovery. They also allow users to log in using social identity providers such as Facebook, Google, and Amazon, which meets the requirement of allowing users to log in with social media credentials. This makes them ideal for applications that require user management and social login capabilities."",
    ""AWS Directory Service."": ""AWS Directory Service is used to set up and run Microsoft Active Directory on AWS or connect AWS resources with an existing on-premises Active Directory. It is not designed for handling user registration or social media logins for applications. Its primary use case is for managing Windows-based workloads and integrating with on-premises directories, which does not align with the requirements of the question.""
}
```"
"A company wants to implement authentication for its new REST service using Amazon API Gateway. To authenticate the calls, each request must include HTTP headers with a client ID and user ID. These credentials must be compared to authentication data in an Amazon DynamoDB table. What MUST the company do to implement this authentication in API Gateway?","[""Implement an AWS Lambda authorizer that references the DynamoDB authentication table."", ""Create a model that requires the credentials, then grant API Gateway access to the authentication table."", ""Modify the integration requests to require the credentials, then grant API Gateway access to the authentication table."", ""Implement an Amazon Cognito authorizer that references the DynamoDB authentication table.""]","[""Implement an AWS Lambda authorizer that references the DynamoDB authentication table.""]","```json
{
    ""Implement an AWS Lambda authorizer that references the DynamoDB authentication table."": ""This choice is correct because AWS Lambda authorizers allow you to use a Lambda function to control access to your API Gateway methods. The Lambda function can be configured to read from a DynamoDB table to validate the client ID and user ID against stored authentication data. This approach is flexible and allows for custom authentication logic, which is necessary in this scenario."",
    ""Create a model that requires the credentials, then grant API Gateway access to the authentication table."": ""This choice is incorrect because creating a model in API Gateway is used for validating request payloads, not for authentication. Models define the data structure of the request and response payloads, but they do not handle authentication logic. Granting API Gateway access to the DynamoDB table would not enable it to perform authentication checks directly."",
    ""Modify the integration requests to require the credentials, then grant API Gateway access to the authentication table."": ""This choice is incorrect because modifying integration requests to require credentials does not implement authentication. Integration requests are used to map the incoming request to the backend, but they do not provide a mechanism for validating credentials against a database. API Gateway does not natively support direct authentication against DynamoDB without a Lambda authorizer."",
    ""Implement an Amazon Cognito authorizer that references the DynamoDB authentication table."": ""This choice is incorrect because Amazon Cognito authorizers are used to validate tokens issued by Amazon Cognito user pools, not to directly authenticate against a DynamoDB table. Cognito authorizers are suitable when using Cognito for user management and authentication, but they do not support custom authentication logic involving DynamoDB without additional components like a Lambda function.""
}
```"
A Developer is trying to make API calls using SDK. The IAM user credentials used by the application require multi-factor authentication for all API calls. Which method the Developer use to access the multi-factor authentication protected API?,"[""GetFederationToken."", ""GetCallerIdentity."", ""GetSessionToken."", ""DecodeAutherizationMessage.""]","[""GetSessionToken.""]","```json
{
    ""GetFederationToken."": ""GetFederationToken is used to create temporary credentials for federated users who are authenticated in your system, such as through an identity provider. It is not used for IAM users requiring multi-factor authentication (MFA). Therefore, this method is incorrect for accessing API calls that require MFA."",
    ""GetCallerIdentity."": ""GetCallerIdentity is an AWS STS API call that returns details about the IAM user or role whose credentials are used to call the operation. It does not provide temporary credentials or handle MFA requirements, making it incorrect for accessing MFA-protected APIs."",
    ""GetSessionToken."": ""GetSessionToken is the correct method because it is used to obtain temporary security credentials for IAM users who require MFA. This call returns a set of temporary credentials that include an MFA token, allowing the user to make API calls that require MFA. This aligns with AWS best practices for securing API calls with MFA."",
    ""DecodeAutherizationMessage."": ""DecodeAuthorizationMessage is used to decode error messages that are encoded by AWS when an AWS service returns a message that is too long to display in the console. It is not related to obtaining temporary credentials or handling MFA, making it incorrect for this scenario.""
}
```"
"An application is running on a cluster of Amazon EC2 instances. While trying to read objects stored within a single Amazon S3 bucket that are encrypted with server-side encryption with AWS KMS managed keys (SSE-KMS), the application receives the following error. Which combination of steps should be taken to prevent this failure? (Choose TWO)","[""Contact AWS Support to request an AWS KMS rate limit increase."", ""Perform error retries with exponential backoff in the application code."", ""Contact AWS Support to request a S3 rate limit increase."", ""Import a customer master key (CMK) with a larger key size."", ""Use more than one customer master key (CMK) to encrypt S3 data.""]","[""Contact AWS Support to request a S3 rate limit increase."", ""Import a customer master key (CMK) with a larger key size.""]","```json
{
    ""Contact AWS Support to request an AWS KMS rate limit increase."": ""This choice is incorrect. AWS KMS has its own set of quotas, but contacting AWS Support for a rate limit increase is not typically the first step. Instead, optimizing the use of KMS through techniques like batching requests or using data keys more efficiently is recommended. Additionally, AWS Support may not be able to increase KMS limits as they are often fixed."",
    ""Perform error retries with exponential backoff in the application code."": ""This choice is correct. Implementing error retries with exponential backoff is a best practice when dealing with AWS services, including S3 and KMS, to handle transient errors and throttling. This approach helps to mitigate temporary spikes in request rates and can prevent errors related to rate limits."",
    ""Contact AWS Support to request a S3 rate limit increase."": ""This choice is incorrect. S3 rate limits are generally very high, and contacting AWS Support for an increase is not a typical solution. Instead, optimizing the application to distribute requests more evenly or using techniques like multipart uploads can help manage high request rates."",
    ""Import a customer master key (CMK) with a larger key size."": ""This choice is incorrect. The key size of a CMK does not affect the rate limits or the ability to decrypt objects in S3. AWS KMS manages encryption and decryption operations regardless of the key size, and increasing the key size does not resolve rate limit issues."",
    ""Use more than one customer master key (CMK) to encrypt S3 data."": ""This choice is incorrect. Using multiple CMKs does not inherently solve rate limit issues. The problem is more likely related to the number of requests being made to KMS, and using multiple keys could complicate key management without addressing the underlying issue.""
}
```"
"When developing an AWS Lambda function that processes Amazon Kinesis Data Streams, Administrators within the company must receive a notice that includes the processed data. How should the Developer write the function to send processed data to the Administrators?","[""Separate the Lambda handler from the core logic."", ""Use Amazon CloudWatch Events to send the processed data."", ""Publish the processed data to an Amazon SNS topic."", ""Push the processed data to Amazon SQS.""]","[""Use Amazon CloudWatch Events to send the processed data.""]","```json
{
    ""Separate the Lambda handler from the core logic."": ""This choice is incorrect. Separating the Lambda handler from the core logic is a best practice for organizing code and improving maintainability, but it does not address the requirement of notifying administrators with the processed data. This choice does not involve any mechanism for sending notifications or data to administrators."",
    ""Use Amazon CloudWatch Events to send the processed data."": ""This choice is incorrect. Amazon CloudWatch Events is used for responding to changes in AWS resources, not for sending processed data to administrators. It is typically used to trigger actions based on events, such as invoking a Lambda function or sending a notification, but it is not designed to directly send processed data to users or administrators."",
    ""Publish the processed data to an Amazon SNS topic."": ""This choice is correct. Amazon SNS (Simple Notification Service) is designed for sending notifications and messages to subscribers. By publishing the processed data to an SNS topic, administrators can subscribe to receive notifications via email, SMS, or other supported protocols. This aligns with the requirement to notify administrators with the processed data."",
    ""Push the processed data to Amazon SQS."": ""This choice is incorrect. Amazon SQS (Simple Queue Service) is used for decoupling and buffering messages between distributed systems, not for sending notifications to administrators. While SQS can store and forward messages, it does not have the capability to directly notify users or administrators without additional components.""
}
```"
"A Developer is storing sensitive documents in Amazon S3 that will require encryption at rest. The encryption keys must be rotated annually, at least. What is the easiest way to achieve this?","[""Encrypt the data before sending it to Amazon S3."", ""Import a custom key into AWS KMS with annual rotation enabled."", ""Use AWS KMS with automatic key rotation."", ""Export a key from AWS KMS to encrypt the data.""]","[""Import a custom key into AWS KMS with annual rotation enabled.""]","```json
{
    ""Encrypt the data before sending it to Amazon S3."": ""This choice is incorrect because while client-side encryption ensures data is encrypted before reaching S3, it does not address the requirement for key rotation. The developer would need to manage key rotation manually, which is not the easiest method compared to using AWS services that automate this process."",
    ""Import a custom key into AWS KMS with annual rotation enabled."": ""This choice is correct because importing a custom key into AWS KMS allows for control over the key material, and AWS KMS provides the option to enable automatic key rotation annually. This meets the requirement for encryption at rest and simplifies the key management process by leveraging AWS's capabilities."",
    ""Use AWS KMS with automatic key rotation."": ""This choice is incorrect because while AWS KMS does support automatic key rotation, it only applies to AWS-managed keys (CMKs) and not to customer-managed keys imported into KMS. Therefore, it does not directly meet the requirement of using imported keys with annual rotation."",
    ""Export a key from AWS KMS to encrypt the data."": ""This choice is incorrect because AWS KMS does not support exporting keys. Keys managed by AWS KMS are designed to stay within the service to ensure security and compliance. Exporting keys would also require manual handling of key rotation, which is not the easiest method.""
}
```"
A company is creating a REST service using an Amazon API Gateway with AWS Lambda integration. The service must run different versions for testing purposes. What would be the BEST way to accomplish this?,"[""Use an x-Version header to denote which version is being called and pass that header to the Lambda function(s)."", ""Create an API Gateway Lambda authorizer to route API clients to the correct API version."", ""Create an API Gateway resource policy to isolate versions and provide context to the Lambda function(s)."", ""Deploy the API versions as unique stages with unique endpoints and use stage variables to provide further context.""]","[""Deploy the API versions as unique stages with unique endpoints and use stage variables to provide further context.""]","```json
{
    ""Use an x-Version header to denote which version is being called and pass that header to the Lambda function(s)."": ""This approach is incorrect because it relies on the client to specify the version, which can lead to errors and inconsistencies. It also requires additional logic in the Lambda function to handle different versions, which can complicate the code and increase maintenance overhead. This method does not leverage API Gateway's built-in versioning capabilities."",
    ""Create an API Gateway Lambda authorizer to route API clients to the correct API version."": ""This is incorrect because Lambda authorizers are primarily used for authentication and authorization, not for routing requests to different API versions. Using an authorizer for version routing is an unconventional use of the service and can add unnecessary complexity to the API management process."",
    ""Create an API Gateway resource policy to isolate versions and provide context to the Lambda function(s)."": ""This option is incorrect because resource policies in API Gateway are used to control access to the API, not for version management. They are designed to restrict access based on IP addresses or AWS accounts, not to manage different API versions."",
    ""Deploy the API versions as unique stages with unique endpoints and use stage variables to provide further context."": ""This is the correct approach because API Gateway stages allow you to deploy different versions of your API with unique endpoints. Each stage can have its own configuration and settings, including stage variables, which can be used to pass context to the Lambda functions. This method aligns with AWS best practices for managing API versions and provides a clear and organized way to handle multiple versions of an API.""
}
```"
A Developer must encrypt a 100-GB object using AWS KMS. What is the BEST approach?,"[""Make an Encrypt API call to encrypt the plaintext data as ciphertext using a customer master key (CMK)."", ""Make an Encrypt API call to encrypt the plaintext data as ciphertext using a customer master key (CMK) with imported key material."", ""Make an GenerateDataKey API call that returns a plaintext key and an encrypted copy of a data key. Use a plaintext key to encrypt the data."", ""Make an GenerateDataKeyWithoutPlaintext API call that returns an encrypted copy of a data key. Use an encrypted key to encrypt the data.""]","[""Make an GenerateDataKeyWithoutPlaintext API call that returns an encrypted copy of a data key. Use an encrypted key to encrypt the data.""]","```json
{
    ""Make an Encrypt API call to encrypt the plaintext data as ciphertext using a customer master key (CMK)."": ""This choice is incorrect because the Encrypt API call is not suitable for encrypting large objects such as a 100-GB file. The Encrypt API is designed for small amounts of data, typically up to 4 KB. For larger data, AWS recommends using the GenerateDataKey API to perform envelope encryption, where a data key is used to encrypt the large object and the data key itself is encrypted with the CMK."",
    ""Make an Encrypt API call to encrypt the plaintext data as ciphertext using a customer master key (CMK) with imported key material."": ""This choice is incorrect for the same reason as the previous one. The Encrypt API is not intended for large data sizes. Additionally, the use of imported key material does not change the limitation of the Encrypt API regarding data size. The best practice for large data is to use envelope encryption with the GenerateDataKey API."",
    ""Make an GenerateDataKey API call that returns a plaintext key and an encrypted copy of a data key. Use a plaintext key to encrypt the data."": ""This choice is partially correct. The GenerateDataKey API is indeed the correct approach for encrypting large objects using envelope encryption. It provides both a plaintext data key for immediate use and an encrypted version of the data key for storage. However, the question specifies the 'BEST' approach, and using GenerateDataKeyWithoutPlaintext is more secure as it avoids exposing the plaintext key unnecessarily."",
    ""Make an GenerateDataKeyWithoutPlaintext API call that returns an encrypted copy of a data key. Use an encrypted key to encrypt the data."": ""This choice is correct. The GenerateDataKeyWithoutPlaintext API call provides an encrypted data key without exposing the plaintext key, enhancing security. This method is part of the envelope encryption process, which is recommended for encrypting large objects like a 100-GB file. The encrypted data key can be stored securely, and the plaintext key is never exposed, aligning with AWS best practices for data encryption.""
}
```"
A Development team would like to migrate their existing application code from a GitHub repository to AWS CodeCommit. What needs to be created before they can migrate a cloned repository to CodeCommit over HTTPS?,"[""A GitHub secure authentication token."", ""A public and private SSH key file."", ""A set of Git credentials generated from IAM."", ""An Amazon EC2 IAM role with CodeCommit permissions.""]","[""A set of Git credentials generated from IAM.""]","{""A GitHub secure authentication token."": ""This choice is incorrect because a GitHub secure authentication token is used for authenticating with GitHub, not AWS CodeCommit. When migrating to CodeCommit, you need credentials that allow you to authenticate with AWS services, not GitHub."", ""A public and private SSH key file."": ""This choice is incorrect in the context of using HTTPS for connecting to CodeCommit. SSH keys are used for SSH connections, not HTTPS. While SSH can be used with CodeCommit, the question specifically asks about HTTPS, which requires a different authentication method."", ""A set of Git credentials generated from IAM."": ""This choice is correct because when using HTTPS to connect to AWS CodeCommit, you need a set of Git credentials generated from IAM. These credentials are specifically created for accessing CodeCommit repositories over HTTPS, as per AWS best practices. This involves generating a username and password in the IAM console that can be used with Git clients."", ""An Amazon EC2 IAM role with CodeCommit permissions."": ""This choice is incorrect because an IAM role with CodeCommit permissions is used for granting permissions to AWS resources like EC2 instances, not for authenticating a user over HTTPS. The question is about user authentication for Git operations, which requires Git credentials, not an IAM role.""}"
What item operation allows the retrieval of multiple items from a DynamoDB table in a single API call?,"[""Getltem."", ""BatchGetltem."", ""GetMultipleltems."", ""GetltemRange.""]","[""BatchGetltem.""]","```json
{
    ""Getltem."": ""GetItem is incorrect because it retrieves a single item from a DynamoDB table using its primary key. It does not support retrieving multiple items in a single API call. GetItem is typically used when you need to fetch a specific item and know its primary key."",
    ""BatchGetltem."": ""BatchGetItem is correct because it allows the retrieval of multiple items from one or more DynamoDB tables in a single API call. This operation is efficient and reduces the number of network round trips required to fetch multiple items. It is useful in scenarios where you need to retrieve a set of items with known keys, such as when displaying a list of user profiles or product details."",
    ""GetMultipleltems."": ""GetMultipleItems is incorrect because there is no such operation in DynamoDB. This option might be a misconception or a misunderstanding of the available DynamoDB API operations. The correct operation for retrieving multiple items is BatchGetItem."",
    ""GetltemRange."": ""GetItemRange is incorrect because there is no such operation in DynamoDB. This option might confuse with the Query operation, which retrieves items based on a range of keys but is not named GetItemRange. The Query operation is used for retrieving items within a partition key and a specified range of sort keys.""
}
```"
"After launching an instance that you intend to serve as a NAT (Network Address Translation) device in a public subnet you modify your route tables to have the NAT device be the target of internet bound traffic of your private subnet. When you try and make an outbound connection to the Internet from an instance in the private subnet, you are not successful. NAT device be the target of internet bound traffic of your private subnet. Which of the following steps could resolve the issue?","[""Attaching a second Elastic Network interface (ENI) to the NAT instance, and placing it in the private subnet."", ""Attaching a second Elastic Network Interface (ENI) to the instance in the private subnet, and placing it in the public subnet."", ""Disabling the Source/Destination Check attribute on the NAT instance."", ""Attaching an Elastic IP address to the instance in the private subnet.""]","[""Disabling the Source/Destination Check attribute on the NAT instance.""]","```json
{
    ""Attaching a second Elastic Network interface (ENI) to the NAT instance, and placing it in the private subnet."": ""This choice is incorrect because a NAT instance does not require a second ENI in the private subnet to function correctly. The NAT instance should have a single ENI in the public subnet with an Elastic IP to allow outbound internet access for instances in the private subnet. The key requirement for a NAT instance is to disable the Source/Destination Check, not to add additional ENIs."",
    ""Attaching a second Elastic Network Interface (ENI) to the instance in the private subnet, and placing it in the public subnet."": ""This choice is incorrect because placing an ENI of a private instance in the public subnet defeats the purpose of having a private subnet. The instance in the private subnet should remain isolated from direct internet access, and the NAT instance should handle the outbound traffic. This setup does not align with the best practices of using a NAT instance for internet access."",
    ""Disabling the Source/Destination Check attribute on the NAT instance."": ""This choice is correct because disabling the Source/Destination Check is a necessary step for a NAT instance to function properly. By default, EC2 instances perform source/destination checks, which means they must be the source or destination of any traffic they send or receive. For a NAT instance, this check must be disabled to allow it to forward traffic from instances in the private subnet to the internet and vice versa. This is a documented AWS best practice for setting up a NAT instance."",
    ""Attaching an Elastic IP address to the instance in the private subnet."": ""This choice is incorrect because attaching an Elastic IP to an instance in a private subnet does not provide internet access. Instances in private subnets are not directly accessible from the internet, even with an Elastic IP. The correct approach is to route outbound traffic through a NAT instance or NAT gateway in a public subnet.""
}
```"
"You attempt to store an object in the US-STANDARD region in Amazon S3, and receive a confirmation that it has been successfully stored. You then immediately make another API call and attempt to read this object. S3 tells you that the object does not exist. What could explain this behavior?","[""US-STANDARD uses eventual consistency and it can take time for an object to be readable in a bucket."", ""Objects in Amazon S3 do not become visible until they are replicated to a second region."", ""US-STANDARD imposes a 1 second delay before new objects are readable."", ""ou exceeded the bucket object limit, and once this limit is raised the object will be visible.""]","[""US-STANDARD uses eventual consistency and it can take time for an object to be readable in a bucket.""]","```json
{
    ""US-STANDARD uses eventual consistency and it can take time for an object to be readable in a bucket."": ""This choice is correct. Amazon S3 provides eventual consistency for overwrite PUTS and DELETES in all regions. For new objects, S3 offers read-after-write consistency, but there can be slight delays in propagation, especially in the US-STANDARD region (now known as the US East (N. Virginia) region). This means that immediately after writing an object, there might be a short period where a subsequent read request does not find the object, leading to the behavior described."",
    ""Objects in Amazon S3 do not become visible until they are replicated to a second region."": ""This choice is incorrect. Amazon S3 does not require objects to be replicated to a second region before they become visible. Objects are typically available in the region they are stored in immediately after a successful PUT operation, subject to eventual consistency for overwrite operations. Cross-region replication is an optional feature for redundancy and does not affect the visibility of objects in the original region."",
    ""US-STANDARD imposes a 1 second delay before new objects are readable."": ""This choice is incorrect. There is no specific 1 second delay imposed by the US-STANDARD region for object visibility. While there might be a slight delay due to eventual consistency, especially in the case of overwrites, there is no fixed delay time for new objects to become readable."",
    ""You exceeded the bucket object limit, and once this limit is raised the object will be visible."": ""This choice is incorrect. Amazon S3 does not have a limit on the number of objects you can store in a bucket. The visibility of an object is not related to any bucket object limit, as S3 is designed to scale horizontally and handle virtually unlimited storage.""
}
```"
What is the maximum number of S3 Buckets available per AWS account?,"[""100 per region."", ""there is no limit."", ""100 per account."", ""500 per account."", ""100 per IAM user.""]","[""100 per account.""]","{""100 per region."": ""This choice is incorrect. AWS S3 bucket limits are not set on a per-region basis. The limit is per AWS account, not per region. This is a common misconception as many AWS services do have regional limits, but S3 bucket limits are global per account."", ""there is no limit."": ""This choice is incorrect. There is a limit to the number of S3 buckets you can create per AWS account. The default limit is 100 buckets per account, although you can request an increase through AWS Support. This limit is in place to help manage resources and prevent abuse."", ""100 per account."": ""This choice is correct. According to AWS documentation, the default limit for the number of S3 buckets per AWS account is 100. This is a global limit, meaning it applies across all regions. This limit can be increased by submitting a request to AWS Support, but 100 is the default. This knowledge is crucial for planning and managing resources in AWS, especially for organizations that use a large number of buckets for different applications or departments."", ""500 per account."": ""This choice is incorrect. The default limit is 100 buckets per account, not 500. While it is possible to request a limit increase, the default remains 100. This choice might stem from a misunderstanding of AWS's ability to increase limits upon request, but it does not reflect the default setting."", ""100 per IAM user."": ""This choice is incorrect. S3 bucket limits are not set per IAM user; they are set per AWS account. IAM users operate under the constraints of the account's limits, not their own individual limits. This is a key distinction in understanding how AWS resource limits are applied.""}"
Which of the following items are required to allow an application deployed on an EC2 instance to write data to a DynamoDB table? Assume that no security Keys are allowed to be stored on the EC2 instance. (Choose TWO),"[""Create an IAM User that allows write access to the DynamoDB table."", ""Add an IAM Role to a running EC2 instance."", ""Add an IAM User to a running EC2 Instance."", ""Launch an EC2 Instance with the IAM Role included in the launch configuration."", ""Create an IAM Role that allows write access to the DynamoDB table."", ""Launch an EC2 Instance with the IAM User included in the launch configuration.""]","[""Launch an EC2 Instance with the IAM Role included in the launch configuration."", ""Create an IAM Role that allows write access to the DynamoDB table.""]","{""Create an IAM User that allows write access to the DynamoDB table."": ""Incorrect. IAM Users are not suitable for EC2 instances when no security keys are allowed to be stored. Instead, IAM Roles should be used to grant permissions to AWS services running on EC2 instances, as they provide temporary credentials that are automatically rotated."", ""Add an IAM Role to a running EC2 instance."": ""Correct. Adding an IAM Role to a running EC2 instance allows the instance to assume the role and access AWS resources like DynamoDB without needing to store security keys on the instance. This is in line with AWS best practices for security and management."", ""Add an IAM User to a running EC2 Instance."": ""Incorrect. IAM Users cannot be directly added to EC2 instances. IAM Roles are the correct mechanism for granting permissions to applications running on EC2 instances, as they provide temporary credentials and are more secure."", ""Launch an EC2 Instance with the IAM Role included in the launch configuration."": ""Correct. Launching an EC2 instance with an IAM Role included in the launch configuration ensures that the instance has the necessary permissions to access AWS resources like DynamoDB from the start. This is a best practice for managing permissions securely."", ""Create an IAM Role that allows write access to the DynamoDB table."": ""Correct. Creating an IAM Role with the necessary permissions to write to a DynamoDB table is essential. This role can then be attached to an EC2 instance to allow secure access without storing credentials on the instance."", ""Launch an EC2 Instance with the IAM User included in the launch configuration."": ""Incorrect. IAM Users cannot be included in EC2 launch configurations. IAM Roles should be used instead, as they are designed to provide permissions to AWS services running on EC2 instances securely.""}"
Which of the following are correct statements with policy evaluation logic in AWS Identity and Access Management? (Choose TWO),"[""By default, all requests are denied."", ""An explicit allow overrides an explicit deny."", ""An explicit allow overrides default deny."", ""An explicit deny does not override an explicit allow."", ""By default, all request are allowed.""]","[""By default, all requests are denied."", ""An explicit allow overrides default deny.""]","```json
{
    ""By default, all requests are denied."": ""This statement is correct. In AWS Identity and Access Management (IAM), the default behavior is to deny all requests. This is known as the 'default deny' policy. Unless there is an explicit allow policy attached to a user, group, or role, the request will be denied. This principle of least privilege ensures that no access is granted unless explicitly specified."",
    ""An explicit allow overrides an explicit deny."": ""This statement is incorrect. In AWS IAM policy evaluation logic, an explicit deny always overrides any explicit allow. If there is a policy that explicitly denies a request, it will take precedence over any policies that explicitly allow the request. This is a fundamental concept in AWS IAM to ensure that critical security controls cannot be bypassed."",
    ""An explicit allow overrides default deny."": ""This statement is correct. While the default behavior is to deny all requests, an explicit allow policy will override this default deny. When a user, group, or role has an explicit allow policy for a specific action or resource, they are granted access despite the default deny. This is a key part of IAM's policy evaluation logic, allowing for controlled access based on defined policies."",
    ""An explicit deny does not override an explicit allow."": ""This statement is incorrect. In AWS IAM, an explicit deny always takes precedence over an explicit allow. If a policy explicitly denies access to a resource or action, it will override any other policies that might allow it. This ensures that security controls are enforced and cannot be bypassed by other policies."",
    ""By default, all request are allowed."": ""This statement is incorrect. The default stance in AWS IAM is to deny all requests unless there is an explicit allow policy. This 'deny by default' approach is a security best practice to ensure that no unintended access is granted.""
}
```"
"You have an environment that consists of a public subnet using Amazon VPC and 3 instances that are running in this subnet. These three instances can successfully communicate with other hosts on the Internet. You launch a fourth instance in the same subnet, using the same AMI and security group configuration you used for the others, but find that this instance cannot be accessed from the Internet. What should you do to enable internet access?","[""Deploy a NAT instance into the public subnet."", ""Modify the routing table for the public subnet."", ""Configure a publically routable IP Address In the host OS of the fourth instance."", ""Assign an Elastic IP address to the fourth instance.""]","[""Assign an Elastic IP address to the fourth instance.""]","```json
{
    ""Deploy a NAT instance into the public subnet."": ""This choice is incorrect. A NAT instance is used to allow instances in a private subnet to initiate outbound traffic to the internet while preventing inbound traffic from the internet. Since the fourth instance is in a public subnet and needs direct internet access, a NAT instance is not applicable here."",
    ""Modify the routing table for the public subnet."": ""This choice is incorrect. The routing table for a public subnet should already have a route to an internet gateway to allow instances to communicate with the internet. Since the other instances in the same subnet can access the internet, the routing table is likely already configured correctly."",
    ""Configure a publically routable IP Address In the host OS of the fourth instance."": ""This choice is incorrect. AWS does not allow you to configure a public IP address directly in the host OS. Instead, AWS assigns public IP addresses at the network interface level. The correct approach is to assign an Elastic IP address through the AWS console or CLI."",
    ""Assign an Elastic IP address to the fourth instance."": ""This choice is correct. Instances in a public subnet need a public IP address to be accessible from the internet. By default, instances do not receive a public IP unless specified. Assigning an Elastic IP address ensures the instance has a static public IP, enabling internet access. This is a common practice in AWS to ensure consistent access to instances from the internet.""
}
```"
"If a message is retrieved from a queue in Amazon SQS, how long is the message inaccessible to other users by default?","[""0 seconds."", ""1 hour."", ""1 day."", ""forever."", ""30 seconds.""]","[""30 seconds.""]","```json
{
    ""0 seconds."": ""This choice is incorrect because when a message is retrieved from an Amazon SQS queue, it becomes temporarily invisible to other consumers for a period known as the visibility timeout. By default, this period is not 0 seconds; it is set to 30 seconds. This timeout allows the consumer to process the message without other consumers retrieving it simultaneously."",
    ""1 hour."": ""This choice is incorrect because the default visibility timeout for a message in Amazon SQS is not 1 hour. The default is 30 seconds. However, the visibility timeout can be configured to be anywhere from 0 seconds to 12 hours, depending on the application's needs, but the default setting is 30 seconds."",
    ""1 day."": ""This choice is incorrect because the default visibility timeout for a message in Amazon SQS is not 1 day. The default is 30 seconds. While the visibility timeout can be adjusted up to 12 hours, 1 day exceeds the maximum allowable visibility timeout for SQS."",
    ""forever."": ""This choice is incorrect because the visibility timeout in Amazon SQS cannot be set to forever. The visibility timeout is a temporary period during which a message is invisible to other consumers, and it is designed to ensure that messages are not processed multiple times. The default period is 30 seconds, and it can be adjusted up to 12 hours."",
    ""30 seconds."": ""This choice is correct because the default visibility timeout for a message in Amazon SQS is 30 seconds. This means that once a message is retrieved, it is invisible to other consumers for 30 seconds, allowing the current consumer to process it. This default setting is part of the SQS service's design to prevent duplicate processing and ensure message integrity. According to AWS documentation, the visibility timeout can be adjusted, but 30 seconds is the default setting.""
}
```"
What is the format of structured notification messages sent by Amazon SNS?,"[""An XML object containing Messageld, UnsubscribeURL, Subject, Message and other values."", ""An JSON object containing Messageld, DuplicateFlag, Message and other values."", ""An XML object containing Messageld, DuplicateFlag, Message and other values."", ""An JSON object containing Messageld, unsubscribeURL, Subject, Message and other values.""]","[""An JSON object containing Messageld, unsubscribeURL, Subject, Message and other values.""]","```json
{
    ""An XML object containing Messageld, UnsubscribeURL, Subject, Message and other values."": ""This choice is incorrect because Amazon SNS sends structured notification messages in JSON format, not XML. The JSON format is widely used for its simplicity and compatibility with web technologies, which is why SNS uses it for message delivery."",
    ""An JSON object containing Messageld, DuplicateFlag, Message and other values."": ""This choice is incorrect because while the format is JSON, the structured notification messages from SNS do not include a 'DuplicateFlag'. Instead, they include fields like 'unsubscribeURL', 'Subject', and 'Message'. The presence of 'DuplicateFlag' is not part of the standard SNS message structure."",
    ""An XML object containing Messageld, DuplicateFlag, Message and other values."": ""This choice is incorrect because SNS does not use XML for structured notification messages. The standard format is JSON, which is more efficient and easier to parse in most modern applications. Additionally, 'DuplicateFlag' is not a standard field in SNS messages."",
    ""An JSON object containing Messageld, unsubscribeURL, Subject, Message and other values."": ""This choice is correct because Amazon SNS sends structured notification messages as JSON objects. These messages typically include fields such as 'MessageId', 'unsubscribeURL', 'Subject', and 'Message'. This format is consistent with AWS's use of JSON for its simplicity and ease of integration with web services. The 'unsubscribeURL' is particularly important for allowing recipients to easily opt-out of receiving further messages.""
}
```"
"When uploading an object, what request header can be explicitly specified in a request to Amazon S3 to encrypt object data when saved on the server side?","[""x-amz-storage-class."", ""Content-MD5."", ""x-amz-security-token."", ""x-amz-server-side-encryption.""]","[""x-amz-server-side-encryption.""]","```json
{
    ""x-amz-storage-class."": ""This header is used to specify the storage class of the object in Amazon S3, such as STANDARD, REDUCED_REDUNDANCY, or INTELLIGENT_TIERING. It is not related to encryption, which is why it is incorrect for this question."",
    ""Content-MD5."": ""This header is used to provide a message integrity check for the object being uploaded to ensure that it was not corrupted during transit. It is not related to server-side encryption, making it incorrect for this question."",
    ""x-amz-security-token."": ""This header is used when making requests with temporary security credentials, such as those obtained from AWS STS. It is not related to specifying encryption for objects in S3, which is why it is incorrect for this question."",
    ""x-amz-server-side-encryption."": ""This header is correct because it is used to specify that Amazon S3 should use server-side encryption to encrypt the object at rest. This can be set to use Amazon S3-managed keys (SSE-S3), AWS KMS-managed keys (SSE-KMS), or customer-provided keys (SSE-C). This is a key concept in ensuring data security and compliance with data protection regulations.""
}
```"
Which of the following platforms are supported by Elastic Beanstalk? (Choose TWO),"[""Apache Tomcat."", "".NET."", ""IBM Websphere."", ""Oracle JBoss."", ""Jetty.""]","[""Apache Tomcat."", "".NET.""]","{""Apache Tomcat."": ""Elastic Beanstalk supports Apache Tomcat as one of its platforms. This is because Elastic Beanstalk is designed to simplify the deployment and management of applications in various programming languages and frameworks, including Java applications running on Apache Tomcat. This support allows developers to deploy Java web applications quickly without worrying about the underlying infrastructure. According to AWS documentation, Apache Tomcat is a supported platform, making this choice correct."", "".NET."": ""Elastic Beanstalk supports the .NET platform, which allows developers to deploy applications using the .NET framework on Windows Server environments. This is a correct choice because AWS provides Elastic Beanstalk environments specifically for .NET applications, enabling easy deployment and management of applications built with Microsoft technologies. This support aligns with AWS's goal to provide a broad range of platform support for developers."", ""IBM Websphere."": ""Elastic Beanstalk does not support IBM Websphere as a platform. IBM Websphere is a separate application server platform that is not natively supported by Elastic Beanstalk. AWS Elastic Beanstalk focuses on widely-used platforms such as Apache Tomcat, .NET, Node.js, and others, but IBM Websphere is not among them. Therefore, this choice is incorrect."", ""Oracle JBoss."": ""Elastic Beanstalk does not support Oracle JBoss as a platform. While JBoss is a popular application server, it is not one of the platforms that Elastic Beanstalk natively supports. Elastic Beanstalk provides support for other Java-based platforms like Apache Tomcat, but not JBoss. This makes this choice incorrect."", ""Jetty."": ""Elastic Beanstalk does not support Jetty as a platform. Jetty is a lightweight Java-based web server and servlet container, but it is not included in the list of platforms supported by Elastic Beanstalk. AWS focuses on supporting more commonly used platforms such as Apache Tomcat for Java applications. Therefore, this choice is incorrect.""}"
Which code snippet below returns the URL of a load balanced web site created in CloudFormation with an `AWS::ElasticLoadBalancing::LoadBalancer` resource name `ElasticLoad Balancer`?,"[""`\""Fn::Join\"":[ \""\"".[\""http://\"", {Fn::GetAtr\"": [ \""ElasticLoadBalancer\"",\""DNSName\""]}]]`."", ""`\""Fn::Join\"":[ \""\"".[\""http://\"", {Fn::GetAtr\"": [ \""ElasticLoadBalancer\"",\""Url\""]}]]`."", ""`\""Fn::Join\"":[ \""\"".[\""http://\"", {\""Ref : \""ElasticLoadBalancerUrl\""}]]`."", ""`\""Fn::Join\"":[ \""\"".[\""http://\"", {\""Ref : \""ElasticLoadBalancer\"",\""DNSName\""}]]`.""]","[""`\""Fn::Join\"":[ \""\"".[\""http://\"", {Fn::GetAtr\"": [ \""ElasticLoadBalancer\"",\""DNSName\""]}]]`.""]","```json
{
    ""\""Fn::Join\"":[ \""\"",[\""http://\"", {\""Fn::GetAtr\"": [ \""ElasticLoadBalancer\"",\""DNSName\""]}]]"": ""This choice is correct because it uses the intrinsic function Fn::GetAtt to retrieve the DNSName attribute of the Elastic Load Balancer. The DNSName is the correct attribute to use when constructing the URL for accessing the load-balanced website. This follows AWS best practices for referencing resource attributes in CloudFormation templates."",
    ""\""Fn::Join\"":[ \""\"",[\""http://\"", {\""Fn::GetAtr\"": [ \""ElasticLoadBalancer\"",\""Url\""]}]]"": ""This choice is incorrect because there is no 'Url' attribute for an AWS::ElasticLoadBalancing::LoadBalancer resource. The correct attribute to retrieve the DNS name of the load balancer is 'DNSName'. Using an incorrect attribute name will result in an error when the CloudFormation stack is created."",
    ""\""Fn::Join\"":[ \""\"",[\""http://\"", {\""Ref : \""ElasticLoadBalancerUrl\""}]]"": ""This choice is incorrect because it uses the Ref function incorrectly. The Ref function returns the physical ID of the resource or a specific value depending on the resource type. For an Elastic Load Balancer, Ref returns the name of the load balancer, not its DNS name or URL. Additionally, 'ElasticLoadBalancerUrl' is not a valid logical resource name in this context."",
    ""\""Fn::Join\"":[ \""\"",[\""http://\"", {\""Ref : \""ElasticLoadBalancer\"",\""DNSName\""}]]"": ""This choice is incorrect because it incorrectly combines the Ref function with an attribute name. The Ref function does not support retrieving specific attributes like DNSName. Instead, Fn::GetAtt should be used to access specific attributes of a resource. This syntax will result in an error.""
}
```"
Which features can be used to restrict access to data in S3? (Choose TWO),"[""Use S3 Virtual Hosting."", ""Set an S3 Bucket policy."", ""Enable IAM Identity Federation."", ""Set an S3 ACL on the bucket or the object."", ""Create a CloudFront distribution for the bucket.""]","[""Set an S3 Bucket policy."", ""Set an S3 ACL on the bucket or the object.""]","{""Use S3 Virtual Hosting."": ""S3 Virtual Hosting is a method to access S3 buckets using a domain name format, but it does not provide any mechanism for restricting access to data. It is used for routing requests to the correct bucket based on the URL, not for access control."", ""Set an S3 Bucket policy."": ""S3 Bucket policies are a powerful way to define access controls at the bucket level. They allow you to specify who can access the bucket and what actions they can perform. This is a correct answer because bucket policies are specifically designed to manage access permissions and are a best practice for securing S3 data."", ""Enable IAM Identity Federation."": ""IAM Identity Federation allows external identities to access AWS resources, but it does not directly restrict access to S3 data. It is more about enabling access rather than restricting it. Therefore, it is not applicable for directly restricting S3 access."", ""Set an S3 ACL on the bucket or the object."": ""S3 Access Control Lists (ACLs) are used to manage access permissions at the object and bucket level. They provide a way to grant read and write permissions to users and groups. This is a correct answer because ACLs are specifically designed for access control in S3."", ""Create a CloudFront distribution for the bucket."": ""CloudFront is a content delivery network (CDN) that can be used to distribute content globally with low latency. While it can be used in conjunction with S3 to improve performance and security (e.g., through signed URLs), it is not inherently a feature for restricting access to S3 data. Therefore, it is not directly related to restricting access.""}"
"What happens, by default, when one of the resources in a CloudFormation stack cannot be created?","[""Previously-created resources are kept but the stack creation terminates."", ""Previously-created resources are deleted and the stack creation terminates."", ""The stack creation continues, and the final results indicate which steps failed."", ""CloudFormation templates are parsed in advance so stack creation is guaranteed to succeed.""]","[""Previously-created resources are deleted and the stack creation terminates.""]","```json
{
    ""Previously-created resources are kept but the stack creation terminates."": ""This choice is incorrect. By default, AWS CloudFormation uses the 'rollback on failure' feature, which means that if any resource in the stack fails to be created, CloudFormation will automatically delete any resources that were successfully created up to that point. This ensures that there are no partial or inconsistent states in the stack."",
    ""Previously-created resources are deleted and the stack creation terminates."": ""This choice is correct. According to AWS CloudFormation's default behavior, if a resource cannot be created, CloudFormation will initiate a rollback, deleting any resources that were successfully created as part of the stack. This behavior helps maintain consistency and prevents partial stack deployments. This is a key AWS best practice to ensure that infrastructure is either fully deployed or not at all, avoiding potential issues with incomplete setups."",
    ""The stack creation continues, and the final results indicate which steps failed."": ""This choice is incorrect. CloudFormation does not continue with stack creation if a resource fails to be created. Instead, it performs a rollback to delete any resources that were already created. This prevents the stack from being left in an inconsistent state and ensures that any issues are addressed before a complete deployment is attempted again."",
    ""CloudFormation templates are parsed in advance so stack creation is guaranteed to succeed."": ""This choice is incorrect. While CloudFormation templates are validated for syntax and some logical errors before execution, there is no guarantee that stack creation will succeed. Factors such as resource limits, permissions, and external dependencies can cause resource creation to fail, triggering a rollback. CloudFormation's design accounts for these possibilities by implementing rollback on failure as a default behavior.""
}
```"
Which of the following are valid arguments for an SNS Publish request? (Choose THREE),"[""TopicAm."", ""Subject."", ""Destination."", ""Format."", ""Message."", ""Language.""]","[""Subject."", ""Destination."", ""Message.""]","```json
{
    ""TopicAm."": ""This is incorrect. The correct parameter for specifying the topic in an SNS Publish request is 'TopicArn', not 'TopicAm.'. This appears to be a typographical error or a misunderstanding of the correct parameter name."",
    ""Subject."": ""This is correct. 'Subject' is a valid parameter in an SNS Publish request. It allows you to specify a short summary or subject line for the message, which is particularly useful when sending notifications via email."",
    ""Destination."": ""This is incorrect. There is no 'Destination' parameter in an SNS Publish request. Instead, you specify the 'TopicArn' or 'TargetArn' to determine where the message should be sent."",
    ""Format."": ""This is incorrect. There is no 'Format' parameter in an SNS Publish request. The message format is typically determined by the protocol of the endpoint (e.g., JSON for HTTP/S endpoints), but it is not specified as a parameter in the request."",
    ""Message."": ""This is correct. 'Message' is a required parameter in an SNS Publish request. It contains the actual content of the message that you want to send to the subscribers of the topic."",
    ""Language."": ""This is incorrect. There is no 'Language' parameter in an SNS Publish request. The content of the message can be in any language, but the language itself is not specified as a parameter.""
}
```"
How can software determine the public and private IP addresses of the Amazon EC2 instance that it is running on?,"[""Query the appropriate Amazon CloudWatch metric."", ""Use ipconfig or ifconfig command."", ""Query the local instance userdata."", ""Query the local instance metadata.""]","[""Query the local instance metadata.""]","```json
{
    ""Query the appropriate Amazon CloudWatch metric."": ""This choice is incorrect because Amazon CloudWatch is primarily used for monitoring and logging metrics related to AWS resources, such as CPU utilization, disk I/O, and network traffic. It does not provide instance metadata information like IP addresses. CloudWatch metrics are not designed to provide real-time configuration details of an instance, such as its IP addresses."",
    ""Use ipconfig or ifconfig command."": ""This choice is partially correct but not the best practice. While using ipconfig (on Windows) or ifconfig (on Linux) can show the private IP address of the instance, it does not provide the public IP address. Additionally, these commands require OS-level access and do not provide a programmatic way to retrieve this information, which is often needed in automated scripts or applications."",
    ""Query the local instance userdata."": ""This choice is incorrect because instance userdata is used to pass configuration information or scripts to an instance at launch. It does not contain network configuration details like IP addresses. Userdata is typically used for bootstrapping instances and is not intended for querying instance metadata."",
    ""Query the local instance metadata."": ""This choice is correct. AWS provides a special URL (http://169.254.169.254/latest/meta-data/) that can be queried from within an EC2 instance to retrieve metadata about the instance, including both public and private IP addresses. This is a best practice for obtaining instance-specific information programmatically and is commonly used in scripts and applications running on the instance. The AWS documentation specifically outlines using instance metadata for this purpose.""
}
```"
EC2 instances are launched from Amazon Machine images (AMIs). A given public AMI can:,"[""Be used to launch EC2 Instances in any AWS region."", ""Only be used to launch EC2 instances in the same country as the AMI is stored."", ""Only be used to launch EC2 instances in the same AWS region as the AMI is stored."", ""Only be used to launch EC2 instances in the same AWS availability zone as the AMI is stored.""]","[""Only be used to launch EC2 instances in the same AWS region as the AMI is stored.""]","```json
{
    ""Be used to launch EC2 Instances in any AWS region."": ""This choice is incorrect. AMIs are region-specific, meaning they are stored in a particular AWS region and can only be used to launch EC2 instances within that same region. To use an AMI in a different region, you must copy the AMI to the desired region first. This is a key AWS concept because it helps manage data locality and compliance requirements."",
    ""Only be used to launch EC2 instances in the same country as the AMI is stored."": ""This choice is incorrect. AWS regions are not defined by country boundaries; they are specific geographic areas that can span multiple countries. AMIs are region-specific, not country-specific. Therefore, the concept of launching instances based on country is not applicable in AWS."",
    ""Only be used to launch EC2 instances in the same AWS region as the AMI is stored."": ""This choice is correct. AMIs are tied to a specific AWS region, and you can only launch EC2 instances from an AMI within the same region. This is a best practice in AWS to ensure data sovereignty and optimize network latency. If you need to use an AMI in another region, you must copy it to that region using the AWS Management Console, AWS CLI, or AWS SDKs."",
    ""Only be used to launch EC2 instances in the same AWS availability zone as the AMI is stored."": ""This choice is incorrect. While AMIs are region-specific, they are not limited to a single availability zone within that region. Once an AMI is in a region, you can launch EC2 instances in any availability zone within that region. This flexibility allows for high availability and fault tolerance across multiple availability zones.""
}
```"
Which EC2 API call would you use to retrieve a list of Amazon Machine Images (AMIs)?,"[""DescribeInstances."", ""DescribeAMIs."", ""DescribeImages."", ""GetAMIs."", ""You cannot retrieve a list of AMIs as there are over 10,000 AMIs.""]","[""DescribeImages.""]","```json
{
    ""DescribeInstances."": ""This choice is incorrect because DescribeInstances is an EC2 API call used to retrieve information about EC2 instances, not AMIs. It provides details such as instance state, type, and tags, but does not pertain to AMIs."",
    ""DescribeAMIs."": ""This choice is incorrect because there is no EC2 API call named DescribeAMIs. The correct API call to retrieve AMI information is DescribeImages. This choice may be a common misconception due to the naming convention."",
    ""DescribeImages."": ""This choice is correct because DescribeImages is the EC2 API call used to retrieve information about AMIs. It allows you to list AMIs available to your account, including public, private, and shared AMIs. This is a key function for managing and deploying instances using specific AMIs."",
    ""GetAMIs."": ""This choice is incorrect because there is no EC2 API call named GetAMIs. The correct API call for retrieving AMI information is DescribeImages. This choice may reflect a misunderstanding of the correct API naming conventions."",
    ""You cannot retrieve a list of AMIs as there are over 10,000 AMIs."": ""This choice is incorrect because it is a misconception. Despite the large number of AMIs, AWS provides the DescribeImages API call to list and filter AMIs based on various criteria such as owner, platform, and architecture. AWS services are designed to handle large datasets efficiently.""
}
```"
"In AWS, which security aspects are the customer's responsibility? (Choose FOUR)","[""Life-cycle management of IAM credentials."", ""Decommissioning storage devices."", ""Security Group and ACL (Access Control List) settings."", ""Encryption of EBS (Elastic Block Storage) volumes."", ""Controlling physical access to compute resources."", ""Patch management on the EC2 instance's operating system.""]","[""Life-cycle management of IAM credentials."", ""Security Group and ACL (Access Control List) settings."", ""Encryption of EBS (Elastic Block Storage) volumes."", ""Patch management on the EC2 instance's operating system.""]","```json
{
    ""Life-cycle management of IAM credentials."": ""This is correct because AWS Identity and Access Management (IAM) is a service that helps you securely control access to AWS services and resources. Customers are responsible for managing the lifecycle of IAM credentials, including creating, rotating, and deleting them as needed. This is part of the shared responsibility model where AWS manages the infrastructure security, and customers manage their data and access control."",
    ""Decommissioning storage devices."": ""This is incorrect because AWS is responsible for the decommissioning of storage devices. AWS follows strict procedures to ensure that data is securely erased from storage devices before they are reused or disposed of, as part of their compliance with various security standards."",
    ""Security Group and ACL (Access Control List) settings."": ""This is correct because configuring Security Groups and Network ACLs is part of the customer's responsibility. Security Groups act as a virtual firewall for your instance to control inbound and outbound traffic, and Network ACLs provide an additional layer of security at the subnet level. Customers must configure these settings to ensure their resources are protected according to their security requirements."",
    ""Encryption of EBS (Elastic Block Storage) volumes."": ""This is correct because while AWS provides the tools and services to encrypt data at rest and in transit, it is the customer's responsibility to enable and manage encryption for EBS volumes. Customers can use AWS Key Management Service (KMS) to manage the encryption keys."",
    ""Controlling physical access to compute resources."": ""This is incorrect because AWS is responsible for the physical security of the infrastructure that runs all of the services offered in the AWS Cloud. This includes controlling physical access to the data centers where compute resources are housed."",
    ""Patch management on the EC2 instance's operating system."": ""This is correct because customers are responsible for managing the operating system on their EC2 instances, including applying security patches and updates. AWS provides tools like AWS Systems Manager to help automate patch management, but the responsibility lies with the customer.""
}
```"
"When using a large Scan operation in DynamoDB, what technique can be used to minimize the impact of a scan on a table's provisioned throughput?","[""Set a smaller page size for the scan."", ""Use parallel scans."", ""Define a range index on the table."", ""Prewarm the table by updating all items.""]","[""Set a smaller page size for the scan.""]","{""Set a smaller page size for the scan."": ""This choice is correct. By setting a smaller page size, you reduce the amount of data read in a single request, which in turn reduces the impact on the table's provisioned throughput. DynamoDB allows you to control the number of items returned per request, which can help manage the read capacity units consumed during a scan operation. This is a recommended practice when you want to minimize the disruption to other operations on the table."", ""Use parallel scans."": ""This choice is incorrect in the context of minimizing impact on provisioned throughput. Parallel scans are used to increase the speed of a scan by dividing the table into segments and scanning each segment in parallel. While this speeds up the scan, it can increase the impact on provisioned throughput because it consumes more read capacity units simultaneously."", ""Define a range index on the table."": ""This choice is incorrect. Defining a range index (or sort key) is related to how data is organized and queried in DynamoDB, but it does not directly affect the impact of a scan operation on provisioned throughput. Scans read every item in the table, regardless of indexes, so adding a range index does not reduce the throughput impact of a scan."", ""Prewarm the table by updating all items."": ""This choice is incorrect. Prewarming a table by updating all items is not a recognized technique for managing scan operations in DynamoDB. Updating all items would actually increase the write throughput usage and could lead to additional costs and performance issues. It does not address the impact of a scan on read throughput.""}"
How can you secure data at rest on an EBS volume?,"[""Attach the volume to an instance using EC2's SSL interface."", ""Write the data randomly instead of sequentially."", ""Use an encrypted file system on top of the EBS volume."", ""Encrypt the volume using the S3 server-side encryption service."", ""Encrypt the volume using the S3 server-side encryption service.""]","[""Use an encrypted file system on top of the EBS volume.""]","```json
{
    ""Attach the volume to an instance using EC2's SSL interface."": ""This choice is incorrect because SSL (Secure Sockets Layer) is a protocol for encrypting data in transit, not at rest. EBS volumes require encryption at the storage level, not just during data transfer."",
    ""Write the data randomly instead of sequentially."": ""This choice is incorrect because the order in which data is written does not inherently provide security or encryption. Data security at rest requires encryption, not just a change in write patterns."",
    ""Use an encrypted file system on top of the EBS volume."": ""This choice is correct because using an encrypted file system, such as Amazon EFS with encryption enabled, provides a layer of encryption for data at rest on an EBS volume. This ensures that data is encrypted when stored, adhering to AWS best practices for securing data at rest."",
    ""Encrypt the volume using the S3 server-side encryption service."": ""This choice is incorrect because S3 server-side encryption is specific to Amazon S3, not EBS volumes. EBS volumes have their own encryption mechanisms, such as AWS KMS (Key Management Service) for encrypting data at rest."",
    ""Encrypt the volume using the S3 server-side encryption service."": ""This choice is incorrect for the same reason as the previous one. S3 server-side encryption is not applicable to EBS volumes. EBS volumes should be encrypted using their own encryption options, such as AWS KMS.""
}
```"
Which of the following is chosen as the default region when making an API call with an AWS SDK?,"[""ap-northeast-1."", ""us-west-2."", ""us-east-1."", ""eu-west-1."", ""us-central-1.""]","[""us-east-1.""]","```json
{
    ""ap-northeast-1."": ""This is incorrect because ap-northeast-1 is not the default region for AWS SDK API calls. The default region is us-east-1, which is the first region AWS launched and is often used as the default due to its historical significance and widespread use."",
    ""us-west-2."": ""This is incorrect because us-west-2 is not the default region for AWS SDK API calls. Although it is a popular region for deploying applications due to its location and availability, it is not set as the default in the AWS SDKs."",
    ""us-east-1."": ""This is correct because us-east-1 is the default region for AWS SDK API calls. This region, located in Northern Virginia, was the first AWS region and is often used as the default due to its central role in AWS's infrastructure and its extensive service offerings. AWS documentation and SDKs typically default to us-east-1 unless specified otherwise."",
    ""eu-west-1."": ""This is incorrect because eu-west-1 is not the default region for AWS SDK API calls. While it is a commonly used region in Europe, particularly for applications targeting European users, it is not the default setting in AWS SDKs."",
    ""us-central-1."": ""This is incorrect because us-central-1 is not a valid AWS region. AWS regions are typically named with a format that includes a geographic area and a number, such as us-east-1 or eu-west-1. There is no us-central-1 region in AWS.""
}
```"
Which of the following statements about SWF are true? (Choose THREE),"[""SWF tasks are assigned once and never duplicated."", ""SWF requires an S3 bucket for workflow storage."", ""SWF workflow executions can last up to a year."", ""SWF triggers SNS notifications on task assignment."", ""SWF uses deciders and workers to complete tasks."", ""SWF requires at least 1 EC2 instance per domain.""]","[""SWF tasks are assigned once and never duplicated."", ""SWF workflow executions can last up to a year."", ""SWF uses deciders and workers to complete tasks.""]","```json
{
    ""SWF tasks are assigned once and never duplicated."": ""This statement is correct. In Amazon SWF (Simple Workflow Service), tasks are assigned to a single worker at a time and are not duplicated. This ensures that each task is processed exactly once, which is crucial for maintaining the integrity and consistency of workflows. This behavior is part of SWF's design to manage distributed application components and ensure reliable task execution."",
    ""SWF requires an S3 bucket for workflow storage."": ""This statement is incorrect. Amazon SWF does not require an S3 bucket for workflow storage. SWF is a fully managed service that coordinates tasks across distributed application components. It handles the storage of workflow state and history internally, without the need for external storage like S3."",
    ""SWF workflow executions can last up to a year."": ""This statement is correct. Amazon SWF supports long-running workflows, and workflow executions can last up to a year. This is suitable for applications that require long-duration processes, such as order processing or complex business operations that span extended periods."",
    ""SWF triggers SNS notifications on task assignment."": ""This statement is incorrect. Amazon SWF does not natively trigger SNS notifications on task assignment. While SNS can be integrated with SWF for notifications, it is not a built-in feature of SWF to automatically send notifications upon task assignment. SWF focuses on task coordination and execution."",
    ""SWF uses deciders and workers to complete tasks."": ""This statement is correct. In Amazon SWF, deciders and workers are key components. Deciders are responsible for controlling the flow of activity tasks in a workflow execution, while workers perform the actual tasks. This division of roles allows for flexible and scalable workflow management."",
    ""SWF requires at least 1 EC2 instance per domain."": ""This statement is incorrect. Amazon SWF does not require a specific number of EC2 instances per domain. SWF is a managed service that does not impose infrastructure requirements on the user. The number of EC2 instances needed depends on the application's specific requirements and workload, not on SWF itself.""
}
```"
"A startup s photo-sharing site is deployed in a VPC. An ELB distributes web traffic across two subnets. ELB session stickiness is configured to use the AWSgenerated session cookie, with a session TTL of 5 minutes. The webserver Auto Scaling Group is configured as: min-size=4, max-size=4, The startups preparing for a public launch, by running load-testing software installed on a single EC2 instance running in us-west-2 After 60 minutes of load-testing, the webserver logs show: Which recommendations can help ensure load-testing HTTP requests are evenly distributed across the four webservers? (Choose TWO)","[""Launch and run the load-tester EC2 instance from us-east-1 instead."", ""Re-configure the load-testing software to re-resolve DNS for each web request."", ""Use a 3rd-party load-testing service which offers globally-distributed test clients."", ""Configure ELB and Auto Scaling to distribute across us-west-2a and us-west-2c."", ""Configure ELB session stickiness to use the app-specific session cookie.""]","[""Re-configure the load-testing software to re-resolve DNS for each web request."", ""Use a 3rd-party load-testing service which offers globally-distributed test clients.""]","```json
{
    ""Launch and run the load-tester EC2 instance from us-east-1 instead."": ""This choice is incorrect because launching the load-tester in a different region (us-east-1) does not address the issue of uneven load distribution across the web servers. The problem is related to DNS resolution and session stickiness, not the geographical location of the load tester."",
    ""Re-configure the load-testing software to re-resolve DNS for each web request."": ""This choice is correct because ELB uses DNS to distribute traffic. If the load-testing software caches the DNS resolution, it may repeatedly send requests to the same IP address, causing uneven distribution. By re-resolving DNS for each request, the load tester can ensure that requests are distributed more evenly across all available instances behind the ELB."",
    ""Use a 3rd-party load-testing service which offers globally-distributed test clients."": ""This choice is correct because using a globally-distributed load-testing service can simulate real-world traffic patterns more accurately. It helps in distributing the load across multiple instances by originating requests from different locations, which can lead to better testing of the ELB's ability to balance traffic."",
    ""Configure ELB and Auto Scaling to distribute across us-west-2a and us-west-2c."": ""This choice is incorrect because the question already states that the ELB distributes traffic across two subnets. The issue is not related to the distribution across availability zones but rather the DNS resolution and session stickiness affecting load distribution."",
    ""Configure ELB session stickiness to use the app-specific session cookie."": ""This choice is incorrect because changing the session stickiness to use an app-specific cookie does not resolve the issue of uneven load distribution caused by DNS caching. The session stickiness configuration is not the primary cause of the problem described.""
}
```"
Which of the following are valid SNS delivery transports? (Choose TWO),"[""HTTP."", ""UDP."", ""SMS."", ""DynamoDB."", ""Named Pipes.""]","[""HTTP."", ""SMS.""]","```json
{
    ""HTTP."": ""HTTP is a valid delivery transport for Amazon SNS. SNS can send notifications to HTTP/HTTPS endpoints, making it suitable for web applications that need to receive real-time notifications. This is supported by AWS documentation and is a common use case for integrating SNS with web services."",
    ""UDP."": ""UDP is not a valid delivery transport for Amazon SNS. SNS does not support UDP because it is a connectionless protocol and does not guarantee message delivery, which is contrary to the reliable message delivery that SNS aims to provide."",
    ""SMS."": ""SMS is a valid delivery transport for Amazon SNS. SNS can send notifications via SMS to mobile devices, which is useful for alerting users directly on their phones. This is a documented feature of SNS and is often used for sending critical alerts or notifications."",
    ""DynamoDB."": ""DynamoDB is not a valid delivery transport for Amazon SNS. DynamoDB is a NoSQL database service and is not used as a transport mechanism for SNS messages. SNS is designed to send notifications to endpoints like HTTP, email, and SMS, not to databases."",
    ""Named Pipes."": ""Named Pipes are not a valid delivery transport for Amazon SNS. Named Pipes are a method for inter-process communication on the same machine, which is not applicable to the cloud-based, distributed nature of SNS notifications.""
}
```"
"Company C has recently launched an online commerce site for bicycles on AWS. They have a `Product` DynamoDB table that stores details for each bicycle, such as, manufacturer, color, price, quantity and size to display in the online store. Due to customer demand, they want to include an image for each bicycle along with the existing details. Which approach below provides the least impact to provisioned throughput on the `Product` table?","[""Serialize the image and store it in multiple DynamoDB tables."", ""Create an `Images` DynamoDB table to store the Image with a foreign key constraint to the `Product` table."", ""Add an image data type to the `Product` table to store the images in binary format."", ""Store the images in Amazon S3 and add an S3 URL pointer to the `Product` table item for each image.""]","[""Store the images in Amazon S3 and add an S3 URL pointer to the `Product` table item for each image.""]","```json
{
    ""Serialize the image and store it in multiple DynamoDB tables."": ""This approach is incorrect because storing images directly in DynamoDB, especially serialized across multiple tables, is inefficient and can significantly impact throughput. DynamoDB is optimized for small, fast-access items, and storing large binary data like images can lead to increased costs and reduced performance. This method does not align with AWS best practices for handling large objects."",
    ""Create an `Images` DynamoDB table to store the Image with a foreign key constraint to the `Product` table."": ""This choice is incorrect because DynamoDB does not support foreign key constraints. While you could create a separate table for images, storing large binary data in DynamoDB is not recommended due to potential throughput and cost issues. Additionally, managing relationships between tables in DynamoDB is complex and not natively supported."",
    ""Add an image data type to the `Product` table to store the images in binary format."": ""This option is incorrect because DynamoDB does not have a specific image data type, and storing images as binary data directly in the `Product` table can lead to increased costs and reduced performance. Large binary objects can quickly consume provisioned throughput and storage, which is not ideal for a high-performance application."",
    ""Store the images in Amazon S3 and add an S3 URL pointer to the `Product` table item for each image."": ""This is the correct approach because Amazon S3 is designed for storing large objects like images. By storing images in S3 and referencing them with URLs in the DynamoDB table, you minimize the impact on DynamoDB's provisioned throughput. This method is cost-effective, scalable, and aligns with AWS best practices for separating data storage concerns. S3 provides high durability and availability, making it ideal for storing static assets like images.""
}
```"
Which DynamoDB limits can be raised by contacting AWS support? (Choose TWO),"[""The number of hash keys per account."", ""The maximum storage used per account."", ""The number of tables per account."", ""The number of local secondary indexes per account."", ""The number of provisioned throughput units per account.""]","[""The number of tables per account."", ""The number of provisioned throughput units per account.""]","{""The number of hash keys per account."": ""This is incorrect because DynamoDB does not impose a limit on the number of hash keys per account. Each table can have a unique set of hash keys, and there is no account-level restriction on the number of hash keys."", ""The maximum storage used per account."": ""This is incorrect because DynamoDB automatically scales storage as needed. There is no fixed limit on the amount of data you can store in DynamoDB, and thus no need to request a limit increase for storage."", ""The number of tables per account."": ""This is correct because AWS imposes a default limit on the number of tables you can create per account. However, this limit can be increased by contacting AWS support, as documented in AWS service limits."", ""The number of local secondary indexes per account."": ""This is incorrect because the limit on local secondary indexes is per table, not per account. Each table can have up to 5 local secondary indexes, and this limit cannot be increased."", ""The number of provisioned throughput units per account."": ""This is correct because AWS sets default limits on the amount of provisioned throughput capacity you can request per account. These limits can be increased by contacting AWS support, allowing for higher throughput capacity as needed.""}"
"When a Simple Queue Service message triggers a task that takes 5 minutes to complete, which process below will result in successful processing of the message and remove it from the queue while minimizing the chances of duplicate processing?","[""Retrieve the message with an increased visibility timeout, process the message, delete the message from the queue."", ""Retrieve the message with an increased visibility timeout, delete the message from the queue, process the message."", ""Retrieve the message with increased DelaySeconds, process the message, delete the message from the queue."", ""Retrieve the message with increased DelaySeconds, delete the message from the queue, process the message.""]","[""Retrieve the message with an increased visibility timeout, process the message, delete the message from the queue.""]","{""Retrieve the message with an increased visibility timeout, process the message, delete the message from the queue."": ""This choice is correct because increasing the visibility timeout ensures that the message is not visible to other consumers while it is being processed, reducing the chance of duplicate processing. Processing the message before deleting it ensures that the task is completed successfully before the message is removed from the queue. This follows AWS best practices for handling messages in SQS, where the visibility timeout should be set to cover the processing time to prevent the message from becoming visible again and being processed by another consumer."", ""Retrieve the message with an increased visibility timeout, delete the message from the queue, process the message."": ""This choice is incorrect because deleting the message before processing it can lead to data loss if the processing fails. The message should only be deleted after successful processing to ensure reliability and fault tolerance. AWS best practices recommend processing the message first to confirm successful task completion before removing it from the queue."", ""Retrieve the message with increased DelaySeconds, process the message, delete the message from the queue."": ""This choice is incorrect because DelaySeconds is used to postpone the delivery of new messages to the queue, not to extend the processing time of a message already being processed. Increasing DelaySeconds does not affect the visibility timeout of a message that has already been received. Therefore, it does not help in preventing duplicate processing during long tasks."", ""Retrieve the message with increased DelaySeconds, delete the message from the queue, process the message."": ""This choice is incorrect for two reasons: first, DelaySeconds is not relevant to the processing of a message that has already been received, as it only affects the initial delay before a message becomes visible in the queue. Second, deleting the message before processing it can lead to data loss if the processing fails, which is against AWS best practices for ensuring reliable message processing.""}"
Company A has an S3 bucket containing premier content that they intend to make available to only paid subscribers of their website. The S3 bucket currently has default permissions of all objects being private to prevent inadvertent exposure of the premier content to non-paying website visitors. How can Company A provide only paid subscribers the ability to download a premier content file in the S3 bucket?,"[""Apply a bucket policy that grants anonymous users to download the content from the S3 bucket."", ""Generate a pre-signed object URL for the premier content file when a paid subscriber requests a download."", ""Add a bucket policy that requires Multi-Factor Authentication for requests to access the S3 bucket objects."", ""Enable server side encryption on the S3 bucket for data protection against the non-paying website visitors.""]","[""Generate a pre-signed object URL for the premier content file when a paid subscriber requests a download.""]","```json
{
    ""Apply a bucket policy that grants anonymous users to download the content from the S3 bucket."": ""This choice is incorrect because granting anonymous users access would make the content publicly accessible to anyone, not just paid subscribers. This goes against the requirement of restricting access to only paid subscribers. AWS best practices recommend using more secure methods to control access, such as IAM policies, bucket policies with specific conditions, or pre-signed URLs."",
    ""Generate a pre-signed object URL for the premier content file when a paid subscriber requests a download."": ""This choice is correct because pre-signed URLs allow you to grant time-limited access to specific S3 objects to users without making the objects publicly accessible. This method is ideal for providing temporary access to paid subscribers, as the URL can be generated dynamically and expire after a set duration, ensuring that only authorized users can download the content. This aligns with AWS best practices for securely sharing S3 objects."",
    ""Add a bucket policy that requires Multi-Factor Authentication for requests to access the S3 bucket objects."": ""This choice is incorrect because while MFA can add an extra layer of security, it does not directly address the requirement of providing access only to paid subscribers. MFA is typically used to secure access for IAM users rather than end-users accessing content through a web application. Additionally, implementing MFA for S3 access is more complex and not commonly used for this scenario."",
    ""Enable server side encryption on the S3 bucket for data protection against the non-paying website visitors."": ""This choice is incorrect because server-side encryption protects data at rest but does not control access to the data. Encryption ensures that data is stored securely, but it does not restrict who can download or access the data. Access control should be managed through bucket policies, IAM policies, or pre-signed URLs, not encryption alone.""
}
```"
Which of the following is an example of a good DynamoDB hash key schema for provisioned throughput efficiency?,"[""User ID, where the application has many different users."", ""Status Code where most status codes are the same."", ""Device ID, where one is by far more popular than all the others."", ""Game Type, where there are three possible game types.""]","[""User ID, where the application has many different users.""]","{""User ID, where the application has many different users."": ""This choice is correct because using a User ID as a hash key in DynamoDB is a good practice when the application has many different users. This ensures a wide distribution of requests across partitions, which is crucial for provisioned throughput efficiency. DynamoDB partitions data based on the hash key, and having a large number of unique hash keys helps evenly distribute the workload, preventing any single partition from becoming a bottleneck. This aligns with AWS best practices for designing scalable and efficient DynamoDB tables."", ""Status Code where most status codes are the same."": ""This choice is incorrect because using a Status Code as a hash key, where most status codes are the same, leads to uneven distribution of data across partitions. If most of the data is associated with a few status codes, it can cause hot partitions, where some partitions receive a disproportionate amount of traffic. This can lead to throttling and inefficient use of provisioned throughput, which is against AWS best practices for DynamoDB."", ""Device ID, where one is by far more popular than all the others."": ""This choice is incorrect because using a Device ID as a hash key, where one device is significantly more popular, can result in a hot partition. This means that the partition containing the popular Device ID will receive a much higher volume of requests compared to others, leading to throttling and inefficient use of provisioned throughput. AWS recommends designing hash keys that ensure even distribution of traffic across partitions."", ""Game Type, where there are three possible game types."": ""This choice is incorrect because using Game Type as a hash key with only three possible values will lead to a very limited distribution of data across partitions. With such a small number of unique hash keys, it's likely that the workload will not be evenly distributed, causing some partitions to become hot and others underutilized. This violates AWS best practices for DynamoDB, which emphasize the importance of having a high cardinality hash key to ensure efficient throughput distribution.""}"
"An application stores payroll information nightly in DynamoDB for a large number of employees across hundreds of offices. Item attributes consist of individual name, office identifier, and cumulative daily hours. Managers run reports for ranges of names working in their office. One query is: `Return all Items in this office for names starting with A through E`. Which table configuration will result in the lowest impact on provisioned throughput for this query?","[""Configure the table to have a hash index on the name attribute, and a range index on the office identifier."", ""Configure the table to have a range index on the name attribute, and a hash index on the office identifier."", ""Configure a hash index on the name attribute and no range index."", ""Configure a hash index on the office Identifier attribute and no range index.""]","[""Configure the table to have a range index on the name attribute, and a hash index on the office identifier.""]","```json
{
    ""Configure the table to have a hash index on the name attribute, and a range index on the office identifier."": ""This configuration is incorrect because it does not align with the query requirements. The query needs to filter by office identifier and then by a range of names. Using the name as the hash key would not allow efficient querying by office identifier first, which is essential for reducing the impact on provisioned throughput."",
    ""Configure the table to have a range index on the name attribute, and a hash index on the office identifier."": ""This configuration is correct because it uses the office identifier as the hash key, allowing efficient partitioning of data by office. The range key on the name attribute enables querying for a range of names (A through E) within a specific office. This setup minimizes the impact on provisioned throughput by ensuring that queries are targeted and efficient, as they only scan the necessary partitions."",
    ""Configure a hash index on the name attribute and no range index."": ""This configuration is incorrect because it does not support the query requirements effectively. Without a range index, it is not possible to efficiently query for a range of names within a specific office. The lack of a range index would result in scanning more data than necessary, increasing the impact on provisioned throughput."",
    ""Configure a hash index on the office Identifier attribute and no range index."": ""This configuration is incorrect because, while it allows partitioning by office identifier, it does not support querying for a range of names efficiently. Without a range index on the name attribute, the query would require scanning all items within an office, leading to higher throughput consumption.""
}
```"
What is one key difference between an Amazon EBS-backed and an instance-store backed instance?,"[""Virtual Private Cloud requires EBS backed instances."", ""Amazon EBS-backed instances can be stopped and restarted."", ""Auto scaling requires using Amazon EBS-backed instances."", ""Instance-store backed instances can be stopped and restarted.""]","[""Amazon EBS-backed instances can be stopped and restarted.""]","```json
{
    ""Virtual Private Cloud requires EBS backed instances."": ""This statement is incorrect. Virtual Private Cloud (VPC) is a networking service that allows you to launch AWS resources in a logically isolated virtual network. It does not require instances to be EBS-backed. Both EBS-backed and instance-store backed instances can be launched in a VPC."",
    ""Amazon EBS-backed instances can be stopped and restarted."": ""This statement is correct. One of the key advantages of Amazon EBS-backed instances is that they can be stopped and restarted without losing data stored on the EBS volume. This is because the root volume is stored on EBS, which is persistent storage. This feature is useful for maintenance tasks and cost savings when instances are not needed."",
    ""Auto scaling requires using Amazon EBS-backed instances."": ""This statement is incorrect. Auto Scaling can be used with both EBS-backed and instance-store backed instances. However, EBS-backed instances are often preferred due to their ability to be stopped and restarted, which provides more flexibility in scaling operations."",
    ""Instance-store backed instances can be stopped and restarted."": ""This statement is incorrect. Instance-store backed instances cannot be stopped and restarted. If an instance-store backed instance is stopped, the data on the instance store is lost. This is because the root volume is ephemeral and tied to the lifecycle of the instance.""
}
```"
Which of the following services are included at no additional cost with the use of the AWS platform?,"[""Simple Storage Service."", ""Elastic Compute Cloud."", ""Auto Scaling."", ""Elastic Load Balancing."", ""CloudFormation."", ""Simple Workflow Service.""]","[""Auto Scaling."", ""CloudFormation.""]","```json
{
    ""Simple Storage Service."": ""Amazon Simple Storage Service (S3) is a storage service that charges based on the amount of data stored, data transfer, and requests made. It is not included at no additional cost, as costs vary depending on usage. Therefore, this choice is incorrect."",
    ""Elastic Compute Cloud."": ""Amazon Elastic Compute Cloud (EC2) provides scalable computing capacity in the cloud and is charged based on the instance type, usage time, and other factors like data transfer. It is not included at no additional cost, making this choice incorrect."",
    ""Auto Scaling."": ""Auto Scaling is a service that automatically adjusts the number of EC2 instances in response to demand. While the service itself does not incur additional charges, the resources it manages (like EC2 instances) do incur costs. However, the management and scaling features are provided at no additional cost, which is why this choice is correct."",
    ""Elastic Load Balancing."": ""Elastic Load Balancing (ELB) automatically distributes incoming application traffic across multiple targets, such as EC2 instances. It incurs charges based on the number of load balancers and the amount of data processed. Therefore, this service is not included at no additional cost, making this choice incorrect."",
    ""CloudFormation."": ""AWS CloudFormation is a service that helps model and set up AWS resources so that you can spend less time managing those resources and more time focusing on your applications. CloudFormation itself does not incur additional charges, as you only pay for the resources created using the templates. This makes this choice correct."",
    ""Simple Workflow Service."": ""Amazon Simple Workflow Service (SWF) is a web service that makes it easy to coordinate work across distributed application components. It incurs charges based on the number of workflow executions, tasks, and other factors. Therefore, it is not included at no additional cost, making this choice incorrect.""
}
```"
Your application is trying to upload a 6 GB file to Simple Storage Service and receive a `Your proposed upload exceeds the maximum allowed object size.` error message. What is a possible solution for this?,"[""None, Simple Storage Service objects are limited to 5 GB."", ""Use the multi-part upload API for this object."", ""Use the large object upload API for this object."", ""Contact support to increase your object size limit."", ""Upload to a different region.""]","[""Use the multi-part upload API for this object.""]","```json
{
    ""None, Simple Storage Service objects are limited to 5 GB."": ""This choice is incorrect. While a single PUT operation to S3 is limited to 5 GB, S3 itself supports objects up to 5 TB in size. The error message indicates that the upload method used is not suitable for large files, not that S3 cannot store them."",
    ""Use the multi-part upload API for this object."": ""This choice is correct. AWS S3 supports multi-part uploads for objects larger than 5 GB. This method allows you to upload a single large object as a set of parts, which can be uploaded independently and in parallel, making it suitable for large files. This is a best practice for handling large uploads in S3."",
    ""Use the large object upload API for this object."": ""This choice is incorrect because there is no specific 'large object upload API' in AWS S3. The correct approach for large files is to use the multi-part upload API."",
    ""Contact support to increase your object size limit."": ""This choice is incorrect. The limitation is not due to a configurable account limit but rather the method of upload. S3 inherently supports objects up to 5 TB without needing support intervention, provided the correct upload method is used."",
    ""Upload to a different region."": ""This choice is incorrect. The issue is not related to the region but to the method of uploading the file. All AWS regions support the same object size limits for S3, so changing the region will not resolve the error.""
}
```"
What AWS products and features can be deployed by Elastic Beanstalk? (Choose THREE),"[""Auto scaling groups."", ""Route 53 hosted zones."", ""Elastic Load Balancers."", ""RDS Instances."", ""Elastic IP addresses."", ""SQS Queues.""]","[""Auto scaling groups."", ""Elastic Load Balancers."", ""RDS Instances.""]","```json
{
    ""Auto scaling groups."": ""Elastic Beanstalk automatically creates and manages auto scaling groups to handle the scaling of the application environment. This is a key feature of Elastic Beanstalk, allowing applications to automatically adjust capacity to maintain steady, predictable performance at the lowest possible cost."",
    ""Route 53 hosted zones."": ""Elastic Beanstalk does not directly manage Route 53 hosted zones. While you can use Route 53 with Elastic Beanstalk for DNS management, it is not a service that Elastic Beanstalk deploys or manages directly. Route 53 is a separate service that must be configured independently."",
    ""Elastic Load Balancers."": ""Elastic Beanstalk can deploy and manage Elastic Load Balancers as part of the environment configuration. This is crucial for distributing incoming application traffic across multiple targets, such as EC2 instances, to ensure high availability and fault tolerance."",
    ""RDS Instances."": ""Elastic Beanstalk can provision and manage Amazon RDS instances as part of the environment setup. This allows developers to easily integrate a managed database with their application, simplifying the process of setting up a database backend."",
    ""Elastic IP addresses."": ""Elastic Beanstalk does not directly manage Elastic IP addresses. While Elastic IPs can be associated with EC2 instances, Elastic Beanstalk manages the underlying infrastructure and does not expose direct control over Elastic IPs to the user."",
    ""SQS Queues."": ""Elastic Beanstalk does not directly deploy or manage SQS queues. While applications running on Elastic Beanstalk can use SQS for messaging, the setup and management of SQS queues are handled separately from Elastic Beanstalk.""
}
```"
Games-R-Us is launching a new game app for mobile devices. Users will log into the game using their existing Facebook account and the game will record player data and scoring information directly to a DynamoDB table. What is the most secure approach for signing requests to the DynamoDB API?,"[""Create an IAM user with access credentials that are distributed with the mobile app to sign the requests."", ""Distribute the AWS root account access credentials with the mobile app to sign the requests."", ""Request temporary security credentials using web identity federation to sign the requests."", ""Establish cross account access between the mobile app and the DynamoDB table to sign the requests""]","[""Request temporary security credentials using web identity federation to sign the requests.""]","```json
{
    ""Create an IAM user with access credentials that are distributed with the mobile app to sign the requests."": ""This approach is incorrect because distributing IAM user credentials directly with a mobile app is a security risk. If the app is decompiled or intercepted, the credentials could be exposed, leading to unauthorized access to AWS resources. AWS best practices recommend against embedding long-term credentials in client-side applications."",
    ""Distribute the AWS root account access credentials with the mobile app to sign the requests."": ""This is highly insecure and incorrect. The AWS root account has full access to all resources and should never be used for day-to-day operations, especially not distributed in a client application. If compromised, it could lead to a complete takeover of the AWS account. AWS best practices strongly advise against using root credentials for anything other than initial account setup."",
    ""Request temporary security credentials using web identity federation to sign the requests."": ""This is the correct approach. By using web identity federation, the app can authenticate users through a third-party identity provider like Facebook, and then assume an IAM role that provides temporary security credentials. This minimizes the risk of credential exposure and follows AWS best practices for securely accessing AWS resources from mobile apps. Temporary credentials are short-lived and can be scoped with specific permissions, enhancing security."",
    ""Establish cross account access between the mobile app and the DynamoDB table to sign the requests."": ""This option is incorrect as it misunderstands the concept of cross-account access, which is used to allow resources in one AWS account to access resources in another account. It is not applicable for mobile app authentication and authorization. The correct approach for mobile apps is to use temporary credentials via identity federation or Amazon Cognito.""
}
```"
Which of the following programming languages have an officially supported AWS SDK? (Choose TWO),"[""Perl."", ""PHP."", ""Pascal."", ""Java."", ""SQL.""]","[""PHP."", ""Java.""]","```json
{
    ""Perl."": ""Perl does not have an officially supported AWS SDK. AWS provides SDKs for several programming languages to facilitate integration with AWS services, but Perl is not among them. Developers using Perl would need to rely on third-party libraries or custom implementations to interact with AWS services."",
    ""PHP."": ""PHP has an officially supported AWS SDK known as the AWS SDK for PHP. This SDK allows developers to easily integrate PHP applications with AWS services such as S3, EC2, and DynamoDB. The SDK provides a set of libraries that simplify the process of making API requests to AWS services, handling authentication, and managing responses. This is particularly useful for web developers who often use PHP for server-side scripting."",
    ""Pascal."": ""Pascal does not have an officially supported AWS SDK. AWS focuses on providing SDKs for languages that are widely used in modern application development, and Pascal is not commonly used in this context. Developers using Pascal would need to create their own solutions to interact with AWS services."",
    ""Java."": ""Java has an officially supported AWS SDK known as the AWS SDK for Java. This SDK is widely used in enterprise environments where Java is a popular choice for building scalable and robust applications. The SDK provides comprehensive support for AWS services, enabling developers to easily integrate Java applications with AWS. It includes features such as asynchronous programming, automatic retries, and request signing, which are essential for building reliable cloud-based applications."",
    ""SQL."": ""SQL is not a programming language that has an AWS SDK. SQL is primarily used for querying databases rather than building applications. While AWS offers services like Amazon RDS and Amazon Redshift that support SQL queries, there is no SDK for SQL itself. Developers typically use SDKs in conjunction with programming languages to interact with AWS services, rather than using SQL directly.""
}
```"
"A meteorological system monitors 600 temperature gauges, obtaining temperature samples every minute and saving each sample to a DynamoDB table Each sample involves writing 1K of data and the writes are evenly distributed over time. How much write throughput is required for the target table?","[""1 write capacity unit."", ""10 write capacity units."", ""60 write capacity units."", ""600 write capacity units."", ""3600 write capacity units.""]","[""10 write capacity units.""]","```json
{
    ""1 write capacity unit."": ""This choice is incorrect because 1 write capacity unit (WCU) in DynamoDB allows you to write 1KB of data per second. Since the system writes 600 samples per minute, which translates to 10 writes per second (600/60), 1 WCU is insufficient to handle this throughput."",
    ""10 write capacity units."": ""This choice is correct because the system writes 600 samples per minute, which translates to 10 writes per second (600/60). Each write is 1KB, and 1 WCU supports 1KB per second. Therefore, 10 WCUs are required to handle 10 writes per second. This aligns with AWS best practices for provisioning DynamoDB throughput."",
    ""60 write capacity units."": ""This choice is incorrect because it overestimates the required throughput. The system only needs to handle 10 writes per second, not 60. Therefore, 60 WCUs would be excessive and not cost-effective."",
    ""600 write capacity units."": ""This choice is incorrect because it significantly overestimates the required throughput. The system needs to handle 10 writes per second, not 600. Provisioning 600 WCUs would lead to unnecessary costs."",
    ""3600 write capacity units."": ""This choice is incorrect because it grossly overestimates the required throughput. The system only needs to handle 10 writes per second. Provisioning 3600 WCUs would be extremely inefficient and costly.""
}
```"
"In DynamoDB, what type of HTTP response codes indicate that a problem was found with the client request sent to the service?","[""5xx HTTP response code."", ""200 HTTP response code."", ""306 HTTP response code."", ""4xx HTTP response code.""]","[""4xx HTTP response code.""]","{""5xx HTTP response code."": ""5xx HTTP response codes indicate server-side errors. These errors mean that the server failed to fulfill a valid request due to an error on the server side. They do not indicate a problem with the client request itself, which is why this choice is incorrect for identifying client request issues."", ""200 HTTP response code."": ""A 200 HTTP response code indicates a successful request. This means the request was received, understood, and processed successfully by the server. It does not indicate any problems with the client request, which is why this choice is incorrect."", ""306 HTTP response code."": ""306 HTTP response code is a deprecated status code that was used in HTTP/1.1 for 'Switch Proxy'. It is not used in modern HTTP communications and does not relate to client request issues in DynamoDB or any other AWS service, making this choice incorrect."", ""4xx HTTP response code."": ""4xx HTTP response codes indicate client-side errors. These errors occur when the request sent by the client is incorrect or cannot be processed by the server. Common examples include 400 (Bad Request) and 404 (Not Found). In AWS DynamoDB, a 4xx error would indicate issues such as invalid parameters or unauthorized access, making this the correct choice for identifying client request problems.""}"
"Company B provides an online image recognition service and utilizes SQS to decouple system components for scalability The SQS consumers poll the imaging queue as often as possible to keep end-to-end throughput as high as possible. However, Company B is realizing that polling in tight loops is burning CPU cycles and increasing costs with empty responses. How can Company B reduce the number of empty responses?","[""Set the imaging queue visibility Timeout attribute to 20 seconds."", ""Set the Imaging queue ReceiveMessageWaitTimeSeconds attribute to 20 seconds."", ""Set the imaging queue MessageRetentionPeriod attribute to 20 seconds."", ""Set the DelaySeconds parameter of a message to 20 seconds.""]","[""Set the Imaging queue ReceiveMessageWaitTimeSeconds attribute to 20 seconds.""]","```json
{
    ""Set the imaging queue visibility Timeout attribute to 20 seconds."": ""This is incorrect. The visibility timeout determines how long a message is invisible to other consumers after being read from the queue. It does not affect the number of empty responses or the polling behavior of the consumers."",
    ""Set the Imaging queue ReceiveMessageWaitTimeSeconds attribute to 20 seconds."": ""This is correct. Setting the ReceiveMessageWaitTimeSeconds attribute to a non-zero value enables long polling, which reduces the number of empty responses by allowing the poll to wait for up to the specified time for a message to arrive. This reduces CPU usage and costs associated with empty responses."",
    ""Set the imaging queue MessageRetentionPeriod attribute to 20 seconds."": ""This is incorrect. The MessageRetentionPeriod attribute specifies how long a message is retained in the queue before being deleted. It does not affect polling behavior or the number of empty responses."",
    ""Set the DelaySeconds parameter of a message to 20 seconds."": ""This is incorrect. The DelaySeconds parameter delays the delivery of individual messages, not the polling behavior of the queue. It does not reduce the number of empty responses.""
}
```"
"An Amazon S3 bucket, `myawsbucket` is configured with website hosting in Tokyo region, what is the region-specific website endpoint?","[""`www.myawsbucket.ap-northeast-1.amazonaws.com`."", ""`myawsbucket.s3-website-ap-northeast-1.amazonaws.com`."", ""`myawsbucket.amazonaws.com`."", ""`myawsbucket.tokyo.amazonaws.com`.""]","[""`myawsbucket.s3-website-ap-northeast-1.amazonaws.com`.""]","```json
{
    ""`www.myawsbucket.ap-northeast-1.amazonaws.com`."": ""This choice is incorrect because it does not follow the standard format for Amazon S3 website endpoints. The 'www' prefix is not used in S3 website endpoints, and the correct format includes 's3-website' to indicate website hosting."",
    ""`myawsbucket.s3-website-ap-northeast-1.amazonaws.com`."": ""This choice is correct because it follows the standard format for Amazon S3 website endpoints. When an S3 bucket is configured for static website hosting, the endpoint is in the format 'bucket-name.s3-website-region.amazonaws.com'. In this case, 'ap-northeast-1' is the region code for Tokyo."",
    ""`myawsbucket.amazonaws.com`."": ""This choice is incorrect because it does not specify the region or the 's3-website' prefix required for static website hosting endpoints. It resembles the general S3 bucket URL format, which is used for direct object access, not website hosting."",
    ""`myawsbucket.tokyo.amazonaws.com`."": ""This choice is incorrect because it uses 'tokyo' instead of the correct region code 'ap-northeast-1'. AWS uses region codes rather than city names in endpoint URLs. Additionally, it lacks the 's3-website' prefix necessary for website hosting.""
}
```"
"You are inserting 1000 new items every second in a DynamoDB table. Once an hour these items are analyzed and then are no longer needed. You need to minimize provisioned throughput, storage, and API calls. Given these requirements, what is the most efficient way to manage these Items after the analysis?","[""Retain the items in a single table."", ""Delete items individually over a 24 hour period."", ""Delete the table and create a new table per hour."", ""Create a new table per hour.""]","[""Delete the table and create a new table per hour.""]","{""Retain the items in a single table."": ""This option is incorrect because retaining items in a single table would result in unnecessary storage costs and increased provisioned throughput as the table size grows. Since the items are only needed for an hour, keeping them longer than necessary is inefficient and does not align with the requirement to minimize storage and API calls."", ""Delete items individually over a 24 hour period."": ""This option is incorrect because deleting items individually incurs additional API calls, which can increase costs and does not efficiently minimize provisioned throughput. Since the items are no longer needed after analysis, deleting them individually over a 24-hour period is not an optimal solution."", ""Delete the table and create a new table per hour."": ""This option is correct because deleting the table after the items are analyzed and creating a new table per hour minimizes storage costs and API calls. It efficiently manages throughput by ensuring that only necessary data is stored and processed. This approach aligns with AWS best practices for managing temporary data and optimizing resource usage."", ""Create a new table per hour."": ""This option is incorrect because it suggests creating a new table without deleting the old one, which would lead to increased storage costs and provisioned throughput. The requirement is to minimize these aspects, and without deleting the old table, storage will continue to grow unnecessarily.""}"
"You have written an application that uses the Elastic Load Balancing service to spread traffic to several web servers. Your users complain that they are sometimes forced to login again in the middle of using your application, after they have already logged in. This is not behavior you have designed. What is a possible solution to prevent this happening?","[""Use instance memory to save session state."", ""Use instance storage to save session state."", ""Use EBS to save session state."", ""Use ElastiCache to save session state."", ""Use Glacier to save session slate.""]","[""Use ElastiCache to save session state.""]","```json
{
    ""Use instance memory to save session state."": ""This choice is incorrect because using instance memory to save session state is not reliable in a distributed environment. If the instance fails or is terminated, the session state will be lost, causing users to be logged out. AWS best practices recommend using a centralized store for session state that is accessible by all instances."",
    ""Use instance storage to save session state."": ""This choice is incorrect because instance storage is ephemeral and tied to the lifecycle of the instance. If the instance is stopped or terminated, the data will be lost, leading to session state loss. This is not suitable for maintaining session state in a load-balanced environment."",
    ""Use EBS to save session state."": ""This choice is incorrect because while EBS volumes provide persistent storage, they are attached to a single instance at a time. This does not support the distributed nature of a load-balanced application where multiple instances need to access the same session data."",
    ""Use ElastiCache to save session state."": ""This choice is correct because ElastiCache provides a distributed, in-memory data store that can be accessed by all instances behind a load balancer. It supports caching session data in a highly available and scalable manner, which is ideal for maintaining session state across multiple web servers."",
    ""Use Glacier to save session slate."": ""This choice is incorrect because Glacier is designed for long-term archival storage and is not suitable for session state management. It has high latency and is not designed for frequent read/write operations required for session management.""
}
```"
"You run an ad-supported photo sharing website using S3 to serve photos to visitors of your site. At some point you find out that other sites have been linking to the photos on your site, causing loss to your business. What is an effective method to mitigate this?","[""Store photos on an EBS volume of the web server."", ""Remove public read access and use signed URLs with expiry dates."", ""Use CloudFront distributions for static content."", ""Block the IPs of the offending websites in Security Groups.""]","[""Remove public read access and use signed URLs with expiry dates.""]","```json
{
    ""Store photos on an EBS volume of the web server."": ""This choice is incorrect because storing photos on an EBS volume would not prevent other sites from linking to the photos. EBS volumes are block storage devices that are attached to EC2 instances and are not designed for serving static content directly over the internet. Additionally, using EBS for serving static content is not cost-effective or scalable compared to S3."",
    ""Remove public read access and use signed URLs with expiry dates."": ""This choice is correct because removing public read access ensures that only authorized users can access the photos. Using signed URLs with expiry dates allows you to generate temporary URLs that grant time-limited access to the photos, preventing unauthorized hotlinking. This approach leverages S3's security features and is a best practice for controlling access to content."",
    ""Use CloudFront distributions for static content."": ""While using CloudFront can help with caching and reducing latency, it does not inherently prevent other sites from linking to your photos. To prevent hotlinking, you would still need to implement signed URLs or signed cookies with CloudFront. Therefore, this option alone does not address the problem of unauthorized access."",
    ""Block the IPs of the offending websites in Security Groups."": ""This choice is incorrect because Security Groups are used to control inbound and outbound traffic to and from EC2 instances, not S3 buckets. Blocking IPs at the Security Group level would not prevent access to S3-hosted content. Additionally, blocking IPs is not a scalable or effective solution for preventing hotlinking, as IP addresses can change.""
}
```"
Which statements about DynamoDB are true? (Choose TWO),"[""DynamoDB uses a pessimistic locking model."", ""DynamoDB uses optimistic concurrency control."", ""DynamoDB uses conditional writes for consistency."", ""DynamoDB restricts item access during reads."", ""DynamoDB restricts item access during writes.""]","[""DynamoDB uses optimistic concurrency control."", ""DynamoDB uses conditional writes for consistency.""]","```json
{
    ""DynamoDB uses a pessimistic locking model."": ""This statement is incorrect. DynamoDB does not use a pessimistic locking model. Instead, it uses an optimistic concurrency control model, which allows multiple transactions to proceed without locking resources, assuming that conflicts will be rare. This approach is more suitable for distributed systems and high-availability applications, which is why DynamoDB adopts it."",
    ""DynamoDB uses optimistic concurrency control."": ""This statement is correct. DynamoDB uses optimistic concurrency control, which is a method of handling concurrent updates to a database. It allows multiple transactions to proceed without locking resources, and conflicts are resolved by checking if the data has changed before committing a transaction. This is efficient for high-throughput applications and aligns with AWS's goal of providing scalable and highly available services."",
    ""DynamoDB uses conditional writes for consistency."": ""This statement is correct. DynamoDB uses conditional writes to ensure consistency. Conditional writes allow you to specify conditions that must be met for a write operation to succeed, which helps prevent overwriting data that has been modified by another transaction. This feature is crucial for maintaining data integrity in distributed systems."",
    ""DynamoDB restricts item access during reads."": ""This statement is incorrect. DynamoDB does not restrict item access during reads. It allows for eventual consistency or strong consistency models, but it does not lock items during read operations. This design choice ensures high availability and low latency for read operations."",
    ""DynamoDB restricts item access during writes."": ""This statement is incorrect. While DynamoDB uses conditional writes to manage concurrent updates, it does not restrict access to items during write operations. Instead, it uses optimistic concurrency control to handle potential conflicts, allowing for high throughput and availability.""
}
```"
You are providing AWS consulting services for a company developing a new mobile application that will be leveraging Amazon SNS Mobile Push for push notifications. In order to send direct notification messages to individual devices each device registration identifier or token needs to be registered with SNS; however the developers are not sure of the best way to do this. You advise them to:,"[""Bulk upload the device tokens contained in a CSV file via the AWS Management Console."", ""Let the push notification service (e.g. Amazon Device Messaging) handle the registration."", ""Implement a token vending service to handle the registration."", ""Call the `CreatePlatformEndPoint` API function to register multiple device tokens.""]","[""Call the `CreatePlatformEndPoint` API function to register multiple device tokens.""]","```json
{
    ""Bulk upload the device tokens contained in a CSV file via the AWS Management Console."": ""This choice is incorrect because AWS SNS does not support bulk uploading of device tokens via the AWS Management Console. Each device token must be registered individually using the API to create a platform endpoint. Bulk operations for device registration are not supported in this manner."",
    ""Let the push notification service (e.g. Amazon Device Messaging) handle the registration."": ""This choice is incorrect because while services like Amazon Device Messaging can handle certain aspects of push notifications, the registration of device tokens with Amazon SNS must be explicitly handled by the application developer. SNS requires the use of the CreatePlatformEndpoint API to register each device token."",
    ""Implement a token vending service to handle the registration."": ""This choice is incorrect in this context because a token vending service is typically used for distributing temporary credentials to clients, not for registering device tokens with SNS. The registration of device tokens with SNS requires direct interaction with the SNS API."",
    ""Call the `CreatePlatformEndPoint` API function to register multiple device tokens."": ""This choice is correct because the CreatePlatformEndpoint API function is specifically designed for registering individual device tokens with Amazon SNS. This is the recommended approach for registering devices to receive push notifications. Each device token must be registered as a platform endpoint in SNS, which is a best practice for managing push notifications.""
}
```"
"You are writing to a DynamoDB table and receive the following exception: `ProvisionedThroughputExceededException`. though according to your Cloudwatch metrics for the table, you are not exceeding your provisioned throughput. What could be an explanation for this?","[""You haven't provisioned enough DynamoDB storage instances."", ""You're exceeding your capacity on a particular Range Key."", ""You're exceeding your capacity on a particular Hash Key."", ""You're exceeding your capacity on a particular Sort Key."", ""You haven't configured DynamoDB Auto Scaling triggers.""]","[""You're exceeding your capacity on a particular Hash Key.""]","```json
{
    ""You haven't provisioned enough DynamoDB storage instances."": ""This is incorrect because DynamoDB is a fully managed service and does not require provisioning of storage instances. Storage is automatically managed by AWS, and the exception in question is related to throughput, not storage capacity."",
    ""You're exceeding your capacity on a particular Range Key."": ""This is incorrect because DynamoDB does not limit throughput based on Range Key (or Sort Key) usage. Throughput limits are applied to partitions, which are determined by the Hash Key. The Range Key is used for sorting within a partition but does not affect throughput capacity directly."",
    ""You're exceeding your capacity on a particular Hash Key."": ""This is correct because DynamoDB partitions data based on the Hash Key. If a particular Hash Key is accessed more frequently than others, it can lead to hot partitions, which exceed the provisioned throughput for that specific partition, resulting in a ProvisionedThroughputExceededException. This is a common scenario when data is not evenly distributed across Hash Keys."",
    ""You're exceeding your capacity on a particular Sort Key."": ""This is incorrect because, similar to the Range Key, the Sort Key is used for ordering data within a partition and does not directly affect throughput capacity. Throughput issues are related to the distribution of requests across Hash Keys."",
    ""You haven't configured DynamoDB Auto Scaling triggers."": ""This is incorrect because while Auto Scaling can help manage throughput by automatically adjusting capacity, the specific exception mentioned is related to exceeding provisioned throughput on a specific Hash Key. Auto Scaling would not prevent hot partitions if the data is not evenly distributed across Hash Keys.""
}
```"
"If an application is storing hourly log files from thousands of instances from a high traffic web site, which naming scheme would give optimal performance on S3?","[""Sequential."", ""instancelD_log-HH-DD-MM-YYYY."", ""instancelDJog-YYYY-MM-DD-HH."", ""HH-DD-MM-YYYY-log_instancelD."", ""YYYY-MM-DD-HH-logJnstancelD.""]","[""HH-DD-MM-YYYY-log_instancelD.""]","```json
{
    ""Sequential."": ""Using a sequential naming scheme can lead to performance bottlenecks in Amazon S3. This is because S3 partitions data based on the key name, and sequential keys can cause 'hot spots' where a small number of partitions are overwhelmed with requests, leading to throttling and reduced performance. AWS recommends using a naming scheme that distributes keys more evenly across partitions."",
    ""instancelD_log-HH-DD-MM-YYYY."": ""This naming scheme starts with the instance ID, which can lead to uneven distribution of keys across S3 partitions if there are a large number of logs from a few instances. This can cause some partitions to become 'hot' while others remain underutilized, leading to suboptimal performance."",
    ""instancelDJog-YYYY-MM-DD-HH."": ""Similar to the previous option, starting with the instance ID can result in uneven distribution of keys. While the date and time are included, the initial part of the key is not varied enough to ensure even distribution across S3 partitions, which can lead to performance issues."",
    ""HH-DD-MM-YYYY-log_instancelD."": ""This naming scheme is optimal because it starts with the hour, which varies frequently and ensures that keys are distributed more evenly across S3 partitions. By varying the prefix, it helps avoid 'hot spots' and ensures better performance. This aligns with AWS best practices for S3 key naming to optimize performance."",
    ""YYYY-MM-DD-HH-logJnstancelD."": ""While this naming scheme includes the date and time, starting with the year can still lead to uneven distribution of keys, especially if there are many logs generated within the same year. Although better than some other options, it is not as effective as starting with a more frequently changing component like the hour.""
}
```"
Which of the following statements about SQS is true?,"[""Messages will be delivered exactly once and messages will be delivered in First in, First out order."", ""Messages will be delivered exactly once and message delivery order is indeterminate."", ""Messages will be delivered one or more times and messages will be delivered in First in, First out order."", ""Messages will be delivered one or more times and message delivery order is indeterminate.""]","[""Messages will be delivered one or more times and message delivery order is indeterminate.""]","{""Messages will be delivered exactly once and messages will be delivered in First in, First out order."": ""This statement is incorrect because it describes the behavior of Amazon SQS FIFO queues, not the standard queues. FIFO queues ensure exactly-once processing and preserve the order of messages, but standard queues do not guarantee exactly-once delivery or message order."", ""Messages will be delivered exactly once and message delivery order is indeterminate."": ""This statement is incorrect because it suggests exactly-once delivery, which is not guaranteed by Amazon SQS standard queues. Standard queues offer at-least-once delivery, meaning messages can be delivered more than once. The order of messages is also indeterminate in standard queues."", ""Messages will be delivered one or more times and messages will be delivered in First in, First out order."": ""This statement is incorrect because it combines characteristics of both SQS standard and FIFO queues. While standard queues may deliver messages one or more times, they do not guarantee FIFO order. FIFO queues, on the other hand, do provide FIFO order but ensure exactly-once delivery."", ""Messages will be delivered one or more times and message delivery order is indeterminate."": ""This statement is correct for Amazon SQS standard queues. Standard queues guarantee at-least-once delivery, meaning messages can be delivered more than once, and they do not guarantee the order of message delivery. This is suitable for applications where message order is not critical and duplicates can be handled, such as processing log data or batch operations.""}"
"A corporate web application is deployed within an Amazon VPC, and is connected to the corporate data center via IPSec VPN. The application must authenticate against the on-premise LDAP server. Once authenticated, logged-in users can only access an S3 keyspace specific to the user. Which two approaches can satisfy the objectives? (Choose TWO)","[""The application authenticates against LDAP. The application then calls the IAM Security Service to login to IAM using the LDAP credentials. The application can use the 1AM temporary credentials to access the appropriate S3 bucket."", ""The application authenticates against LDAP, and retrieves the name of an IAM role associated with the user. The application then calls the IAM Security Token Service to assume that IAM Role. The application can use the temporary credentials to access the appropriate S3 bucket."", ""The application authenticates against IAM Security Token Service using the LDAP credentials. The application uses those temporary AWS security credentials to access the appropriate S3 bucket."", ""Develop an identity broker which authenticates against LDAP, and then calls IAM Security Token Service to get IAM federated user credentials. The application calls the identity broker to get IAM federated user credentials with access to the appropriate S3 bucket."", ""Develop an identity broker which authenticates against IAM Security Token Service to assume an IAM Role to get temporary AWS security credentials. The application calls the identity broker to get AWS temporary security credentials with access to the appropriate S3 bucket.""]","[""The application authenticates against LDAP, and retrieves the name of an IAM role associated with the user. The application then calls the IAM Security Token Service to assume that IAM Role. The application can use the temporary credentials to access the appropriate S3 bucket."", ""Develop an identity broker which authenticates against LDAP, and then calls IAM Security Token Service to get IAM federated user credentials. The application calls the identity broker to get IAM federated user credentials with access to the appropriate S3 bucket.""]","```json
{
    ""The application authenticates against LDAP. The application then calls the IAM Security Service to login to IAM using the LDAP credentials. The application can use the 1AM temporary credentials to access the appropriate S3 bucket."": ""This choice is incorrect because IAM does not support direct authentication using LDAP credentials. AWS IAM does not provide a mechanism to directly log in using LDAP credentials; instead, you need to use an identity broker or similar mechanism to map LDAP users to IAM roles."",
    ""The application authenticates against LDAP, and retrieves the name of an IAM role associated with the user. The application then calls the IAM Security Token Service to assume that IAM Role. The application can use the temporary credentials to access the appropriate S3 bucket."": ""This choice is correct. This approach involves authenticating users against LDAP and then mapping them to specific IAM roles. The application uses the AWS Security Token Service (STS) to assume the IAM role, which provides temporary credentials to access AWS resources like S3. This is a common pattern for integrating on-premises identity systems with AWS."",
    ""The application authenticates against IAM Security Token Service using the LDAP credentials. The application uses those temporary AWS security credentials to access the appropriate S3 bucket."": ""This choice is incorrect because IAM Security Token Service (STS) does not support direct authentication using LDAP credentials. STS is used to obtain temporary security credentials for IAM roles, not for direct authentication with LDAP."",
    ""Develop an identity broker which authenticates against LDAP, and then calls IAM Security Token Service to get IAM federated user credentials. The application calls the identity broker to get IAM federated user credentials with access to the appropriate S3 bucket."": ""This choice is correct. An identity broker can authenticate users against LDAP and then use AWS STS to obtain federated user credentials. This is a recommended approach for integrating external identity providers with AWS, allowing users to access AWS resources based on their LDAP credentials."",
    ""Develop an identity broker which authenticates against IAM Security Token Service to assume an IAM Role to get temporary AWS security credentials. The application calls the identity broker to get AWS temporary security credentials with access to the appropriate S3 bucket."": ""This choice is incorrect because it suggests that the identity broker authenticates directly with STS, which is not possible. The identity broker should authenticate against the LDAP server and then use STS to assume an IAM role, not authenticate directly with STS.""
}
```"
"Company C is currently hosting their corporate site in an Amazon S3 bucket with Static Website Hosting enabled. Currently, when visitors go to `http://www.companyc.com` the `index.html` page is returned. Company C now would like a new page welcome.html to be returned when a visitor enters `http://www.companyc.com` in the browser. Which of the following steps will allow Company C to meet this requirement? (Choose TWO)","[""Upload an html page named welcome.html to their S3 bucket."", ""Create a welcome subfolder in their S3 bucket."", ""Set the Index Document property to welcome.html."", ""Move the `index.html` page to a welcome subfolder."", ""Set the Error Document property to welcome.html.""]","[""Upload an html page named welcome.html to their S3 bucket."", ""Set the Index Document property to welcome.html.""]","```json
{
    ""Upload an html page named welcome.html to their S3 bucket."": ""This choice is correct because for the S3 static website hosting to serve the 'welcome.html' page as the index document, the file must exist in the S3 bucket. Without uploading 'welcome.html', the bucket cannot serve this file as the index document."",
    ""Create a welcome subfolder in their S3 bucket."": ""This choice is incorrect because creating a subfolder does not affect which file is served as the index document. The index document setting in S3 static website hosting specifies which file to serve at the root level, and it does not involve creating subfolders."",
    ""Set the Index Document property to welcome.html."": ""This choice is correct because setting the Index Document property to 'welcome.html' in the S3 bucket's static website hosting configuration tells S3 to serve 'welcome.html' as the default page when a user accesses the root URL. This is a key configuration step to achieve the desired behavior."",
    ""Move the `index.html` page to a welcome subfolder."": ""This choice is incorrect because moving 'index.html' to a subfolder does not change the default index document setting. The S3 static website hosting configuration needs to be explicitly set to serve 'welcome.html' as the index document, regardless of the location of 'index.html'."",
    ""Set the Error Document property to welcome.html."": ""This choice is incorrect because the Error Document property is used to specify a custom error page, not the default index page. Setting 'welcome.html' as the Error Document would not change the default page served at the root URL.""
}
```"
What type of block cipher does Amazon S3 offer for server side encryption?,"[""Triple DES."", ""Advanced Encryption Standard."", ""Blowfish."", ""RC5.""]","[""Advanced Encryption Standard.""]","```json
{
    ""Triple DES."": ""Triple DES is an older encryption standard that is not used by Amazon S3 for server-side encryption. AWS has moved towards more secure and efficient encryption standards like AES due to its stronger security and performance characteristics. Triple DES is considered less secure compared to AES, which is why it is not used in AWS services for server-side encryption."",
    ""Advanced Encryption Standard."": ""Advanced Encryption Standard (AES) is the correct choice for Amazon S3 server-side encryption. AWS uses AES-256, which is a secure and efficient encryption standard, widely recognized for its strong security and performance. AES is the industry standard for encryption and is recommended by AWS for encrypting data at rest. This aligns with AWS best practices for data protection and compliance requirements."",
    ""Blowfish."": ""Blowfish is not used by Amazon S3 for server-side encryption. While Blowfish is a symmetric-key block cipher, it is not as widely adopted as AES for modern encryption needs. AWS prefers AES due to its established security, efficiency, and compliance with industry standards, making Blowfish an incorrect choice for this context."",
    ""RC5."": ""RC5 is another encryption algorithm that is not used by Amazon S3 for server-side encryption. Similar to Blowfish and Triple DES, RC5 is not as widely used or recommended as AES. AWS selects AES for its strong security properties and compliance with industry standards, which is why RC5 is not the correct answer.""
}
```"
"A Development team wants to instrument their code to provide more detailed information to AWS X-Ray than simple outgoing and incoming requests. This will generate large amounts of data, so the Development team wants to implement indexing so they can filter the data. What should the Development team do to achieve this?","[""Add annotations to the segment document and the code."", ""Add metadata to the segment document and the code."", ""Configure the necessary X-Ray environment variables."", ""Install required plugins for the appropriate AWS SDK.""]","[""Add annotations to the segment document and the code.""]","```json
{
    ""Add annotations to the segment document and the code."": ""This choice is correct because annotations in AWS X-Ray are indexed for use with filter expressions, allowing developers to filter traces based on specific criteria. Annotations are key-value pairs that are indexed, making them suitable for querying and filtering large amounts of trace data. This aligns with the requirement to implement indexing to filter the data."",
    ""Add metadata to the segment document and the code."": ""This choice is incorrect because metadata in AWS X-Ray is not indexed and is used for storing additional information that is not needed for filtering. Metadata is useful for adding context to traces but does not support the indexing requirement needed for filtering."",
    ""Configure the necessary X-Ray environment variables."": ""This choice is incorrect because configuring environment variables in AWS X-Ray is related to setting up the X-Ray daemon or SDK behavior, such as specifying the log level or enabling/disabling tracing. It does not address the need for indexing trace data for filtering purposes."",
    ""Install required plugins for the appropriate AWS SDK."": ""This choice is incorrect because installing plugins for AWS SDKs is related to enabling X-Ray tracing for specific AWS services or environments. While it is important for instrumenting applications, it does not directly address the requirement of indexing and filtering trace data.""
}
```"
A team of Developers must migrate an application running inside an AWS Elastic Beanstalk environment from a Classic Load Balancer to an Application Load Balancer. Which steps should be taken to accomplish the task using the AWS Management Console?,"[""1. Update the application code in the existing deployment. 2. Select a new load balancer type before running the deployment. 3. Deploy the new version of the application code to the environment."", ""1. Create a new environment with the same configurations except for the load balancer type. 2. Deploy the same application version as used in the original environment. 3. Run the `swap-environment-cnames` action."", ""1. Clone the existing environment, changing the associated load balancer type. 2. Deploy the same application version as used in the original environment. 3. Run the `swap-environment-cnames` action."", ""1. Edit the environment definitions in the existing deployment. 2. Change the associated load balancer type according to the requirements. 3. Rebuild the environment with the new load balancer type.""]","[""1. Update the application code in the existing deployment. 2. Select a new load balancer type before running the deployment. 3. Deploy the new version of the application code to the environment.""]","```json
{
    ""1. Update the application code in the existing deployment. 2. Select a new load balancer type before running the deployment. 3. Deploy the new version of the application code to the environment."": ""This choice is incorrect because AWS Elastic Beanstalk does not allow changing the load balancer type directly within an existing environment. The load balancer type is a fundamental part of the environment configuration and cannot be changed without creating a new environment."",
    ""1. Create a new environment with the same configurations except for the load balancer type. 2. Deploy the same application version as used in the original environment. 3. Run the `swap-environment-cnames` action."": ""This choice is correct. The best practice for changing the load balancer type in an AWS Elastic Beanstalk environment is to create a new environment with the desired load balancer type and deploy the application to this new environment. Once the new environment is verified to be working correctly, the `swap-environment-cnames` action is used to switch the CNAMEs, effectively redirecting traffic from the old environment to the new one. This approach minimizes downtime and ensures a smooth transition."",
    ""1. Clone the existing environment, changing the associated load balancer type. 2. Deploy the same application version as used in the original environment. 3. Run the `swap-environment-cnames` action."": ""This choice is incorrect because AWS Elastic Beanstalk does not provide a direct 'clone' feature that allows changing the load balancer type during the cloning process. The correct approach is to manually create a new environment with the desired configuration and then use the `swap-environment-cnames` action."",
    ""1. Edit the environment definitions in the existing deployment. 2. Change the associated load balancer type according to the requirements. 3. Rebuild the environment with the new load balancer type."": ""This choice is incorrect because you cannot change the load balancer type of an existing AWS Elastic Beanstalk environment by editing its definitions. The load balancer type is set at the time of environment creation and cannot be modified afterward. The correct method involves creating a new environment with the desired load balancer type.""
}
```"
A company needs a version control system for collaborative software development. Features of the system must include the following: Support for batches of changes across multiple files Parallel branching Version tracking Which AWS service will meet these requirements?,"[""AWS CodePipeline."", ""Amazon S3."", ""AWS Code Build."", ""AWS CodeCommit.""]","[""AWS CodeCommit.""]","```json
{
    ""AWS CodePipeline."": ""AWS CodePipeline is a continuous integration and continuous delivery (CI/CD) service for fast and reliable application and infrastructure updates. It is not a version control system. It orchestrates the build, test, and deploy phases of your release process. Therefore, it does not support version control features like parallel branching or version tracking, which are essential for collaborative software development."",
    ""Amazon S3."": ""Amazon S3 is an object storage service that offers scalability, data availability, security, and performance. It is not a version control system and does not provide features like support for batches of changes across multiple files, parallel branching, or version tracking. S3 is used for storing and retrieving any amount of data at any time, from anywhere on the web, but it is not designed for source code management."",
    ""AWS Code Build."": ""AWS CodeBuild is a fully managed continuous integration service that compiles source code, runs tests, and produces software packages that are ready to deploy. It is not a version control system and does not provide version control features such as parallel branching or version tracking. CodeBuild is used for building and testing code, not for managing source code versions."",
    ""AWS CodeCommit."": ""AWS CodeCommit is a fully managed source control service that makes it easy for teams to host secure and scalable Git repositories. It supports all standard Git features, including support for batches of changes across multiple files, parallel branching, and version tracking. CodeCommit is designed specifically for collaborative software development, making it the correct choice for a version control system as described in the question. It integrates with other AWS services and third-party tools, providing a seamless experience for developers.""
}
```"
A company is using continuous integration and continuous delivery systems. A Developer now needs to automate a software package deployment to both Amazon EC2 instances and virtual servers running on-premises. Which AWS service should be used to accomplish this?,"[""AWS CodePipeline."", ""AWS CodeBuild."", ""AWS Elastic Beanstalk."", ""AWS CodeDeploy.""]","[""AWS CodeDeploy.""]","{""AWS CodePipeline."": ""AWS CodePipeline is a continuous integration and continuous delivery (CI/CD) service for fast and reliable application and infrastructure updates. While it orchestrates the build, test, and deploy phases of your release process, it does not directly handle the deployment to EC2 instances or on-premises servers. Therefore, it is not the correct choice for automating software package deployment to these environments."", ""AWS CodeBuild."": ""AWS CodeBuild is a fully managed build service that compiles source code, runs tests, and produces software packages that are ready to deploy. It is primarily focused on the build phase of the CI/CD pipeline and does not handle the deployment of software packages to EC2 instances or on-premises servers, making it an incorrect choice for this scenario."", ""AWS Elastic Beanstalk."": ""AWS Elastic Beanstalk is an easy-to-use service for deploying and scaling web applications and services. It abstracts infrastructure management and is not designed for deploying software packages to both EC2 instances and on-premises servers. It is more suited for web applications and not for the specific deployment needs described in the question."", ""AWS CodeDeploy."": ""AWS CodeDeploy is a deployment service that automates application deployments to Amazon EC2 instances, on-premises servers, and Lambda functions. It is specifically designed for deploying software packages across a variety of environments, making it the correct choice for automating deployments to both EC2 instances and on-premises servers as described in the question. AWS CodeDeploy supports blue/green deployments, rolling updates, and integrates with other AWS services, which aligns with best practices for continuous delivery.""}"
A Developer created a new AWS account and must create a scalable AWS Lambda function that meets the following requirements for concurrent execution: Average execution time of 100 seconds 50 requests per second. Which step must be taken prior to deployment to prevent errors?,"[""Implement dead-letter queues to capture invocation errors."", ""Add an event source from Amazon API Gateway to the Lambda function."", ""Implement error handling within the application code."", ""Contact AWS Support to increase the concurrent execution limits.""]","[""Contact AWS Support to increase the concurrent execution limits.""]","```json
{
    ""Implement dead-letter queues to capture invocation errors."": ""This choice is incorrect because dead-letter queues (DLQs) are used to capture failed events for further analysis and troubleshooting, not to prevent errors related to concurrent execution limits. DLQs are useful for handling errors post-execution, but they do not address the issue of exceeding concurrent execution limits."",
    ""Add an event source from Amazon API Gateway to the Lambda function."": ""This choice is incorrect because adding an event source like Amazon API Gateway does not address the issue of concurrent execution limits. While API Gateway can trigger Lambda functions, it does not inherently solve the problem of scaling to meet high concurrency demands. The question specifically asks about preventing errors related to concurrent execution, which requires addressing the Lambda concurrency limits."",
    ""Implement error handling within the application code."": ""This choice is incorrect because while implementing error handling is a best practice for robust application development, it does not address the specific issue of concurrent execution limits. Error handling within the code can manage runtime exceptions but cannot prevent errors due to exceeding AWS Lambda's concurrency limits."",
    ""Contact AWS Support to increase the concurrent execution limits."": ""This choice is correct because AWS Lambda has a default concurrency limit per account. To handle 50 requests per second with an average execution time of 100 seconds, the developer needs a concurrency limit of at least 5000 (50 requests/second * 100 seconds). If the default limit is insufficient, contacting AWS Support to request an increase is necessary to prevent throttling errors due to concurrency limits.""
}
```"
"A Developer is building a three-tier web application that should be able to handle a minimum of 5000 requests per minute. Requirements state that the web tier should be completely stateless while the application maintains session state for the users. How can session data be externalized, keeping latency at the LOWEST possible value?","[""Create an Amazon RDS instance, then implement session handling at the application level to leverage a database inside the RDS database instance for session data storage."", ""Implement a shared file system solution across the underlying Amazon EC2 instances, then implement session handling at the application level to leverage the shared file system for session data storage."", ""Create an Amazon ElastiCache Memcached cluster, then implement session handling at the application level to leverage the cluster for session data storage."", ""Create an Amazon DynamoDB table, then implement session handling at the application level to leverage the table for session data storage.""]","[""Create an Amazon RDS instance, then implement session handling at the application level to leverage a database inside the RDS database instance for session data storage.""]","```json
{
    ""Create an Amazon RDS instance, then implement session handling at the application level to leverage a database inside the RDS database instance for session data storage."": ""This choice is incorrect because while Amazon RDS can be used to store session data, it is not the most optimal solution for low latency. RDS is a relational database service which may introduce higher latency compared to in-memory data stores. For session data which requires fast read/write operations, in-memory stores like ElastiCache are preferred."",
    ""Implement a shared file system solution across the underlying Amazon EC2 instances, then implement session handling at the application level to leverage the shared file system for session data storage."": ""This choice is incorrect because using a shared file system for session data storage can introduce significant latency and complexity. File systems are not optimized for the rapid read/write operations required for session data, and managing a shared file system across multiple EC2 instances can be cumbersome and error-prone."",
    ""Create an Amazon ElastiCache Memcached cluster, then implement session handling at the application level to leverage the cluster for session data storage."": ""This choice is correct because Amazon ElastiCache, particularly with Memcached, is an in-memory data store that provides low-latency access to session data. It is designed for high-performance, scalable caching, making it ideal for storing session data that needs to be accessed quickly. This aligns with AWS best practices for maintaining low latency in web applications."",
    ""Create an Amazon DynamoDB table, then implement session handling at the application level to leverage the table for session data storage."": ""This choice is also a valid option but not the best for lowest latency. DynamoDB is a highly scalable NoSQL database that offers low-latency access, but in-memory stores like ElastiCache typically provide even lower latency. DynamoDB is a good choice for session data if durability and scalability are more critical than the absolute lowest latency.""
}
```"
"An Amazon DynamoDB table uses a Global Secondary Index (GSI) to support read queries. The primary table is write-heavy, whereas the GSI is used for read operations. Looking at Amazon CloudWatch metrics, the Developer notices that write operations to the primary table are throttled frequently under heavy write activity. However, write capacity units to the primary table are available and not fully consumed. Why is the table being throttled?","[""The GSI write capacity units are underprovisioned."", ""There are not enough read capacity units on the primary table."", ""Amazon DynamoDB Streams is not enabled on the table."", ""A large write operation is being performed against another table.""]","[""The GSI write capacity units are underprovisioned.""]","{""The GSI write capacity units are underprovisioned."": ""This choice is correct. In DynamoDB, when a table is write-heavy and uses a GSI, the GSI must have sufficient write capacity units (WCUs) to handle the data being written to it. If the GSI is underprovisioned, it can cause throttling of write operations because every write to the primary table also requires a write to the GSI. This is a common scenario where developers need to ensure that the GSI's capacity is appropriately provisioned to match the write demands of the primary table. AWS documentation emphasizes the importance of monitoring and adjusting GSI capacity to prevent throttling."", ""There are not enough read capacity units on the primary table."": ""This choice is incorrect. The issue described in the question is related to write throttling, not read throttling. Read capacity units (RCUs) are relevant for read operations, and since the problem is with write operations, RCUs on the primary table would not affect this scenario. Additionally, the GSI is used for read operations, so the primary table's read capacity is not directly related to the problem."", ""Amazon DynamoDB Streams is not enabled on the table."": ""This choice is incorrect. DynamoDB Streams is a feature that captures changes to items in a table and can be used for various purposes like triggering AWS Lambda functions. However, it is not directly related to the issue of write throttling. The problem described is about write capacity and throttling, which is unrelated to whether Streams are enabled or not."", ""A large write operation is being performed against another table."": ""This choice is incorrect. The question specifically mentions throttling on the primary table due to write operations. A large write operation on another table would not impact the write capacity or throttling of the table in question. DynamoDB tables are independent in terms of their capacity and operations, so activity on one table does not affect another unless they are explicitly linked, which is not mentioned here.""}"
"A company runs an e-commerce website that uses Amazon DynamoDB where pricing for items is dynamically updated in real time. At any given time, multiple updates may occur simultaneously for pricing information on a particular product. This is causing the original editor's changes to be overwritten without a proper review process. Which DynamoDB write option should be selected to prevent this overwriting?","[""Concurrent writes."", ""Conditional writes."", ""Atomic writes."", ""Batch writes.""]","[""Conditional writes.""]","```json
{
    ""Concurrent writes."": ""Concurrent writes refer to multiple write operations happening at the same time. However, this does not inherently solve the problem of overwriting changes, as concurrent writes can still lead to conflicts where one write overwrites another without any checks. This option does not provide a mechanism to ensure that updates are applied only if certain conditions are met, which is necessary to prevent overwriting without review."",
    ""Conditional writes."": ""Conditional writes in DynamoDB allow you to specify conditions that must be met for the write operation to proceed. This is the correct choice because it prevents overwriting by allowing updates only if the current item state matches the specified condition, such as a version number or a timestamp. This ensures that changes are not applied if the item has been modified since it was last read, thus preventing overwriting without a review process. This is a best practice for handling concurrent updates in DynamoDB, as documented in AWS best practices."",
    ""Atomic writes."": ""Atomic writes ensure that a series of operations are completed as a single unit of work, meaning either all operations succeed, or none do. While atomicity is important for ensuring consistency, it does not address the issue of preventing overwrites due to concurrent updates. Atomic writes do not provide a mechanism to check conditions before applying updates, which is necessary to prevent overwriting without review."",
    ""Batch writes."": ""Batch writes in DynamoDB allow you to perform multiple write operations in a single API call. While this can improve efficiency, it does not inherently prevent overwriting of data. Batch writes do not include conditional logic to ensure that updates are only applied if certain conditions are met, which is needed to prevent overwriting changes without a review process.""
}
```"
"A Developer has been asked to create an AWS Lambda function that is triggered any time updates are made to items in an Amazon DynamoDB table. The function has been created, and appropriate permissions have been added to the Lambda execution role. Amazon DynamoDB streams have been enabled for the table, but the function is still not being triggered. Which option would enable DynamoDB table updates to trigger the Lambda function?","[""Change the StreamViewType parameter value to NEW_AND_OLD_IMAGES for the DynamoDB table."", ""Configure event source mapping for the Lambda function."", ""Map an Amazon SNS topic to the DynamoDB streams."", ""Increase the maximum execution time (timeout) setting of the Lambda function.""]","[""Configure event source mapping for the Lambda function.""]","```json
{
    ""Change the StreamViewType parameter value to NEW_AND_OLD_IMAGES for the DynamoDB table."": ""This choice is incorrect. While the StreamViewType determines the information written to the stream, it does not affect the triggering of a Lambda function. The StreamViewType can be any valid option (e.g., KEYS_ONLY, NEW_IMAGE, OLD_IMAGE, NEW_AND_OLD_IMAGES) and the Lambda function can still be triggered as long as event source mapping is configured. Changing the StreamViewType is more about what data you want to capture, not about enabling the trigger."",
    ""Configure event source mapping for the Lambda function."": ""This choice is correct. Event source mapping is required to connect a DynamoDB stream to a Lambda function. It tells Lambda to poll the stream and invoke the function when records are detected. Without this mapping, the Lambda function will not be triggered by updates to the DynamoDB table, even if the stream is enabled. This is a key step in integrating DynamoDB with Lambda for real-time data processing."",
    ""Map an Amazon SNS topic to the DynamoDB streams."": ""This choice is incorrect. SNS topics are not directly involved in triggering Lambda functions from DynamoDB streams. SNS is used for pub/sub messaging, not for direct integration with DynamoDB streams. To trigger a Lambda function from a DynamoDB stream, you must use event source mapping, not SNS."",
    ""Increase the maximum execution time (timeout) setting of the Lambda function."": ""This choice is incorrect. The timeout setting of a Lambda function determines how long the function can run before it is forcibly terminated, but it does not affect whether the function is triggered. If the function is not being triggered, adjusting the timeout will not resolve the issue. The problem lies in the lack of event source mapping, not in the function's execution time.""
}
```"
A company is running a Docker application on Amazon ECS. The application must scale based on user load in the last 15 seconds. How should a Developer instrument the code so that the requirement can be met?,"[""Create a high-resolution custom Amazon CloudWatch metric for user activity data, then publish data every 30 seconds."", ""Create a high-resolution custom Amazon CloudWatch metric for user activity data, then publish data every 5 seconds."", ""Create a standard-resolution custom Amazon CloudWatch metric for user activity data, then publish data every 30 seconds."", ""Create a standard-resolution custom Amazon CloudWatch metric for user activity data, then publish data every 5 seconds.""]","[""Create a high-resolution custom Amazon CloudWatch metric for user activity data, then publish data every 5 seconds.""]","```json
{
    ""Create a high-resolution custom Amazon CloudWatch metric for user activity data, then publish data every 30 seconds."": ""This choice is incorrect because while it uses high-resolution metrics, which are necessary for capturing data at a granularity of 1 second, publishing data every 30 seconds does not meet the requirement to scale based on user load in the last 15 seconds. High-resolution metrics should be published more frequently to capture the rapid changes in user load."",
    ""Create a high-resolution custom Amazon CloudWatch metric for user activity data, then publish data every 5 seconds."": ""This choice is correct because it uses high-resolution metrics, which allow for data granularity of 1 second, and publishing data every 5 seconds ensures that the application can respond to changes in user load within the required 15-second window. This aligns with AWS best practices for applications that need to scale rapidly based on real-time data."",
    ""Create a standard-resolution custom Amazon CloudWatch metric for user activity data, then publish data every 30 seconds."": ""This choice is incorrect because standard-resolution metrics only provide data granularity of 1 minute, which is insufficient for the requirement to scale based on user load in the last 15 seconds. Additionally, publishing every 30 seconds does not provide the necessary frequency to capture rapid changes in user activity."",
    ""Create a standard-resolution custom Amazon CloudWatch metric for user activity data, then publish data every 5 seconds."": ""This choice is incorrect because even though data is published every 5 seconds, standard-resolution metrics only provide data granularity of 1 minute. This means that the metric cannot accurately reflect changes in user load within a 15-second window, making it unsuitable for the requirement.""
}
```"
A company needs to ingest terabytes of data each hour from thousands of sources that are delivered almost continually throughout the day. The volume of messages generated varies over the course of the day. Messages must be delivered in real time for fraud detection and live operational dashboards. Which approach will meet these requirements?,"[""Send the messages to an Amazon SQS queue, then process the messages by using a fleet of Amazon EC2 instances."", ""Use the Amazon S3 API to write messages to an S3 bucket, then process the messages by using Amazon Redshift."", ""Use AWS Data Pipeline to automate the movement and transformation of data."", ""Use Amazon Kinesis Data Streams with Kinesis Client Library to ingest and deliver messages.""]","[""Use Amazon Kinesis Data Streams with Kinesis Client Library to ingest and deliver messages.""]","```json
{
    ""Send the messages to an Amazon SQS queue, then process the messages by using a fleet of Amazon EC2 instances."": ""This choice is incorrect because Amazon SQS is designed for message queuing and not for real-time data streaming. SQS is suitable for decoupling and scaling microservices, distributed systems, and serverless applications, but it does not provide the real-time processing capabilities required for fraud detection and live dashboards. Additionally, using EC2 instances for processing adds complexity and does not inherently provide the scaling and real-time processing capabilities of Kinesis."",
    ""Use the Amazon S3 API to write messages to an S3 bucket, then process the messages by using Amazon Redshift."": ""This choice is incorrect because Amazon S3 and Redshift are not designed for real-time data ingestion and processing. S3 is a storage service, and while it can handle large volumes of data, it does not provide real-time processing. Redshift is a data warehousing solution optimized for complex queries on large datasets, not for real-time data streaming. This approach would introduce latency and is not suitable for real-time fraud detection and operational dashboards."",
    ""Use AWS Data Pipeline to automate the movement and transformation of data."": ""This choice is incorrect because AWS Data Pipeline is used for data-driven workflows and batch processing, not for real-time data ingestion and processing. It is designed to move and transform data at specified intervals, which does not meet the requirement for real-time message delivery and processing."",
    ""Use Amazon Kinesis Data Streams with Kinesis Client Library to ingest and deliver messages."": ""This choice is correct because Amazon Kinesis Data Streams is designed for real-time data streaming. It can handle high throughput of data from thousands of sources and provides real-time processing capabilities. The Kinesis Client Library (KCL) allows for easy processing of streaming data with automatic load balancing and checkpointing, making it ideal for applications like fraud detection and live operational dashboards. This approach aligns with AWS best practices for real-time data processing and is supported by AWS documentation.""
}
```"
"A Developer accesses AWS CodeCommit over SSH. The SSH keys configured to access AWS CodeCommit are tied to a user with the following permissions. The Developer needs to create/delete branches. Which specific IAM permissions need to be added, based on the principle of least privilege?","[""`\""codecommit:CreateBranch\"" \""codecommit:DeleteBranch\""`."", ""`\""codecommit:Put*\""`."", ""`\""codecommit:Update*\""`."", ""`\""codecommit:*\""`.""]","[""`\""codecommit:CreateBranch\"" \""codecommit:DeleteBranch\""`.""]","```json
{
    ""\""codecommit:CreateBranch\"" \""codecommit:DeleteBranch\""\""."": ""This choice is correct because it grants the specific permissions needed to create and delete branches in AWS CodeCommit. According to the principle of least privilege, users should only be granted permissions necessary to perform their tasks. These permissions directly address the requirement to create and delete branches without granting unnecessary access."",
    ""\""codecommit:Put*\""."": ""This choice is incorrect because it grants broader permissions than necessary. The 'Put*' wildcard would allow various actions beyond creating and deleting branches, such as updating repository files, which is not required in this scenario. This violates the principle of least privilege by granting more access than needed."",
    ""\""codecommit:Update*\""."": ""This choice is incorrect because it does not include the specific permissions needed to create or delete branches. The 'Update*' wildcard typically involves modifying existing resources, not creating or deleting them. Therefore, it does not fulfill the requirement of creating or deleting branches."",
    ""\""codecommit:*\""."": ""This choice is incorrect because it grants all possible CodeCommit permissions, which is excessive for the task at hand. While it includes the necessary permissions to create and delete branches, it also includes many other permissions that are not needed, violating the principle of least privilege.""
}
```"
An AWS Lambda function must access an external site by using a regularly rotated user name and password. These items must be kept securely and cannot be stored in the function code. What combination of AWS services can be used to accomplish this? (Choose TWO),"[""AWS Certificate Manager (ACM)."", ""AWS Systems Manager Parameter Store."", ""AWS Trusted Advisor."", ""AWS KMS."", ""Amazon GuardDuty.""]","[""AWS Systems Manager Parameter Store."", ""AWS KMS.""]","```json
{
    ""AWS Certificate Manager (ACM)."": ""ACM is used for managing SSL/TLS certificates for securing network communications and establishing the identity of websites over the internet or resources on a private network. It is not used for storing or managing user names and passwords, which makes it incorrect for this use case."",
    ""AWS Systems Manager Parameter Store."": ""AWS Systems Manager Parameter Store is a service that provides secure, hierarchical storage for configuration data management and secrets management. It is suitable for storing sensitive data such as user names and passwords, which can be encrypted and accessed securely by AWS Lambda functions. This makes it a correct choice for the scenario."",
    ""AWS Trusted Advisor."": ""AWS Trusted Advisor is a tool that provides real-time guidance to help you provision your resources following AWS best practices. It does not provide storage or management of secrets like user names and passwords, making it irrelevant to the scenario."",
    ""AWS KMS."": ""AWS Key Management Service (KMS) is used to create and manage cryptographic keys and control their use across a wide range of AWS services and in your applications. It can be used to encrypt sensitive data stored in AWS Systems Manager Parameter Store, ensuring that the credentials are stored securely. This makes it a correct choice for the scenario."",
    ""Amazon GuardDuty."": ""Amazon GuardDuty is a threat detection service that continuously monitors for malicious activity and unauthorized behavior to protect your AWS accounts and workloads. It is not used for storing or managing secrets like user names and passwords, making it incorrect for this use case.""
}
```"
A Developer is trying to deploy a serverless application using AWS CodeDeploy. The application was updated and needs to be redeployed. What file does the Developer need to update to push that change through CodeDeploy?,"[""`dockerrun.aws.json`."", ""`buildspec.yml`."", ""`appspec.yml`."", ""`ebextensions.config`.""]","[""`appspec.yml`.""]","```json
{
    ""`dockerrun.aws.json`."": ""This file is used with Amazon Elastic Beanstalk to define the configuration of a Docker container. It is not used by AWS CodeDeploy for serverless applications. Therefore, it is incorrect in the context of deploying a serverless application with CodeDeploy."",
    ""`buildspec.yml`."": ""This file is used by AWS CodeBuild to define the build process of an application. While it is crucial for specifying build commands, it is not used by AWS CodeDeploy to manage deployments. Therefore, it is incorrect for the purpose of redeploying a serverless application using CodeDeploy."",
    ""`appspec.yml`."": ""This is the correct answer because the `appspec.yml` file is used by AWS CodeDeploy to manage the deployment of applications, including serverless applications. It defines the deployment actions and specifies the resources that need to be updated. Updating this file is essential to push changes through CodeDeploy."",
    ""`ebextensions.config`."": ""This file is used with AWS Elastic Beanstalk to customize the environment and is not related to AWS CodeDeploy. It is not applicable for deploying serverless applications using CodeDeploy, making it an incorrect choice in this context.""
}
```"
A Developer is working on an application that handles 10MB documents that contain highly-sensitive data. The application will use AWS KMS to perform clientside encryption. What steps must be followed?,"[""Invoke the Encrypt API passing the plaintext data that must be encrypted, then reference the customer managed key ARN in the KeyId parameter."", ""Invoke the GenerateRandom API to get a data encryption key, then use the data encryption key to encrypt the data."", ""Invoke the GenerateDataKey API to retrieve the encrypted version of the data encryption key to encrypt the data."", ""Invoke the GenerateDataKey API to retrieve the plaintext version of the data encryption key to encrypt the data.""]","[""Invoke the GenerateDataKey API to retrieve the plaintext version of the data encryption key to encrypt the data.""]","```json
{
    ""Invoke the Encrypt API passing the plaintext data that must be encrypted, then reference the customer managed key ARN in the KeyId parameter."": ""This choice is incorrect because the Encrypt API is used for server-side encryption, where the data is sent to AWS KMS for encryption. In client-side encryption, the data encryption key is used locally to encrypt the data before it is sent to AWS services. The Encrypt API is not suitable for client-side encryption of large data like 10MB documents."",
    ""Invoke the GenerateRandom API to get a data encryption key, then use the data encryption key to encrypt the data."": ""This choice is incorrect because the GenerateRandom API is used to generate random byte strings, not data encryption keys. For client-side encryption, a data encryption key is needed, which is specifically generated using the GenerateDataKey API."",
    ""Invoke the GenerateDataKey API to retrieve the encrypted version of the data encryption key to encrypt the data."": ""This choice is incorrect because the encrypted version of the data encryption key cannot be used to encrypt data directly. The encrypted key is meant to be stored securely alongside the encrypted data, while the plaintext version of the key is used for the actual encryption process."",
    ""Invoke the GenerateDataKey API to retrieve the plaintext version of the data encryption key to encrypt the data."": ""This choice is correct because the GenerateDataKey API provides both the plaintext and encrypted versions of a data encryption key. The plaintext version is used to encrypt the data locally in client-side encryption scenarios. This follows AWS best practices for encrypting sensitive data before sending it to AWS services, ensuring that the data remains secure.""
}
```"
"A Developer is building a web application that uses Amazon API Gateway to expose an AWS Lambda function to process requests from clients. During testing, the Developer notices that the API Gateway times out even though the Lambda function finishes under the set time limit. Which of the following API Gateway metrics in Amazon CloudWatch can help the Developer troubleshoot the issue? (Choose TWO)","[""CacheHitCount."", ""IntegrationLatency."", ""CacheMissCount."", ""Latency."", ""Count.""]","[""IntegrationLatency."", ""Latency.""]","```json
{
    ""CacheHitCount."": ""CacheHitCount is a metric that measures the number of requests served from the cache in API Gateway. It is related to caching behavior and not directly relevant to troubleshooting latency or timeout issues between API Gateway and Lambda. Therefore, it is incorrect in this context."",
    ""IntegrationLatency."": ""IntegrationLatency measures the time between when API Gateway relays a request to the backend (in this case, the Lambda function) and when it receives a response from the backend. This metric is crucial for diagnosing issues related to backend processing time and helps determine if the delay is occurring within the Lambda function execution. It is correct because it directly relates to the problem of API Gateway timing out."",
    ""CacheMissCount."": ""CacheMissCount measures the number of requests that were not served from the cache and had to be forwarded to the backend. Like CacheHitCount, it is related to caching and not relevant to diagnosing latency or timeout issues with Lambda. Therefore, it is incorrect in this context."",
    ""Latency."": ""Latency measures the total time from when API Gateway receives a request from a client to when it returns a response to the client. This includes the time spent in API Gateway and the backend processing time. It is correct because it helps identify if the delay is occurring before or after the request is sent to the Lambda function, which is essential for troubleshooting timeout issues."",
    ""Count."": ""Count is a metric that measures the total number of API requests received by API Gateway. While useful for understanding traffic volume, it does not provide insights into latency or timeout issues. Therefore, it is incorrect in this context.""
}
```"
A company needs to distribute firmware updates to its customers around the world. Which service will allow easy and secure control of the access to the downloads at the lowest cost?,"[""Use Amazon CloudFront with signed URLs for Amazon S3."", ""Create a dedicated Amazon CloudFront Distribution for each customer."", ""Use Amazon CloudFront with AWS Lambda@Edge."", ""Use Amazon API Gateway and AWS Lambda to control access to an S3 bucket.""]","[""Use Amazon CloudFront with signed URLs for Amazon S3.""]","```json
{
    ""Use Amazon CloudFront with signed URLs for Amazon S3."": ""This is the correct choice because Amazon CloudFront can distribute content globally with low latency and high transfer speeds. Using signed URLs allows for secure access control to the content stored in Amazon S3 by ensuring that only authorized users can download the firmware updates. This approach is cost-effective as it leverages CloudFront's caching capabilities to reduce data transfer costs and does not require additional infrastructure or complex configurations. AWS documentation recommends using CloudFront with signed URLs for secure content distribution."",
    ""Create a dedicated Amazon CloudFront Distribution for each customer."": ""This option is incorrect because creating a dedicated CloudFront distribution for each customer would be unnecessarily complex and costly. It would require managing multiple distributions, which increases operational overhead and costs. CloudFront with signed URLs can achieve the same goal of secure access control without the need for multiple distributions, making it a more efficient and cost-effective solution."",
    ""Use Amazon CloudFront with AWS Lambda@Edge."": ""This option is incorrect because while Lambda@Edge can be used to run code closer to users and customize content delivery, it is not necessary for the basic requirement of securely distributing firmware updates. Using signed URLs with CloudFront is sufficient for access control, and adding Lambda@Edge would increase complexity and cost without providing additional benefits for this specific use case."",
    ""Use Amazon API Gateway and AWS Lambda to control access to an S3 bucket."": ""This option is incorrect because although API Gateway and Lambda can be used to control access to S3, this setup is more complex and potentially more expensive than using CloudFront with signed URLs. API Gateway introduces additional costs and latency, and this architecture is better suited for scenarios where you need to implement complex logic or transformations, which is not necessary for simple content distribution.""
}
```"
"An application writes items to an Amazon DynamoDB table. As the application scales to thousands of instances, calls to the DynamoDB API generate occasional `ThrottlingException` errors. The application is coded in a language incompatible with the AWS SDK. How should the error be handled?","[""Add exponential backoff to the application logic."", ""Use Amazon SQS as an API message bus."", ""Pass API calls through Amazon API Gateway."", ""Send the items to DynamoDB through Amazon Kinesis Data Firehose.""]","[""Add exponential backoff to the application logic.""]","```json
{
    ""Add exponential backoff to the application logic."": ""This is the correct answer. Exponential backoff is a best practice for handling throttling in AWS services, including DynamoDB. When a `ThrottlingException` occurs, it indicates that the request rate is too high. Exponential backoff helps by retrying the request after progressively longer intervals, reducing the load on the service and increasing the likelihood of successful requests. This approach is recommended by AWS for handling throttling errors, especially when the application cannot use the AWS SDK."",
    ""Use Amazon SQS as an API message bus."": ""This is incorrect. While Amazon SQS can be used to decouple components of an application and manage message queuing, it does not directly address the issue of `ThrottlingException` errors from DynamoDB. SQS would not solve the problem of API throttling; it would only queue messages, which would still eventually need to be processed by DynamoDB. The primary issue here is handling throttling, which is best addressed by exponential backoff."",
    ""Pass API calls through Amazon API Gateway."": ""This is incorrect. Amazon API Gateway can help manage and scale API requests, but it does not inherently solve the problem of DynamoDB throttling. API Gateway could add another layer to manage incoming requests, but it would not prevent `ThrottlingException` errors from DynamoDB. The issue is with the rate of requests to DynamoDB, which needs to be managed by the application logic itself, such as through exponential backoff."",
    ""Send the items to DynamoDB through Amazon Kinesis Data Firehose."": ""This is incorrect. Amazon Kinesis Data Firehose is used for streaming data to destinations like S3, Redshift, or Elasticsearch, but it is not typically used for DynamoDB. While it can buffer and batch data, it does not directly address the problem of handling `ThrottlingException` errors from DynamoDB. The issue here is with the application's request rate to DynamoDB, which is best managed by implementing exponential backoff.""
}
```"
"An e-commerce web application that shares session state on-premises is being migrated to AWS. The application must be fault tolerant, natively highly scalable, and any service interruption should not affect the user experience. What is the best option to store the session state?","[""Store the session state in Amazon ElastiCache."", ""Store the session state in Amazon CloudFront."", ""Store the session state in Amazon S3."", ""Enable session stickiness using elastic load balancers.""]","[""Store the session state in Amazon ElastiCache.""]","```json
{
    ""Store the session state in Amazon ElastiCache."": ""This is the correct choice because Amazon ElastiCache is a managed in-memory data store service that supports Redis and Memcached. It is designed for high performance, low latency, and scalability, making it ideal for storing session state. ElastiCache can handle large volumes of read and write operations, which is crucial for maintaining session state in a fault-tolerant and highly scalable manner. It also supports replication and automatic failover, which ensures that service interruptions do not affect the user experience. AWS best practices recommend using ElastiCache for session management in distributed applications."",
    ""Store the session state in Amazon CloudFront."": ""This choice is incorrect because Amazon CloudFront is a content delivery network (CDN) service that is used to cache and deliver content to users with low latency. It is not designed to store session state data. CloudFront is suitable for distributing static and dynamic web content, but it does not provide the necessary functionality for session management, such as handling read/write operations or maintaining state consistency across requests."",
    ""Store the session state in Amazon S3."": ""This choice is incorrect because Amazon S3 is an object storage service designed for storing and retrieving any amount of data at any time. While S3 is highly durable and scalable, it is not optimized for the low-latency, high-frequency read/write operations required for session state management. Using S3 for session state would result in increased latency and a suboptimal user experience, especially for applications requiring real-time session updates."",
    ""Enable session stickiness using elastic load balancers."": ""This choice is incorrect because enabling session stickiness (also known as session affinity) on an Elastic Load Balancer (ELB) directs requests from the same client to the same backend instance. While this can help maintain session state on a specific instance, it does not provide fault tolerance or scalability. If the instance fails, the session state is lost, which can affect the user experience. Additionally, session stickiness does not distribute load evenly, which can lead to performance bottlenecks. AWS best practices recommend using a distributed caching solution like ElastiCache for session state management.""
}
```"
"A Developer is creating a template that uses AWS CloudFormation to deploy an application. This application is serverless and uses Amazon API Gateway, Amazon DynamoDB, and AWS Lambda. Which tool should the Developer use to define simplified syntax for expressing serverless resources?","[""CloudFormation serverless intrinsic functions."", ""AWS serverless express."", ""An AWS serverless application model."", ""A CloudFormation serverless plugin.""]","[""An AWS serverless application model.""]","```json
{
    ""CloudFormation serverless intrinsic functions."": ""This choice is incorrect because AWS CloudFormation intrinsic functions are used to handle dynamic values in templates, such as referencing parameters or resources, but they do not provide a simplified syntax specifically for serverless resources. Intrinsic functions are more about template logic rather than simplifying serverless resource definitions."",
    ""AWS serverless express."": ""This choice is incorrect because AWS Serverless Express is a library that helps developers run Node.js applications in AWS Lambda using the Express framework. It is not a tool for defining serverless resources in CloudFormation templates. It is used for application logic rather than infrastructure definition."",
    ""An AWS serverless application model."": ""This choice is correct because the AWS Serverless Application Model (AWS SAM) is a framework that extends AWS CloudFormation to provide a simplified syntax specifically for defining serverless applications. It allows developers to define serverless resources like Lambda functions, API Gateway APIs, and DynamoDB tables using a simplified syntax, which is then transformed into CloudFormation during deployment. AWS SAM is widely used for deploying serverless applications efficiently."",
    ""A CloudFormation serverless plugin."": ""This choice is incorrect because there is no specific 'CloudFormation serverless plugin' for defining serverless resources. CloudFormation itself does not have plugins, and serverless resource definitions are typically handled by AWS SAM rather than any plugin system.""
}
```"
A Developer has a stateful web server on-premises that is being migrated to AWS. The Developer must have greater elasticity in the new design. How should the Developer re-factor the application to make it more elastic? (Choose TWO),"[""Use pessimistic concurrency on Amazon DynamoDB."", ""Use Amazon CloudFront with an Auto Scaling group."", ""Use Amazon CloudFront with an AWS Web Application Firewall."", ""Store session state data in an Amazon DynamoDB table."", ""Use an ELB with an Auto Scaling group.""]","[""Store session state data in an Amazon DynamoDB table."", ""Use an ELB with an Auto Scaling group.""]","```json
{
    ""Use pessimistic concurrency on Amazon DynamoDB."": ""This choice is incorrect because pessimistic concurrency is not related to improving elasticity. Pessimistic concurrency is a database management technique to prevent conflicts by locking resources, which can actually reduce scalability and elasticity. Elasticity is achieved by enabling the application to handle varying loads efficiently, which is not directly related to concurrency control methods."",
    ""Use Amazon CloudFront with an Auto Scaling group."": ""This choice is incorrect because while Amazon CloudFront can improve performance by caching content at edge locations, it does not directly contribute to the elasticity of the application itself. Elasticity involves dynamically adjusting resources to meet demand, which is achieved through services like Auto Scaling and ELB. CloudFront is more about distributing content efficiently rather than scaling compute resources."",
    ""Use Amazon CloudFront with an AWS Web Application Firewall."": ""This choice is incorrect because although Amazon CloudFront and AWS WAF can enhance security and performance, they do not directly contribute to the elasticity of the application. Elasticity is about scaling resources up or down based on demand, which is handled by Auto Scaling and ELB. CloudFront and WAF are more focused on content delivery and security."",
    ""Store session state data in an Amazon DynamoDB table."": ""This choice is correct because storing session state data in Amazon DynamoDB allows the application to scale more easily. By decoupling session state from the web server and storing it in a managed database service like DynamoDB, the application can scale horizontally, as any instance can handle any request without needing to maintain session state locally. This is a best practice for achieving elasticity in cloud applications."",
    ""Use an ELB with an Auto Scaling group."": ""This choice is correct because using an Elastic Load Balancer (ELB) with an Auto Scaling group is a fundamental way to achieve elasticity in AWS. The ELB distributes incoming traffic across multiple instances, while the Auto Scaling group automatically adjusts the number of instances based on demand. This combination allows the application to scale up during high demand and scale down during low demand, ensuring efficient resource utilization and cost-effectiveness.""
}
```"
A Developer must analyze performance issues with production-distributed applications written as AWS Lambda functions. These distributed Lambda applications invoke other components that make up the applications. How should the Developer identify and troubleshoot the root cause of the performance issues in production?,"[""Add logging statements to the Lambda functions, then use Amazon CloudWatch to view the logs."", ""Use AWS Cloud Trail and then examine the logs."", ""Use AWS X-Ray, then examine the segments and errors."", ""Run Amazon Inspector agents and then analyze performance.""]","[""Use AWS X-Ray, then examine the segments and errors.""]","```json
{
    ""Add logging statements to the Lambda functions, then use Amazon CloudWatch to view the logs."": ""This choice is incorrect because while adding logging statements and using Amazon CloudWatch Logs can help in understanding what is happening inside the Lambda functions, it does not provide a comprehensive view of the distributed application or help in identifying performance bottlenecks across multiple services. CloudWatch Logs are more suited for debugging specific issues within a single function rather than analyzing performance across a distributed system."",
    ""Use AWS Cloud Trail and then examine the logs."": ""This choice is incorrect because AWS CloudTrail is primarily used for auditing and logging API calls made in an AWS account. It is not designed for performance analysis or troubleshooting application-level issues. CloudTrail logs provide information about who made a request, from where, and when, but not about the performance characteristics of Lambda functions or other AWS services."",
    ""Use AWS X-Ray, then examine the segments and errors."": ""This choice is correct because AWS X-Ray is specifically designed to analyze and debug distributed applications, such as those using AWS Lambda. X-Ray provides insights into the performance of applications by tracing requests as they travel through the application components, identifying bottlenecks, and visualizing service maps. It allows developers to examine segments and errors, making it easier to pinpoint performance issues in a distributed environment. This aligns with AWS best practices for monitoring and troubleshooting distributed applications."",
    ""Run Amazon Inspector agents and then analyze performance."": ""This choice is incorrect because Amazon Inspector is a security assessment service that helps improve the security and compliance of applications deployed on AWS. It is not intended for performance analysis or troubleshooting application performance issues. Amazon Inspector focuses on identifying vulnerabilities and deviations from best practices, not on analyzing performance metrics or tracing requests.""
}
```"
"A Developer wants to debug an application by searching and filtering log data. The application logs are stored in Amazon CloudWatch Logs. The Developer creates a new metric filter to count exceptions in the application logs. However, no results are returned from the logs. What is the reason that no filtered results are being returned?","[""A setup of the Amazon CloudWatch interface VPC endpoint is required for filtering the CloudWatch Logs in the VPC."", ""CloudWatch Logs only publishes metric data for events that happen after the filter is created."", ""The log group for CloudWatch Logs should be first streamed to Amazon Elasticsearch Service before metric filtering returns the results."", ""Metric data points for logs groups can be filtered only after they are exported to an Amazon S3 bucket.""]","[""CloudWatch Logs only publishes metric data for events that happen after the filter is created.""]","```json
{
    ""A setup of the Amazon CloudWatch interface VPC endpoint is required for filtering the CloudWatch Logs in the VPC."": ""This choice is incorrect because setting up a VPC endpoint is not necessary for filtering CloudWatch Logs. VPC endpoints are used to privately connect your VPC to supported AWS services and VPC endpoint services powered by AWS PrivateLink. Filtering logs in CloudWatch is independent of whether the logs are accessed via a VPC endpoint or not."",
    ""CloudWatch Logs only publishes metric data for events that happen after the filter is created."": ""This choice is correct. According to AWS documentation, CloudWatch Logs metric filters only evaluate log events that occur after the filter is created. This means that any log events that happened before the creation of the metric filter will not be counted or included in the metric data. This is a common scenario where developers need to be aware that metric filters are not retroactive."",
    ""The log group for CloudWatch Logs should be first streamed to Amazon Elasticsearch Service before metric filtering returns the results."": ""This choice is incorrect. Streaming logs to Amazon Elasticsearch Service (now Amazon OpenSearch Service) is an option for searching and analyzing logs, but it is not required for CloudWatch Logs metric filtering. Metric filters work directly on the CloudWatch Logs data without needing to stream the logs elsewhere."",
    ""Metric data points for logs groups can be filtered only after they are exported to an Amazon S3 bucket."": ""This choice is incorrect. Exporting logs to an Amazon S3 bucket is a separate process used for archiving or further analysis, but it is not required for metric filtering in CloudWatch Logs. Metric filters operate directly on the log data within CloudWatch Logs and do not require export to S3.""
}
```"
"To include objects defined by the AWS Serverless Application Model (SAM) in an AWS CloudFormation template, in addition to Resources, what section MUST be included in the document root?","[""Conditions."", ""Globals."", ""Transform."", ""Properties.""]","[""Transform.""]","```json
{
    ""Conditions."": ""Conditions are used in AWS CloudFormation templates to define conditions that control whether certain resources are created or certain properties are assigned a value during stack creation or update. They are not specifically required for including AWS SAM objects in a CloudFormation template. Therefore, this choice is incorrect for the question."",
    ""Globals."": ""Globals are used in AWS SAM templates to define default settings for function properties, such as runtime and memory size, that apply to all functions in the template. While useful, they are not mandatory for including SAM objects in a CloudFormation template. Thus, this choice is incorrect."",
    ""Transform."": ""The Transform section is required in a CloudFormation template to include AWS SAM resources. It specifies the use of the AWS::Serverless-2016-10-31 transform, which allows the template to use AWS SAM syntax. This is essential for processing SAM-specific resources, making this the correct answer."",
    ""Properties."": ""Properties are used within resource definitions in AWS CloudFormation templates to specify configuration details for resources. They are not a top-level section required for including AWS SAM objects. Therefore, this choice is incorrect.""
}
```"
A company is using Amazon RDS MySQL instances for its application database tier and Apache Tomcat servers for its web tier. Most of the database queries from web applications are repeated read requests. Use of which AWS service would increase in performance by adding in-memory store for repeated read queries?,"[""Amazon RDS Multi-AZ."", ""Amazon SQS."", ""Amazon ElastiCache."", ""Amazon RDS read replica.""]","[""Amazon ElastiCache.""]","{""Amazon RDS Multi-AZ."": ""Amazon RDS Multi-AZ is designed for high availability and failover support rather than performance improvement. It provides a standby instance in a different Availability Zone to automatically take over if the primary instance fails. This does not address the need for an in-memory store to cache repeated read queries, which is why it is incorrect for improving read performance."", ""Amazon SQS."": ""Amazon SQS is a fully managed message queuing service that enables decoupling of microservices, distributed systems, and serverless applications. It is not designed for caching database queries or improving read performance. Therefore, it is incorrect for the scenario of adding an in-memory store for repeated read queries."", ""Amazon ElastiCache."": ""Amazon ElastiCache is the correct choice as it provides an in-memory data store or cache, which can significantly improve the performance of applications with frequent read queries by storing the results of these queries in memory. This reduces the need to repeatedly access the database, thus lowering latency and improving throughput. It supports Redis and Memcached, both of which are popular for caching purposes."", ""Amazon RDS read replica."": ""Amazon RDS read replicas are used to offload read traffic from the primary database instance, which can improve read scalability. However, they do not provide an in-memory caching solution. While read replicas can help distribute read load, they do not offer the same performance benefits as an in-memory cache like ElastiCache for repeated queries. Therefore, it is not the best choice for the scenario described.""}"
"A Developer is investigating an issue whereby certain requests are passing through an Amazon API Gateway endpoint /MyAPI, but the requests do not reach the AWS Lambda function backing /MyAPI. The Developer found that a second Lambda function sometimes runs at maximum concurrency allowed for the given AWS account. How can the Developer address this issue?","[""Manually reduce the concurrent execution limit at the account level."", ""Add another API Gateway stage for /MyAPI, and shard the requests."", ""Configure the second Lambda function's concurrency execution limit."", ""Reduce the throttling limits in the API Gateway /MyAPI endpoint""]","[""Configure the second Lambda function's concurrency execution limit.""]","```json
{
    ""Manually reduce the concurrent execution limit at the account level."": ""This choice is incorrect because reducing the concurrent execution limit at the account level would affect all Lambda functions in the account, not just the problematic one. This could lead to unintended throttling of other Lambda functions that are operating correctly. AWS best practices recommend managing concurrency at the individual function level to avoid such issues."",
    ""Add another API Gateway stage for /MyAPI, and shard the requests."": ""This choice is incorrect because adding another API Gateway stage and sharding requests does not directly address the issue of Lambda function concurrency limits. The problem is related to the Lambda function's concurrency, not the API Gateway's ability to handle requests. Sharding requests would not prevent the second Lambda function from reaching its concurrency limit."",
    ""Configure the second Lambda function's concurrency execution limit."": ""This choice is correct because setting a concurrency limit on the second Lambda function ensures that it does not consume all available concurrent executions, which would prevent other functions from running. AWS allows you to set reserved concurrency for individual Lambda functions, ensuring that other functions have the necessary capacity to execute. This is a best practice for managing Lambda concurrency and preventing one function from starving others of resources."",
    ""Reduce the throttling limits in the API Gateway /MyAPI endpoint"": ""This choice is incorrect because reducing the throttling limits on the API Gateway endpoint would limit the number of requests that can be processed by the API Gateway, but it does not address the issue of Lambda function concurrency. The problem is not with the API Gateway's ability to handle requests, but with the Lambda function's ability to execute due to concurrency limits.""
}
```"
"A company is migrating a single-server, on-premises web application to AWS. The company intends to use multiple servers behind an Elastic Load Balancer (ELB) to balance the load, and will also store session data in memory on the web server. The company does not want to lose that session data if a server fails or goes offline, and it wants to minimize user's downtime. Where should the company move session data to MOST effectively reduce downtime and make users' session data more fault tolerant?","[""An Amazon ElastiCache for Redis cluster."", ""A second Amazon EBS volume."", ""The web server's primary disk."", ""An Amazon EC2 instance dedicated to session data.""]","[""An Amazon ElastiCache for Redis cluster.""]","```json
{
    ""An Amazon ElastiCache for Redis cluster."": ""This choice is correct because Amazon ElastiCache for Redis is a managed, in-memory data store that can be used to store session data. It provides high availability and automatic failover, which makes it fault-tolerant. Redis supports data replication and persistence, ensuring that session data is not lost if a server fails. This setup minimizes downtime and maintains session continuity for users, aligning with AWS best practices for stateless web applications."",
    ""A second Amazon EBS volume."": ""This choice is incorrect because Amazon EBS volumes are block storage devices that are typically used for persistent storage attached to EC2 instances. Storing session data on an EBS volume does not inherently provide the fault tolerance or high availability needed for session data, as it is tied to a single instance. If the instance fails, the session data would be lost, which does not meet the requirement of minimizing downtime and making session data fault-tolerant."",
    ""The web server's primary disk."": ""This choice is incorrect because storing session data on the web server's primary disk makes it vulnerable to data loss if the server fails or goes offline. This approach does not provide the fault tolerance or high availability needed for session data. It contradicts the requirement to minimize downtime and ensure session data is not lost, as the data is not shared across multiple servers."",
    ""An Amazon EC2 instance dedicated to session data."": ""This choice is incorrect because while dedicating an EC2 instance for session data might centralize the data, it introduces a single point of failure. If the EC2 instance fails, all session data would be lost, which does not provide the fault tolerance or high availability required. Additionally, managing session data on an EC2 instance requires more overhead compared to using a managed service like ElastiCache.""
}
```"
"A Developer created configuration specifications for an AWS Elastic Beanstalk application in a file named healthcheckurl.yaml in the .ebextensions/directory of their application source bundle. The file contains the following: After the application launches, the health check is not being run on the correct path, even though it is valid. What can be done to correct this configuration file?","[""Convert the file to JSON format."", ""Rename the file to a .config extension."", ""Change the configuration section from options_settings to resources."", ""Change the namespace of the option settings to a custom namespace.""]","[""Rename the file to a .config extension.""]","```json
{
    ""Convert the file to JSON format."": ""This choice is incorrect because AWS Elastic Beanstalk configuration files must be in YAML format with a .config extension, not JSON. YAML is the required format for configuration files in the .ebextensions directory, which allows for defining environment options and other settings."",
    ""Rename the file to a .config extension."": ""This choice is correct because AWS Elastic Beanstalk requires configuration files in the .ebextensions directory to have a .config extension. This is a best practice and a requirement for Elastic Beanstalk to recognize and apply the configuration settings specified in the file."",
    ""Change the configuration section from options_settings to resources."": ""This choice is incorrect because the issue is not with the section name but with the file extension. The options_settings section is typically used for defining environment properties, and changing it to resources would not address the issue of the file not being recognized due to its extension."",
    ""Change the namespace of the option settings to a custom namespace."": ""This choice is incorrect because changing the namespace will not solve the problem of the configuration file not being recognized. The issue lies with the file extension, not the namespace. The namespace should match the specific configuration options you are trying to set, but the file must first be recognized by having the correct extension.""
}
```"
"A Developer has created a Lambda function and is finding that the function is taking longer to complete than expected. After some debugging, the Developer has discovered that increasing compute capacity would improve performance. How can the Developer increase the Lambda compute resources?","[""Run on a larger instance size with more compute capacity."", ""Increase the maximum execution time."", ""Specify a larger compute capacity when calling the Lambda function."", ""Increase the allocated memory for the Lambda function.""]","[""Increase the allocated memory for the Lambda function.""]","```json
{
    ""Run on a larger instance size with more compute capacity."": ""This choice is incorrect because AWS Lambda does not use EC2 instances directly. Lambda functions are serverless and do not require the developer to manage instance sizes. Instead, Lambda abstracts the underlying infrastructure, and compute capacity is managed through memory allocation, which also proportionally increases CPU and network resources."",
    ""Increase the maximum execution time."": ""This choice is incorrect because increasing the maximum execution time (timeout) does not directly improve performance. It only allows the function to run longer before timing out. The issue described is related to performance, not timeout, so increasing execution time would not address the need for more compute capacity."",
    ""Specify a larger compute capacity when calling the Lambda function."": ""This choice is incorrect because AWS Lambda does not allow specifying compute capacity directly when invoking a function. Compute resources are managed by adjusting the memory allocation, which indirectly increases CPU and network resources. There is no API or option to specify compute capacity directly during invocation."",
    ""Increase the allocated memory for the Lambda function."": ""This choice is correct because in AWS Lambda, increasing the allocated memory also increases the CPU and network resources available to the function. This is a key AWS concept where memory allocation is directly tied to the compute power available to a Lambda function. By increasing memory, the developer can improve the function's performance, which is consistent with AWS best practices for optimizing Lambda performance.""
}
```"
An e-commerce site allows returning users to log in to display customized web pages. The workflow is shown in the image below. An application is running on EC2 instances. Amazon RDS is used for the database that stores user accounts and preferences. The website freezes or is slow to load while waiting for the login step to complete. The remaining components of the site are well-optimized. Which of the following techniques will resolve this issue? (Select TWO),"[""Implement the user login page as an asynchronous Lambda function."", ""Use Amazon ElastiCache for MemCached to cache user data."", ""Use Amazon Application Load Balancer to load balance the traffic to the website."", ""Call the database asynchronously so the code can continue executing."", ""Batch login requests from hundreds of users together as a single read request to the database.""]","[""Use Amazon ElastiCache for MemCached to cache user data."", ""Call the database asynchronously so the code can continue executing.""]","```json
{
    ""Implement the user login page as an asynchronous Lambda function."": ""This choice is incorrect because Lambda functions are not typically used for handling synchronous web requests that require immediate responses, such as user logins. Lambda is better suited for event-driven architectures and asynchronous processing. Additionally, moving the login logic to Lambda would not inherently solve the issue of slow database access, which is the root cause of the problem."",
    ""Use Amazon ElastiCache for MemCached to cache user data."": ""This choice is correct because using Amazon ElastiCache for MemCached can significantly reduce the load on the database by caching frequently accessed data, such as user session information. This reduces the time it takes to retrieve user data during login, improving the responsiveness of the application. Caching is a best practice for improving application performance and scalability."",
    ""Use Amazon Application Load Balancer to load balance the traffic to the website."": ""This choice is incorrect because while an Application Load Balancer can distribute incoming traffic across multiple EC2 instances, it does not address the issue of slow database access during the login process. The problem described is related to database performance, not traffic distribution."",
    ""Call the database asynchronously so the code can continue executing."": ""This choice is correct because making asynchronous calls to the database allows the application to continue executing other tasks while waiting for the database response. This can improve the perceived performance of the application by not blocking the execution thread, allowing for a more responsive user experience. Asynchronous programming is a common technique to improve application responsiveness."",
    ""Batch login requests from hundreds of users together as a single read request to the database."": ""This choice is incorrect because batching login requests is not feasible or secure for user authentication. Each login request is unique and requires individual authentication and authorization. Batching would not only complicate the login process but also introduce security risks and potential data privacy issues.""
}
```"
A Developer is building a mobile application and needs any update to user profile data to be pushed to all devices accessing the specific identity. The Developer does not want to manage a back end to maintain the user profile data. What is the MOST efficient way for the Developer to achieve these requirements using Amazon Cognito?,"[""Use Cognito federated identities."", ""Use a Cognito user pool."", ""Use Cognito Sync."", ""Use Cognito events.""]","[""Use Cognito Sync.""]","```json
{
    ""Use Cognito federated identities."": ""This choice is incorrect. Cognito federated identities are used to provide temporary AWS credentials to users authenticated via external identity providers like Facebook, Google, or enterprise identity providers via SAML. It does not inherently provide a mechanism for syncing user profile data across devices."",
    ""Use a Cognito user pool."": ""This choice is incorrect. A Cognito user pool is primarily used for user authentication and management, such as sign-up and sign-in functionality. While it stores user profile data, it does not automatically sync profile updates across devices."",
    ""Use Cognito Sync."": ""This choice is correct. Cognito Sync is a service that enables cross-device syncing of user profile data. It allows developers to synchronize user-specific data across multiple devices without having to manage a backend, which fits the requirement of pushing updates to all devices accessing the specific identity."",
    ""Use Cognito events."": ""This choice is incorrect. Cognito events are used to trigger AWS Lambda functions in response to certain events in a Cognito user pool, such as user sign-up or sign-in. They are not designed for syncing user profile data across devices.""
}
```"
"A company maintains a REST service using Amazon API Gateway and the API Gateway native API key validation. The company recently launched a new registration page, which allows users to sign up for the service. The registration page creates a new API key using CreateApiKey and sends the new key to the user. When the user attempts to call the API using this key, the user receives a `403 Forbidden` error. Existing users are unaffected and can still call the API. What code updates will grant these new users access to the API?","[""The createDeployment method must be called so the API can be redeployed to include the newly created API key."", ""The updateAuthorizer method must be called to update the API's authorizer to include the newly created API key."", ""The importApiKeys method must be called to import all newly created API keys into the current stage of the API."", ""The createUsagePlanKey method must be called to associate the newly created API key with the correct usage plan.""]","[""The createUsagePlanKey method must be called to associate the newly created API key with the correct usage plan.""]","{""The createDeployment method must be called so the API can be redeployed to include the newly created API key."": ""This choice is incorrect. The createDeployment method is used to deploy an API Gateway API to a stage, which is typically done after changes to the API configuration or resources. However, API keys are not part of the API configuration that requires redeployment. API keys are managed separately and do not require redeployment to take effect."", ""The updateAuthorizer method must be called to update the API's authorizer to include the newly created API key."": ""This choice is incorrect. The updateAuthorizer method is used to update the authorizer settings for an API Gateway API, which is relevant when using custom authorizers or AWS Lambda authorizers for authentication. API keys are not managed through authorizers; they are used for throttling and quota management, not authentication."", ""The importApiKeys method must be called to import all newly created API keys into the current stage of the API."": ""This choice is incorrect. The importApiKeys method is used to import API keys from an external source in bulk. It is not necessary for individual API key creation and association with a usage plan. The issue here is not about importing keys but associating them with a usage plan."", ""The createUsagePlanKey method must be called to associate the newly created API key with the correct usage plan."": ""This choice is correct. In API Gateway, API keys must be associated with a usage plan to control access and manage quotas and throttling. The createUsagePlanKey method associates an API key with a usage plan, allowing the key to be used to access the API. Without this association, the API key will not be recognized, resulting in a 403 Forbidden error. This is a common step required when managing API keys in API Gateway.""}"
"A Developer is writing a mobile application that allows users to view images from an S3 bucket. The users must be able to log in with their Amazon login, as well as Facebook and/or Google accounts. How can the Developer provide this authentication functionality?","[""Use Amazon Cognito with web identity federation."", ""Use Amazon Cognito with SAML-based identity federation."", ""Use AWS IAM Access/Secret keys in the application code to allow Get* on the S3 bucket."", ""Use AWS STS AssumeRole in the application code and assume a role with Get* permissions on the S3 bucket.""]","[""Use Amazon Cognito with web identity federation.""]","```json
{
    ""Use Amazon Cognito with web identity federation."": ""This is the correct choice because Amazon Cognito supports web identity federation, which allows users to authenticate using external identity providers such as Amazon, Facebook, and Google. This is a common use case for mobile applications where users need to log in using their social media accounts. Cognito handles the authentication process and provides temporary AWS credentials to access resources like S3, following AWS best practices for security and scalability."",
    ""Use Amazon Cognito with SAML-based identity federation."": ""This choice is incorrect because SAML-based identity federation is typically used for enterprise identity providers, not for social logins like Facebook or Google. SAML is more suited for integrating with corporate identity systems, such as Active Directory, rather than public identity providers used in consumer applications."",
    ""Use AWS IAM Access/Secret keys in the application code to allow Get* on the S3 bucket."": ""This choice is incorrect because embedding AWS IAM Access and Secret keys directly in application code is a security risk. It exposes the keys to potential misuse and does not provide a scalable or secure way to manage user authentication and authorization. AWS best practices recommend using services like Cognito to handle authentication securely."",
    ""Use AWS STS AssumeRole in the application code and assume a role with Get* permissions on the S3 bucket."": ""This choice is incorrect because while AWS STS can be used to assume roles and obtain temporary credentials, it does not handle user authentication directly. The application would still need a mechanism to authenticate users with their Amazon, Facebook, or Google accounts, which is not addressed by STS alone. Cognito is the appropriate service for managing authentication and providing temporary credentials.""
}
```"
A Developer wants access to make the log data of an application running on an EC2 instance available to systems administrators. Which of the following enables monitoring of this metric in Amazon CloudWatch?,"[""Retrieve the log data from CloudWatch using the GetMetricData API call."", ""Retrieve the log data from AWS CloudTrail using the LookupEvents API call."", ""Launch a new EC2 instance, configure Amazon CloudWatch Events, and then install the application."", ""Install the Amazon CloudWatch Logs agent on the EC2 instance that the application is running on.""]","[""Install the Amazon CloudWatch Logs agent on the EC2 instance that the application is running on.""]","```json
{
    ""Retrieve the log data from CloudWatch using the GetMetricData API call."": ""This choice is incorrect because GetMetricData is used to retrieve metric data, not log data. CloudWatch Metrics and CloudWatch Logs are different services within AWS. To monitor log data, you need to use CloudWatch Logs specifically, not the metrics API."",
    ""Retrieve the log data from AWS CloudTrail using the LookupEvents API call."": ""This choice is incorrect because AWS CloudTrail is used for auditing and logging API calls made within your AWS account, not for application log data. LookupEvents is used to retrieve events recorded by CloudTrail, which is not suitable for application log monitoring."",
    ""Launch a new EC2 instance, configure Amazon CloudWatch Events, and then install the application."": ""This choice is incorrect because launching a new EC2 instance and configuring CloudWatch Events does not directly relate to making application log data available. CloudWatch Events is used for responding to changes in your AWS environment, not for collecting or monitoring application logs."",
    ""Install the Amazon CloudWatch Logs agent on the EC2 instance that the application is running on."": ""This choice is correct because installing the Amazon CloudWatch Logs agent on an EC2 instance allows you to collect and monitor log data from applications running on the instance. The agent can be configured to send log data to CloudWatch Logs, where it can be monitored and analyzed. This is the recommended approach for making application logs available for monitoring in CloudWatch.""
}
```"
"A nightly batch job loads 1 million new records into a DynamoDB table. The records are only needed for one hour, and the table needs to be empty by the next night's batch job. Which is the MOST efficient and cost-effective method to provide an empty table?","[""Use DeleteItem using a ConditionExpression."", ""Use BatchWriteItem to empty all of the rows."", ""Write a recursive function that scans and calls out DeleteItem."", ""Create and then delete the table after the task has completed.""]","[""Use DeleteItem using a ConditionExpression.""]","```json
{
    ""Use DeleteItem using a ConditionExpression."": ""This choice is incorrect because using DeleteItem with a ConditionExpression is not the most efficient or cost-effective method for emptying a table with 1 million records. DeleteItem operates on a single item at a time, which would require 1 million separate operations, making it both time-consuming and costly. ConditionExpressions are used to specify conditions for deleting items, but they do not optimize the process of deleting large numbers of items."",
    ""Use BatchWriteItem to empty all of the rows."": ""This choice is incorrect because BatchWriteItem is limited to 25 items per batch, which means it would require 40,000 separate batch operations to delete 1 million records. While BatchWriteItem is more efficient than individual DeleteItem calls, it is still not the most efficient or cost-effective method for clearing a large table."",
    ""Write a recursive function that scans and calls out DeleteItem."": ""This choice is incorrect because using a recursive function to scan and delete items is inefficient for large datasets. Scanning the table to find items to delete is costly and time-consuming, especially with 1 million records. Additionally, recursive functions can lead to stack overflow errors if not managed properly."",
    ""Create and then delete the table after the task has completed."": ""This choice is correct because creating a new table for each batch job and deleting it afterward is the most efficient and cost-effective method. Deleting a table automatically removes all its items, and creating a new table ensures it is empty for the next batch job. This approach avoids the overhead of deleting individual items and is supported by AWS best practices for temporary data storage.""
}
```"
"A company has an application that logs all information to Amazon S3. Whenever there is a new log file, an AWS Lambda function is invoked to process the log files. The code works, gathering all of the necessary information. However, when checking the Lambda function logs, duplicate entries with the same request ID are found. What is causing the duplicate entries?","[""The S3 bucket name was specified incorrectly."", ""The Lambda function failed, and the Lambda service retired the invocation with a delay."", ""There was an S3 outage, which caused duplicate entries of the sale log file."", ""The application stopped intermittently and then resumed.""]","[""The Lambda function failed, and the Lambda service retired the invocation with a delay.""]","```json
{
    ""The S3 bucket name was specified incorrectly."": ""This choice is incorrect because an incorrect S3 bucket name would result in the Lambda function not being triggered at all, as the event notification would not be set up correctly. The issue described involves duplicate entries with the same request ID, which indicates that the Lambda function is being invoked multiple times for the same event, not that it is failing to be invoked due to a configuration error."",
    ""The Lambda function failed, and the Lambda service retired the invocation with a delay."": ""This choice is correct because AWS Lambda has a built-in retry mechanism for asynchronous invocations, such as those triggered by S3 events. If the Lambda function fails during execution, AWS Lambda will automatically retry the invocation, which can lead to duplicate entries if the function logs data before failing. The same request ID would be used for retries, which explains the duplicate entries in the logs."",
    ""There was an S3 outage, which caused duplicate entries of the sale log file."": ""This choice is incorrect because an S3 outage would typically prevent log files from being written to S3 or prevent the Lambda function from being triggered, rather than causing duplicate entries. Duplicate entries with the same request ID are indicative of Lambda retries, not S3 outages."",
    ""The application stopped intermittently and then resumed."": ""This choice is incorrect because the described issue involves duplicate entries with the same request ID, which suggests that the same Lambda invocation was retried. An intermittent stop and resume of the application would not cause the same request ID to appear multiple times in the logs; it would result in separate invocations with different request IDs.""
}
```"
A company is providing services to many downstream consumers. Each consumer may connect to one or more services. This has resulted in a complex architecture that is difficult to manage and does not scale well. The company needs a single interface to manage these services to consumers. Which AWS service should be used to refactor this architecture?,"[""AWS Lambda."", ""AWS X-Ray."", ""Amazon SQS."", ""Amazon API Gateway.""]","[""Amazon API Gateway.""]","{""AWS Lambda."": ""AWS Lambda is a compute service that lets you run code without provisioning or managing servers. While it is excellent for executing backend code in response to events, it is not designed to provide a unified interface for multiple services. It does not inherently manage or simplify complex architectures involving multiple consumer services. Therefore, it is not the correct choice for providing a single interface to manage services."", ""AWS X-Ray."": ""AWS X-Ray is a service that helps with debugging and analyzing microservices applications by tracing requests as they travel through the application. It provides insights into performance bottlenecks and errors. However, it does not provide a single interface to manage services for consumers. Its purpose is more aligned with monitoring and debugging rather than simplifying service interfaces."", ""Amazon SQS."": ""Amazon SQS is a fully managed message queuing service that enables decoupling and scaling of microservices, distributed systems, and serverless applications. While it helps in managing communication between services, it does not provide a single interface for managing multiple services to consumers. It is more about message queuing and decoupling rather than providing a unified API interface."", ""Amazon API Gateway."": ""Amazon API Gateway is the correct choice because it is a fully managed service that makes it easy for developers to create, publish, maintain, monitor, and secure APIs at any scale. It provides a single entry point for multiple services, allowing the company to manage complex architectures more efficiently. API Gateway can handle all the tasks involved in accepting and processing up to hundreds of thousands of concurrent API calls, including traffic management, authorization and access control, monitoring, and API version management. This makes it ideal for providing a single interface to manage services to consumers.""}"
"A Developer is creating a serverless website with content that includes HTML files, images, videos, and JavaScript (client-side scripts). Which combination of services should the Developer use to create the website?","[""Amazon S3 and Amazon CloudFront."", ""Amazon EC2 and Amazon ElastiCache."", ""Amazon ECS and Redis."", ""AWS Lambda and Amazon API Gateway.""]","[""Amazon S3 and Amazon CloudFront.""]","```json
{
    ""Amazon S3 and Amazon CloudFront."": ""This choice is correct because Amazon S3 is an ideal service for hosting static content like HTML files, images, videos, and JavaScript due to its scalability, durability, and cost-effectiveness. Amazon CloudFront is a content delivery network (CDN) that can cache this content at edge locations around the world, reducing latency and improving load times for users. This combination is a best practice for serving static websites efficiently and is commonly used in real-world scenarios. AWS documentation supports this setup as a recommended architecture for static websites."",
    ""Amazon EC2 and Amazon ElastiCache."": ""This choice is incorrect because Amazon EC2 is a service for running virtual servers and is not necessary for hosting static content, which can be more efficiently served from Amazon S3. Amazon ElastiCache is used for caching data in-memory to improve application performance, which is not relevant for serving static website content. This combination would be more suitable for dynamic web applications rather than a static website."",
    ""Amazon ECS and Redis."": ""This choice is incorrect because Amazon ECS is a container orchestration service used to run and manage Docker containers, which is not needed for a static website. Redis, often used with ElastiCache, is an in-memory data structure store used for caching, which does not apply to serving static content. This setup would be overkill and unnecessarily complex for a simple static website."",
    ""AWS Lambda and Amazon API Gateway."": ""This choice is incorrect because AWS Lambda and Amazon API Gateway are typically used for building serverless APIs and backend services, not for serving static content. While they are powerful for creating dynamic, event-driven applications, they are not suitable for hosting static files like HTML, images, and JavaScript, which can be more efficiently served from S3 with CloudFront.""
}
```"
A Development team has pushed out 10 applications running on several Amazon EC2 instances. The Operations team is asking for a graphical representation of one key performance metric for each application. These metrics should be available on one screen for easy monitoring. Which steps should the Developer take to accomplish this using Amazon CloudWatch?,"[""Create a custom namespace with a unique metric name for each application."", ""Create a custom dimension with a unique metric name for each application."", ""Create a custom event with a unique metric name for each application."", ""Create a custom alarm with a unique metric name for each application.""]","[""Create a custom namespace with a unique metric name for each application.""]","```json
{
    ""Create a custom namespace with a unique metric name for each application."": ""This choice is correct because creating a custom namespace in Amazon CloudWatch allows you to define and organize your own metrics. By using a unique metric name for each application within this namespace, you can easily track and visualize key performance metrics for each application on a single CloudWatch dashboard. This approach aligns with AWS best practices for monitoring and managing application performance using CloudWatch."",
    ""Create a custom dimension with a unique metric name for each application."": ""This choice is incorrect because dimensions in CloudWatch are used to categorize metrics, not to create them. While dimensions can help filter and organize metrics, they are not used to define unique metrics for applications. Instead, dimensions are typically used to add context to metrics, such as specifying the instance type or environment."",
    ""Create a custom event with a unique metric name for each application."": ""This choice is incorrect because CloudWatch Events are used to respond to changes in your AWS environment, not to create or visualize metrics. Events can trigger actions based on changes or conditions, but they are not used for defining or displaying application performance metrics."",
    ""Create a custom alarm with a unique metric name for each application."": ""This choice is incorrect because CloudWatch Alarms are used to monitor metrics and trigger notifications or actions when certain thresholds are met. While alarms can be set on custom metrics, they are not used to create or visualize metrics on a dashboard. Alarms are more about alerting and responding to specific conditions rather than providing a graphical representation of metrics.""
}
```"
A company is creating an application that will require users to access AWS services and allow them to reset their own passwords. Which of the following would allow the company to manage users and authorization while allowing users to reset their own passwords?,"[""Amazon Cognito identify pools and AWS STS."", ""Amazon Cognito identity pools and AWS IAM."", ""Amazon Cognito user pools and AWS KMS."", ""Amazon Cognito user pools and identity pools.""]","[""Amazon Cognito user pools and identity pools.""]","```json
{
    ""Amazon Cognito identify pools and AWS STS."": ""This choice is incorrect because Amazon Cognito identity pools are used to grant users access to AWS resources, but they do not manage user authentication or password reset capabilities. AWS STS (Security Token Service) is used for temporary security credentials, not for user management or password reset. Therefore, this combination does not fulfill the requirement of allowing users to reset their own passwords."",
    ""Amazon Cognito identity pools and AWS IAM."": ""This choice is incorrect because while Amazon Cognito identity pools can be used to provide temporary AWS credentials for users to access AWS resources, they do not handle user authentication or password reset. AWS IAM is used for managing access to AWS resources but does not provide user management or password reset capabilities for application users. Thus, this combination does not meet the requirement for user password reset functionality."",
    ""Amazon Cognito user pools and AWS KMS."": ""This choice is incorrect because Amazon Cognito user pools are indeed used for user authentication and can handle password reset functionality. However, AWS KMS (Key Management Service) is used for encryption and key management, not for managing user identities or authorizations. Therefore, AWS KMS does not contribute to user management or password reset capabilities, making this combination unsuitable for the requirements."",
    ""Amazon Cognito user pools and identity pools."": ""This choice is correct because Amazon Cognito user pools provide user management and authentication capabilities, including the ability for users to reset their passwords. Identity pools, on the other hand, allow users to obtain temporary AWS credentials to access AWS services. This combination effectively manages user authentication and authorization while allowing users to reset their passwords, aligning with the requirements of the application.""
}
```"
"A company has three different environments: Development, QA, and Production. The company wants to deploy its code first in the Development environment, then QA, and then Production. Which AWS service can be used to meet this requirement?","[""Use AWS CodeCommit to create multiple repositories to deploy the application."", ""Use AWS CodeBuild to create, configure, and deploy multiple build application projects."", ""Use AWS Data Pipeline to create multiple data pipeline provisions to deploy the application."", ""Use AWS CodeDeploy to create multiple deployment groups.""]","[""Use AWS Data Pipeline to create multiple data pipeline provisions to deploy the application.""]","```json
{
    ""Use AWS CodeCommit to create multiple repositories to deploy the application."": ""Incorrect. AWS CodeCommit is a source control service that hosts Git repositories. It is used for storing and versioning source code, not for deploying applications across different environments. While you can use CodeCommit to manage different branches for Development, QA, and Production, it does not handle the deployment process itself."",
    ""Use AWS CodeBuild to create, configure, and deploy multiple build application projects."": ""Incorrect. AWS CodeBuild is a fully managed build service that compiles source code, runs tests, and produces software packages. While it can be part of a CI/CD pipeline, it does not manage deployments across environments like Development, QA, and Production. CodeBuild is typically used in conjunction with other services like CodePipeline and CodeDeploy for a complete CI/CD solution."",
    ""Use AWS Data Pipeline to create multiple data pipeline provisions to deploy the application."": ""Incorrect. AWS Data Pipeline is a web service that helps you process and move data between different AWS compute and storage services. It is not designed for application deployment. The correct service for managing deployments across different environments is AWS CodeDeploy, which allows you to automate code deployments to any instance, including EC2 instances and Lambda functions."",
    ""Use AWS CodeDeploy to create multiple deployment groups."": ""Correct. AWS CodeDeploy is a service that automates code deployments to various compute services like Amazon EC2, AWS Lambda, and on-premises servers. It allows you to create deployment groups for different environments such as Development, QA, and Production, ensuring that your application is deployed in a controlled and automated manner. This aligns with best practices for continuous deployment and environment management.""
}
```"
"A company uses Amazon DynamoDB for managing and tracking orders. The DynamoDB table is partitioned based on the order date. The company receives a huge increase in orders during a sales event, causing DynamoDB writes to throttle, and the consumed throughput is far below the provisioned throughput. According to AWS best practices, how can this issue be resolved with MINIMAL costs?","[""Create a new DynamoDB table for every order date."", ""Increase the read and write capacity units of the DynamoDB table."", ""Add a random number suffix to the partition key values."", ""Add a global secondary index to the DynamoDB table.""]","[""Increase the read and write capacity units of the DynamoDB table.""]","```json
{
    ""Create a new DynamoDB table for every order date."": ""This choice is incorrect because creating a new table for each order date would lead to unnecessary complexity and increased costs. Managing multiple tables would require additional overhead and could complicate data retrieval and management. AWS best practices suggest optimizing the existing table design rather than creating new tables unless absolutely necessary."",
    ""Increase the read and write capacity units of the DynamoDB table."": ""This choice is incorrect as a solution to the problem described. The issue is not with the provisioned throughput being too low, but with the partitioning strategy causing uneven distribution of traffic. Increasing capacity units would increase costs without addressing the root cause of throttling, which is the hot partition issue due to the partition key design."",
    ""Add a random number suffix to the partition key values."": ""This choice is correct because adding a random number suffix to the partition key values can help distribute the write load more evenly across partitions. This approach addresses the hot partition problem by ensuring that writes are spread across multiple partitions, thereby reducing throttling and making better use of the provisioned throughput. This method is cost-effective as it does not require increasing capacity units."",
    ""Add a global secondary index to the DynamoDB table."": ""This choice is incorrect because adding a global secondary index does not address the issue of write throttling due to hot partitions. Global secondary indexes are used to enable querying on non-primary key attributes and do not solve the problem of uneven write distribution across partitions. Implementing this would incur additional costs without resolving the root cause of the issue.""
}
```"
"A Development team currently supports an application that uses an in-memory store to save accumulated game results. Individual results are stored in a database. As part of migrating to AWS, the team needs to use automatic scaling. The team knows this will yield inconsistent results. Where should the team store these accumulated game results to BEST allow for consistent results without impacting performance?","[""Amazon S3."", ""Amazon RDS."", ""Amazon ElastiCache."", ""Amazon Kinesis.""]","[""Amazon ElastiCache.""]","```json
{
    ""Amazon S3."": ""Amazon S3 is a highly durable and scalable object storage service, but it is not suitable for storing in-memory data that requires low-latency access and frequent updates, such as accumulated game results. S3 is designed for storing large amounts of data that do not change frequently, making it inappropriate for this use case where performance and consistency are critical."",
    ""Amazon RDS."": ""Amazon RDS is a managed relational database service that provides automatic scaling and high availability. While it can store accumulated game results consistently, it may not offer the same low-latency performance as an in-memory store like ElastiCache. RDS is better suited for structured data that requires complex queries and transactions, not for high-speed caching needs."",
    ""Amazon ElastiCache."": ""Amazon ElastiCache is the correct choice because it is a managed in-memory data store service that supports both Redis and Memcached. It is designed for use cases requiring low-latency data access and high throughput, such as caching accumulated game results. ElastiCache can handle automatic scaling and provides consistent performance, making it ideal for this scenario."",
    ""Amazon Kinesis."": ""Amazon Kinesis is a service for real-time data streaming and processing, not for storing accumulated results. It is designed to handle large streams of data in real-time, such as log or event data, but it is not suitable for use as an in-memory store for accumulated results that require consistent and fast access.""
}
```"
"In a multi-container Docker environment in AWS Elastic Beanstalk, what is required to configure container instances in the environment?","[""An Amazon ECS task definition."", ""An Amazon ECS cluster."", ""A Dockerfile in an application package."", ""A CLI for Elastic Beanstalk.""]","[""An Amazon ECS task definition.""]","```json
{
    ""An Amazon ECS task definition."": ""Incorrect. In a multi-container Docker environment in AWS Elastic Beanstalk, you do not use an Amazon ECS task definition. Elastic Beanstalk uses a different configuration model where you provide a Dockerrun.aws.json file to define the containers and their configurations. ECS task definitions are specific to Amazon ECS, not Elastic Beanstalk."",
    ""An Amazon ECS cluster."": ""Incorrect. An Amazon ECS cluster is not required for configuring container instances in an AWS Elastic Beanstalk environment. Elastic Beanstalk abstracts the underlying infrastructure management, including the orchestration of Docker containers, which is different from the ECS service that requires an ECS cluster."",
    ""A Dockerfile in an application package."": ""Incorrect. While a Dockerfile can be used to build a Docker image, in the context of AWS Elastic Beanstalk's multi-container Docker environments, you typically use a Dockerrun.aws.json file to specify how to run multiple containers. A Dockerfile alone does not provide the necessary configuration for a multi-container setup in Elastic Beanstalk."",
    ""A CLI for Elastic Beanstalk."": ""Incorrect. While the AWS Elastic Beanstalk CLI (EB CLI) can be used to manage Elastic Beanstalk applications, it is not specifically required to configure container instances in a multi-container Docker environment. The configuration is primarily done through configuration files like Dockerrun.aws.json.""
}
```"
An application that runs on an Amazon EC2 instance needs to access and make API calls to multiple AWS services. What is the MOST secure way to provide access to the AWS services with MINIMAL management overhead?,"[""Use AWS KMS to store and retrieve credentials."", ""Use EC2 instance profiles."", ""Use AWS root user to make requests to the application."", ""Store and retrieve credentials from AWS CodeCommit.""]","[""Use EC2 instance profiles.""]","```json
{
    ""Use AWS KMS to store and retrieve credentials."": ""This choice is incorrect because AWS KMS is primarily used for encryption and managing cryptographic keys rather than storing and retrieving credentials. Using KMS for credentials would require additional management overhead and does not align with AWS best practices for securely managing access to AWS services."",
    ""Use EC2 instance profiles."": ""This choice is correct because EC2 instance profiles allow you to assign an IAM role to an EC2 instance, which grants the instance permissions to access AWS services securely without the need to manage credentials. This approach follows AWS best practices by using temporary security credentials and minimizes management overhead, as the credentials are automatically rotated and managed by AWS."",
    ""Use AWS root user to make requests to the application."": ""This choice is incorrect because using the AWS root user for application requests is against AWS best practices. The root user has unrestricted access to all resources in the account, and using it increases the risk of accidental or malicious actions. AWS recommends using IAM roles and policies to grant the least privilege necessary for applications."",
    ""Store and retrieve credentials from AWS CodeCommit."": ""This choice is incorrect because AWS CodeCommit is a source control service for hosting Git repositories, not a credential management service. Storing credentials in a source code repository is insecure and against best practices, as it can lead to unintentional exposure of sensitive information.""
}
```"
"A company maintains an application responsible for processing several thousand external callbacks each day. The company's System administrators want to know how many callbacks are being received on a rolling basis, and they want this data available for 10 days. The company also wants the ability to issue automated alerts if the number of callbacks exceeds the defined thresholds. What is the MOST cost-effective way to address the need to track and alert on these statistics?","[""Push callback data to an Amazon RDS database that can be queried to show historical data and to alert on exceeded thresholds."", ""Push callback data to AWS X-Ray and use AWS Lambda to query, display, and alert on exceeded thresholds."", ""Push callback data to Amazon Kinesis Data Streams and invoke an AWS Lambda function that stores data in Amazon DynamoDB and sends the required alerts."", ""Push callback data to Amazon CloudWatch as a custom metric and use the CloudWatch alerting mechanisms to alert System Administrators.""]","[""Push callback data to Amazon CloudWatch as a custom metric and use the CloudWatch alerting mechanisms to alert System Administrators.""]","```json
{
    ""Push callback data to an Amazon RDS database that can be queried to show historical data and to alert on exceeded thresholds."": ""This option is incorrect because using Amazon RDS for storing and querying callback data is not the most cost-effective solution for this use case. RDS is a relational database service that is better suited for structured data storage and complex queries, but it involves higher costs for storage, compute, and maintenance. Additionally, setting up alerts based on data in RDS would require additional custom logic and services, making it less efficient compared to native AWS services designed for monitoring and alerting."",
    ""Push callback data to AWS X-Ray and use AWS Lambda to query, display, and alert on exceeded thresholds."": ""This option is incorrect because AWS X-Ray is primarily used for tracing and debugging applications, providing insights into the performance of applications and microservices. It is not designed for tracking metrics or setting up alerts based on thresholds. Using X-Ray for this purpose would be an inappropriate use of the service and would require additional components to achieve the desired functionality, making it less cost-effective."",
    ""Push callback data to Amazon Kinesis Data Streams and invoke an AWS Lambda function that stores data in Amazon DynamoDB and sends the required alerts."": ""This option is incorrect because while Kinesis Data Streams is suitable for processing real-time streaming data, it introduces unnecessary complexity and cost for this specific use case. The requirement is to track and alert on callback data, which can be more efficiently handled by CloudWatch custom metrics. Using Kinesis and DynamoDB involves additional setup and costs that are not justified for the task of simple metric tracking and alerting."",
    ""Push callback data to Amazon CloudWatch as a custom metric and use the CloudWatch alerting mechanisms to alert System Administrators."": ""This option is correct because Amazon CloudWatch is specifically designed for monitoring and alerting on metrics. By pushing callback data as a custom metric to CloudWatch, the company can easily track the number of callbacks and set up alarms to notify administrators when thresholds are exceeded. CloudWatch provides a cost-effective and straightforward solution for this use case, with built-in capabilities for storing metrics and setting up alerts. This aligns with AWS best practices for monitoring and alerting.""
}
```"
A company has a website that is developed in PHP and WordPress and is launched using AWS Elastic Beanstalk. There is a new version of the website that needs to be deployed in the Elastic Beanstalk environment. The company cannot tolerate having the website offline if an update fails. Deployments must have minimal impact and rollback as soon as possible. What deployment method should be used?,"[""All at once."", ""Rolling."", ""Snapshots."", ""Immutable.""]","[""Immutable.""]","{""All at once."": ""The 'All at once' deployment method updates all instances simultaneously. This method is fast but risky because if the deployment fails, the entire application could be offline until a rollback is performed. This does not meet the requirement of minimal impact and immediate rollback, as the website could be offline if the update fails."", ""Rolling."": ""The 'Rolling' deployment method updates instances in batches. While this reduces downtime compared to 'All at once', it still carries the risk of partial downtime if a batch fails. Rollbacks can be more complex because some instances may already be updated while others are not, which does not fully meet the requirement of minimal impact and immediate rollback."", ""Snapshots."": ""Snapshots are typically used for backing up data, such as EBS volumes, and are not a deployment method in Elastic Beanstalk. This choice is incorrect because it does not relate to the deployment of application versions and does not address the requirement for minimal impact and immediate rollback."", ""Immutable."": ""The 'Immutable' deployment method creates a new set of instances with the new version of the application. If the deployment is successful, traffic is routed to the new instances. If it fails, the old instances remain untouched, allowing for immediate rollback with no downtime. This method meets the requirement for minimal impact and quick rollback, making it the best choice for the scenario described. AWS documentation recommends immutable deployments for environments where stability and rollback capability are critical.""}"
"A company has a multi-tiered web application on AWS. During a recent spike in traffic, one of the primary relational databases on Amazon RDS could not serve all the traffic. Some read queries for repeatedly accessed items failed, so users received error messages. What can be done to minimize the impact on database read queries MOST efficiently during future traffic spikes?","[""Use Amazon S3 to cache database query results."", ""Use Amazon RDS as a custom origin for Amazon CloudFront."", ""Use local storage and memory on Amazon EC2 instances to cache data."", ""Use Amazon ElastiCache in front of the primary database to cache data.""]","[""Use Amazon ElastiCache in front of the primary database to cache data.""]","```json
{
    ""Use Amazon S3 to cache database query results."": ""This choice is incorrect because Amazon S3 is primarily used for object storage and not as a caching layer for database queries. While S3 can store static content, it does not provide the in-memory caching capabilities needed to efficiently serve dynamic database query results. Caching solutions like Amazon ElastiCache are designed specifically for this purpose, providing fast, in-memory data storage."",
    ""Use Amazon RDS as a custom origin for Amazon CloudFront."": ""This choice is incorrect because Amazon CloudFront is a content delivery network (CDN) designed to cache and deliver static content, not dynamic database queries. Using RDS as a custom origin for CloudFront would not address the issue of caching database read queries. CloudFront is not suitable for caching dynamic database responses, which is better handled by a caching service like Amazon ElastiCache."",
    ""Use local storage and memory on Amazon EC2 instances to cache data."": ""This choice is incorrect because while using local storage and memory on EC2 instances can provide caching, it is not the most efficient or scalable solution. Managing cache on individual EC2 instances can lead to complexity and inconsistency, especially in a multi-tiered application. Amazon ElastiCache provides a managed, scalable, and distributed caching solution that is more efficient for handling spikes in database read traffic."",
    ""Use Amazon ElastiCache in front of the primary database to cache data."": ""This choice is correct because Amazon ElastiCache is a managed service that provides in-memory data caching, which can significantly reduce the load on the primary database by caching frequently accessed data. This reduces latency and improves application performance during traffic spikes. ElastiCache supports Redis and Memcached, both of which are popular caching engines for improving read performance and minimizing database load.""
}
```"
"A Developer must build an application that uses Amazon DynamoDB. The requirements state that the items being stored in the DynamoDB table will be 7KB in size and that reads must be strongly consistent. The maximum read rate is 3 items per second, and the maximum write rate is 10 items per second. How should the Developer size the DynamoDB table to meet these requirements?","[""Read: 3 read capacity unitsWrite: 70 write capacity units."", ""Read: 6 read capacity unitsWrite: 70 write capacity units."", ""Read: 6 read capacity unitsWrite: 10 write capacity units."", ""Read: 3 read capacity unitsWrite: 10 write capacity units.""]","[""Read: 6 read capacity unitsWrite: 70 write capacity units.""]","```json
{
    ""Read: 3 read capacity unitsWrite: 70 write capacity units."": ""This choice is incorrect because it underestimates the read capacity units needed for strongly consistent reads. Each strongly consistent read of an item up to 4KB in size requires 1 read capacity unit. Since the items are 7KB, each read requires 2 read capacity units (7KB / 4KB = 1.75, rounded up to 2). Therefore, for 3 reads per second, 6 read capacity units are needed (3 reads * 2 units per read). The write capacity is correctly calculated as each write of an item up to 1KB requires 1 write capacity unit, so for 10 writes per second of 7KB items, 70 write capacity units are needed (10 writes * 7 units per write)."",
    ""Read: 6 read capacity unitsWrite: 70 write capacity units."": ""This choice is correct. For strongly consistent reads, each read of an item up to 4KB requires 1 read capacity unit. Since the items are 7KB, each read requires 2 read capacity units (7KB / 4KB = 1.75, rounded up to 2). Therefore, for 3 reads per second, 6 read capacity units are needed (3 reads * 2 units per read). For writes, each write of an item up to 1KB requires 1 write capacity unit, so for 10 writes per second of 7KB items, 70 write capacity units are needed (10 writes * 7 units per write). This aligns with AWS best practices for provisioning DynamoDB capacity."",
    ""Read: 6 read capacity unitsWrite: 10 write capacity units."": ""This choice is incorrect because it underestimates the write capacity units needed. While the read capacity is correctly calculated for strongly consistent reads (6 units for 3 reads per second of 7KB items), the write capacity is insufficient. Each write of an item up to 1KB requires 1 write capacity unit, so for 10 writes per second of 7KB items, 70 write capacity units are needed (10 writes * 7 units per write)."",
    ""Read: 3 read capacity unitsWrite: 10 write capacity units."": ""This choice is incorrect because it underestimates both the read and write capacity units needed. For strongly consistent reads, each read of an item up to 4KB requires 1 read capacity unit, so for 3 reads per second of 7KB items, 6 read capacity units are needed (3 reads * 2 units per read). For writes, each write of an item up to 1KB requires 1 write capacity unit, so for 10 writes per second of 7KB items, 70 write capacity units are needed (10 writes * 7 units per write).""
}
```"
"A Developer is creating an AWS Lambda function to process a stream of data from an Amazon Kinesis Data Stream. When the Lambda function parses the data and encounters a missing field, it exits the function with an error. The function is generating duplicate records from the Kinesis stream. When the Developer looks at the stream output without the Lambda function, there are no duplicate records. What is the reason for the duplicates?","[""The Lambda function did not advance the Kinesis stream pointer to the next record after the error."", ""The Lambda event source used asynchronous invocation, resulting in duplicate records."", ""The Lambda function did not handle the error, and the Lambda service attempted to reprocess the data."", ""The Lambda function is not keeping up with the amount of data coming from the stream.""]","[""The Lambda function did not advance the Kinesis stream pointer to the next record after the error.""]","```json
{
    ""The Lambda function did not advance the Kinesis stream pointer to the next record after the error."": ""This choice is correct. When a Lambda function processes records from a Kinesis stream, it needs to explicitly checkpoint the progress by advancing the stream pointer. If the function exits with an error and does not advance the pointer, the same records will be reprocessed when the function is retried, leading to duplicates. This behavior is consistent with the at-least-once delivery model of Kinesis, where records can be delivered more than once if not properly checkpointed."",
    ""The Lambda event source used asynchronous invocation, resulting in duplicate records."": ""This choice is incorrect. Kinesis Data Streams use synchronous invocation when triggering Lambda functions, not asynchronous. Synchronous invocation means the function processes records in order and must handle errors and retries, whereas asynchronous invocation would not directly cause duplicates in this scenario."",
    ""The Lambda function did not handle the error, and the Lambda service attempted to reprocess the data."": ""This choice is partially correct but not the best answer. While it is true that unhandled errors in a Lambda function can lead to retries, the specific issue here is the lack of advancing the stream pointer. Proper error handling should include logic to checkpoint progress even in the event of an error to prevent reprocessing of the same records."",
    ""The Lambda function is not keeping up with the amount of data coming from the stream."": ""This choice is incorrect. If the Lambda function were not keeping up with the data, it would result in increased latency or throttling, not necessarily duplicate records. The issue of duplicates is specifically related to how the stream pointer is managed in the event of an error.""
}
```"
"A company is developing an application that will run on several Amazon EC2 instances in an Auto Scaling group and can access a database running on Amazon EC2. The application needs to store secrets required to connect to the database. The application must allow for periodic secret rotation, and there should be no changes to the application when a secret changes. What is the SAFEST way to meet these requirements?","[""Associate an IAM role to the EC2 instance where the application is running with permission to access the database."", ""Use AWS Systems Manager Parameter Store with the SecureString data type to store secrets."", ""Configure the application to store secrets in Amazon S3 object metadata."", ""Hard code the database secrets in the application code itself.""]","[""Use AWS Systems Manager Parameter Store with the SecureString data type to store secrets.""]","```json
{
    ""Associate an IAM role to the EC2 instance where the application is running with permission to access the database."": ""This choice is incorrect because while IAM roles can provide permissions for EC2 instances to access AWS services, they do not handle secret management or rotation. IAM roles are used for granting permissions, not for storing or rotating secrets. The requirement is to manage secrets securely with periodic rotation, which IAM roles do not address."",
    ""Use AWS Systems Manager Parameter Store with the SecureString data type to store secrets."": ""This choice is correct because AWS Systems Manager Parameter Store with the SecureString data type is designed for secure storage of sensitive data, such as database credentials. It supports encryption using AWS KMS and allows for seamless secret rotation without requiring changes to the application. This aligns with best practices for secret management, ensuring security and flexibility."",
    ""Configure the application to store secrets in Amazon S3 object metadata."": ""This choice is incorrect because storing secrets in Amazon S3 object metadata is not a secure method for managing sensitive information. S3 is not designed for secret management and does not provide built-in mechanisms for secret rotation or encryption specific to secrets, making it an unsuitable choice for this use case."",
    ""Hard code the database secrets in the application code itself."": ""This choice is incorrect because hard coding secrets in application code is a security risk. It makes secrets difficult to rotate and exposes them to anyone with access to the codebase. This approach violates security best practices and can lead to potential data breaches if the code is compromised.""
}
```"
"A Developer writes an AWS Lambda function and uploads the code in a `.ZIP` file to Amazon S3. The Developer makes changes to the code and uploads a new `.ZIP` file to Amazon S3. However, Lambda executes the earlier code. How can the Developer fix this in the LEAST disruptive way?","[""Create another Lambda function and specify the new `.ZIP` file."", ""Call the `update-function-code` API."", ""Remove the earlier `.ZIP` file first, then add the new `.ZIP` file."", ""Call the `create-alias` API.""]","[""Call the `update-function-code` API.""]","```json
{
    ""Create another Lambda function and specify the new `.ZIP` file."": ""This choice is incorrect because creating a new Lambda function is not necessary to update the code of an existing function. It would be more disruptive as it involves creating a new function, potentially changing the function's ARN and requiring updates to any resources or configurations that reference the original function. This approach is not efficient or aligned with best practices for updating Lambda function code."",
    ""Call the `update-function-code` API."": ""This choice is correct because the `update-function-code` API is specifically designed to update the code of an existing Lambda function. By calling this API, the developer can specify the new `.ZIP` file, ensuring that the Lambda function uses the updated code. This is the least disruptive method as it does not require creating a new function or altering existing configurations. According to AWS best practices, using the `update-function-code` API is the standard way to update Lambda function code."",
    ""Remove the earlier `.ZIP` file first, then add the new `.ZIP` file."": ""This choice is incorrect because removing the earlier `.ZIP` file from S3 does not automatically update the Lambda function's code. Lambda functions are not directly linked to S3 files; instead, they require an explicit update using the `update-function-code` API to change the code. Simply managing files in S3 does not affect the code that Lambda executes."",
    ""Call the `create-alias` API."": ""This choice is incorrect because the `create-alias` API is used to create an alias for a specific version of a Lambda function, not to update the function's code. While aliases can be useful for managing different versions of a function, they do not address the issue of updating the function's code to a new version. The problem described requires updating the code, which is done through the `update-function-code` API.""
}
```"
An AWS Lambda function must read data from an Amazon RDS MySQL database in a VPC and also reach a public endpoint over the internet to get additional data. Which steps must be taken to allow the function to access both the RDS resource and the public endpoint? (Select TWO),"[""Modify the default configuration for the Lambda function to associate it with an Amazon VPC private subnet."", ""Modify the default network access control list to allow outbound traffic."", ""Add a NAT Gateway to the VPC."", ""Modify the default configuration of the Lambda function to associate it with a VPC public subnet."", ""Add an environmental variable to the Lambda function to allow outbound internet access.""]","[""Modify the default configuration for the Lambda function to associate it with an Amazon VPC private subnet."", ""Add a NAT Gateway to the VPC.""]","```json
{
    ""Modify the default configuration for the Lambda function to associate it with an Amazon VPC private subnet."": ""This choice is correct because to access an RDS instance within a VPC, the Lambda function must be associated with a subnet in the same VPC. Using a private subnet ensures that the Lambda function can securely access the RDS instance without exposing it to the internet."",
    ""Modify the default network access control list to allow outbound traffic."": ""This choice is incorrect because while network ACLs can control traffic flow, they are not necessary for allowing outbound internet access from a private subnet. The key requirement for internet access from a private subnet is a NAT Gateway, not modifying network ACLs."",
    ""Add a NAT Gateway to the VPC."": ""This choice is correct because a NAT Gateway is required to allow resources in a private subnet to access the internet. Since the Lambda function needs to reach a public endpoint, the NAT Gateway will facilitate this outbound internet access while keeping the Lambda function in a private subnet."",
    ""Modify the default configuration of the Lambda function to associate it with a VPC public subnet."": ""This choice is incorrect because associating the Lambda function with a public subnet is not necessary and can expose the function to the internet, which is not a best practice for security. The correct approach is to use a private subnet with a NAT Gateway for internet access."",
    ""Add an environmental variable to the Lambda function to allow outbound internet access."": ""This choice is incorrect because environment variables in Lambda are used to pass configuration settings to the function, not to control network access. Outbound internet access is managed by the network configuration, specifically through a NAT Gateway for private subnets.""
}
```"
"A Developer has been asked to make changes to the source code of an AWS Lambda function. The function is managed using an AWS CloudFormation template. The template is configured to load the source code from an Amazon S3 bucket. The Developer manually created a `.ZIP` file deployment package containing the changes and put the file into the correct location on Amazon S3. When the function is invoked, the code changes have not been applied. What step is required to update the function with the changes?","[""Delete the `.ZIP` file on S3, and re-upload by using a different object key name."", ""Update the CloudFormation stack with the correct values for the function code properties S3Bucket, S3Key, or S3ObjectVersion."", ""Ensure that the function source code is base64-encoded before uploading the deployment package to S3."", ""Modify the execution role of the Lambda function to allow S3 access permission to the deployment package `.ZIP` file.""]","[""Update the CloudFormation stack with the correct values for the function code properties S3Bucket, S3Key, or S3ObjectVersion.""]","```json
{
    ""Delete the `.ZIP` file on S3, and re-upload by using a different object key name."": ""This choice is incorrect because simply changing the object key name in S3 does not automatically update the Lambda function. The CloudFormation stack needs to be updated to point to the new object key. CloudFormation manages resources as defined in the template, so any changes to the source code location must be reflected in the stack configuration."",
    ""Update the CloudFormation stack with the correct values for the function code properties S3Bucket, S3Key, or S3ObjectVersion."": ""This choice is correct because AWS CloudFormation manages resources based on the template configuration. To apply changes to the Lambda function code, the CloudFormation stack must be updated with the new S3 location details. This ensures that the Lambda function uses the updated code from the specified S3 bucket and key. This aligns with AWS best practices for managing infrastructure as code."",
    ""Ensure that the function source code is base64-encoded before uploading the deployment package to S3."": ""This choice is incorrect because AWS Lambda does not require the deployment package to be base64-encoded when uploading to S3. The deployment package should be a `.ZIP` file containing the function code and dependencies, and it is directly referenced by the Lambda function through CloudFormation."",
    ""Modify the execution role of the Lambda function to allow S3 access permission to the deployment package `.ZIP` file."": ""This choice is incorrect because the issue is not related to permissions. The Lambda service itself, not the execution role, needs access to the S3 bucket to retrieve the deployment package. The problem described is about updating the function with new code, which requires updating the CloudFormation stack, not modifying the execution role.""
}
```"
A Developer wants to enable AWS X-Ray for a secure application that runs in an Amazon ECS environment. What combination of steps will enable X-Ray? (Select THREE),"[""Create a Docker image that runs the X-Ray daemon."", ""Add instrumentation to the application code for X-Ray."", ""Install the X-Ray daemon on the underlying EC2 instance."", ""Configure and use an IAM EC2 instance role."", ""Register the application with X-Ray."", ""Configure and use an IAM role for tasks.""]","[""Add instrumentation to the application code for X-Ray."", ""Install the X-Ray daemon on the underlying EC2 instance."", ""Configure and use an IAM EC2 instance role.""]","```json
{
    ""Create a Docker image that runs the X-Ray daemon."": ""Incorrect. While creating a Docker image with the X-Ray daemon is a valid approach, it is not necessary if you are installing the X-Ray daemon on the underlying EC2 instance or using the ECS-optimized AMI, which already includes the X-Ray daemon. This choice is not required if the daemon is managed separately."",
    ""Add instrumentation to the application code for X-Ray."": ""Correct. Instrumenting the application code is essential for using AWS X-Ray. This involves adding X-Ray SDKs to your application code, which allows the application to send trace data to X-Ray. This step is crucial for capturing and sending traces from your application."",
    ""Install the X-Ray daemon on the underlying EC2 instance."": ""Correct. The X-Ray daemon is necessary to buffer and send trace data to the X-Ray service. In an ECS environment, the daemon can be run as a sidecar container or installed on the underlying EC2 instance if you are using EC2 launch type. This is a critical step for enabling X-Ray."",
    ""Configure and use an IAM EC2 instance role."": ""Incorrect. This option is not relevant in the context of ECS tasks. Instead, IAM roles for tasks should be used to grant permissions to the containers running in ECS. EC2 instance roles are used for EC2 instances, not ECS tasks."",
    ""Register the application with X-Ray."": ""Incorrect. There is no need to 'register' an application with X-Ray. Instead, you need to ensure that the application is instrumented and that the X-Ray daemon is running to collect and send trace data."",
    ""Configure and use an IAM role for tasks."": ""Correct. In an ECS environment, you should use an IAM role for tasks to grant the necessary permissions for the X-Ray daemon and the application to send data to X-Ray. This is a best practice for managing permissions in ECS.""
}
```"
"A Developer is designing a new application that uses Amazon S3. To satisfy compliance requirements, the Developer must encrypt the data at rest. How can the Developer accomplish this?","[""Use `s3:x-amz-acl` as a condition in the S3 bucket policy."", ""Use Amazon RDS with default encryption."", ""Use `aws:SecureTransport` as a condition in the S3 bucket policy."", ""Turn on S3 default encryption for the S3 bucket.""]","[""Turn on S3 default encryption for the S3 bucket.""]","```json
{
    ""Use `s3:x-amz-acl` as a condition in the S3 bucket policy."": ""This choice is incorrect because `s3:x-amz-acl` is used to control access permissions for objects in an S3 bucket, not for encrypting data at rest. It specifies the access control list (ACL) permissions that can be applied to objects, which is unrelated to encryption."",
    ""Use Amazon RDS with default encryption."": ""This choice is incorrect because Amazon RDS is a relational database service, not a storage service like Amazon S3. While RDS can be encrypted, it does not apply to S3 buckets. The question specifically asks about encrypting data in Amazon S3, so using RDS is irrelevant."",
    ""Use `aws:SecureTransport` as a condition in the S3 bucket policy."": ""This choice is incorrect because `aws:SecureTransport` is used to enforce the use of secure connections (HTTPS) for data in transit, not for encrypting data at rest. It ensures that data is securely transmitted to and from S3, but does not affect how data is stored."",
    ""Turn on S3 default encryption for the S3 bucket."": ""This choice is correct because enabling S3 default encryption ensures that all new objects stored in the bucket are automatically encrypted at rest. This satisfies compliance requirements for data encryption. AWS provides several encryption options, such as SSE-S3, SSE-KMS, and SSE-C, which can be set as default encryption methods for the bucket.""
}
```"
An AWS Elastic Beanstalk application needs to be deployed in multiple regions and requires a different Amazon Machine Image (AMI) in each region. Which AWS CloudFormation template key can be used to specify the correct AMI for each region?,"[""Parameters."", ""Outputs."", ""Mappings."", ""Resources.""]","[""Mappings.""]","```json
{
    ""Parameters."": ""Parameters in AWS CloudFormation are used to allow users to input custom values to their templates each time they create or update a stack. While Parameters can be used to specify AMIs, they are not region-specific by themselves. You would need to manually input the correct AMI for each region, which is not efficient for deploying in multiple regions."",
    ""Outputs."": ""Outputs are used in AWS CloudFormation to declare output values that can be imported into other stacks or used for informational purposes. They do not help in selecting different AMIs based on regions. Outputs are not used to define or select resources within the template."",
    ""Mappings."": ""Mappings in AWS CloudFormation are used to create a lookup table that can be referenced in your template to specify values based on specific keys, such as regions. This is the correct choice because it allows you to map each region to its respective AMI, enabling the template to automatically select the correct AMI based on the region where the stack is deployed. This is efficient and aligns with best practices for multi-region deployments."",
    ""Resources."": ""Resources in AWS CloudFormation define the AWS resources that you want to create and configure. While you specify AMIs within the Resources section when defining EC2 instances, Resources themselves do not provide a mechanism to select different AMIs based on region. This selection logic is better handled by Mappings.""
}
```"
A Developer wants to find a list of items in a global secondary index from an Amazon DynamoDB table. Which DynamoDB API call can the Developer use in order to consume the LEAST number of read capacity units?,"[""Scan operation using eventually-consistent reads."", ""Query operation using strongly-consistent reads."", ""Query operation using eventually-consistent reads."", ""Scan operation using strongly-consistent reads.""]","[""Query operation using eventually-consistent reads.""]","```json
{
    ""Scan operation using eventually-consistent reads."": ""This choice is incorrect because a Scan operation reads every item in the table or index, which consumes more read capacity units than a Query operation. Even though eventually-consistent reads consume fewer read capacity units than strongly-consistent reads, the Scan operation is less efficient compared to Query when you need to retrieve specific items based on a key or index."",
    ""Query operation using strongly-consistent reads."": ""This choice is incorrect because strongly-consistent reads consume more read capacity units than eventually-consistent reads. While a Query operation is more efficient than a Scan operation for retrieving items from a global secondary index, using strongly-consistent reads increases the read capacity units consumed."",
    ""Query operation using eventually-consistent reads."": ""This choice is correct because a Query operation is more efficient than a Scan operation for retrieving items from a global secondary index, as it retrieves only the items that match the query criteria. Additionally, using eventually-consistent reads consumes fewer read capacity units than strongly-consistent reads, making it the optimal choice for consuming the least number of read capacity units."",
    ""Scan operation using strongly-consistent reads."": ""This choice is incorrect because a Scan operation reads every item in the table or index, which is less efficient and consumes more read capacity units than a Query operation. Furthermore, using strongly-consistent reads increases the read capacity units consumed, making this the least efficient option.""
}
```"
"A Developer has published an update to an application that is served to a global user base using Amazon CloudFront. After deploying the application, users are not able to see the updated changes. How can the Developer resolve this issue?","[""Remove the origin from the CloudFront configuration and add it again."", ""Disable forwarding of query strings and request headers from the CloudFront distribution configuration."", ""Invalidate all the application objects from the edge caches."", ""Disable the CloudFront distribution and enable it again to update all the edge locations.""]","[""Invalidate all the application objects from the edge caches.""]","```json
{
    ""Remove the origin from the CloudFront configuration and add it again."": ""This choice is incorrect because removing and re-adding the origin does not affect the cached content at the edge locations. The issue is with the cached content not being updated, and this action does not address that problem. Removing and re-adding the origin would be unnecessarily disruptive and does not guarantee that the edge caches will be updated with the new content."",
    ""Disable forwarding of query strings and request headers from the CloudFront distribution configuration."": ""This choice is incorrect because forwarding query strings and request headers is related to how requests are handled and routed to the origin, not how cached content is managed. Disabling these options would not force the edge locations to update their cached content, which is the root cause of the problem."",
    ""Invalidate all the application objects from the edge caches."": ""This choice is correct because invalidating objects in CloudFront forces the edge locations to fetch the latest version of the content from the origin. This is a common practice when updates are made to content that is cached by CloudFront, ensuring that users receive the most recent version. AWS documentation recommends using invalidation to update cached content across all edge locations."",
    ""Disable the CloudFront distribution and enable it again to update all the edge locations."": ""This choice is incorrect because disabling and re-enabling a CloudFront distribution is not a recommended method for updating cached content. This action would cause downtime and is not necessary to refresh the cache. Invalidation is the appropriate and efficient method to ensure that edge locations serve the updated content.""
}
```"
A Developer must deploy a new AWS Lambda function using an AWS CloudFormation template. Which procedures will deploy a Lambda function? (Select TWO),"[""Upload the code to an AWS CodeCommit repository, then add a reference to it in an `AWS::Lambda::Function` resource in the template."", ""Create an `AWS::Lambda::Function` resource in the template, then write the code directly inside the CloudFormation template."", ""Upload a `.ZIP` file containing the function code to Amazon S3, then add a reference to it in an `AWS::Lambda::Function` resource in the template."", ""Upload a `.ZIP` file to AWS CloudFormation containing the function code, then add a reference to it in an `AWS::Lambda::Function` resource in the template."", ""Upload the function code to a private Git repository, then add a reference to it in an `AWS::Lambda::Function` resource in the template.""]","[""Create an `AWS::Lambda::Function` resource in the template, then write the code directly inside the CloudFormation template."", ""Upload a `.ZIP` file containing the function code to Amazon S3, then add a reference to it in an `AWS::Lambda::Function` resource in the template.""]","```json
{
    ""Upload the code to an AWS CodeCommit repository, then add a reference to it in an `AWS::Lambda::Function` resource in the template."": ""This choice is incorrect. AWS CloudFormation does not support directly referencing code stored in AWS CodeCommit repositories for Lambda functions. Instead, the code must be stored in an S3 bucket or written directly in the template."",
    ""Create an `AWS::Lambda::Function` resource in the template, then write the code directly inside the CloudFormation template."": ""This choice is correct. AWS CloudFormation allows you to define a Lambda function by specifying the code directly in the template using the 'ZipFile' property. This is suitable for small functions and is a valid approach for deploying a Lambda function."",
    ""Upload a `.ZIP` file containing the function code to Amazon S3, then add a reference to it in an `AWS::Lambda::Function` resource in the template."": ""This choice is correct. A common practice for deploying Lambda functions via CloudFormation is to upload the function code as a `.ZIP` file to an S3 bucket. The CloudFormation template can then reference this S3 object using the 'Code' property of the 'AWS::Lambda::Function' resource, which is a recommended best practice for larger functions."",
    ""Upload a `.ZIP` file to AWS CloudFormation containing the function code, then add a reference to it in an `AWS::Lambda::Function` resource in the template."": ""This choice is incorrect. AWS CloudFormation does not allow you to upload `.ZIP` files directly to it. Instead, the `.ZIP` file must be stored in an S3 bucket, and the template should reference the S3 location."",
    ""Upload the function code to a private Git repository, then add a reference to it in an `AWS::Lambda::Function` resource in the template."": ""This choice is incorrect. AWS CloudFormation does not support referencing code stored in private Git repositories for Lambda functions. The code must be stored in an S3 bucket or included directly in the template.""
}
```"
How should custom libraries be utilized in AWS Lambda?,"[""Host the library on Amazon S3 and reference to it from the Lambda function."", ""Install the library locally and upload a `ZIP` file of the Lambda function."", ""Import the necessary Lambda blueprint when creating the function."", ""Modify the function runtime to include the necessary library.""]","[""Install the library locally and upload a `ZIP` file of the Lambda function.""]","```json
{
    ""Host the library on Amazon S3 and reference to it from the Lambda function."": ""Incorrect. AWS Lambda does not natively support referencing libraries directly from Amazon S3. While you can store libraries on S3, you must still download and include them in your deployment package. Lambda requires all dependencies to be included in the deployment package or layer, not dynamically referenced from S3."",
    ""Install the library locally and upload a `ZIP` file of the Lambda function."": ""Correct. This is the recommended approach for including custom libraries in AWS Lambda. You install the library locally in the same directory as your Lambda function code, then package everything into a ZIP file and upload it to Lambda. This ensures that all dependencies are available at runtime. This method is supported by AWS documentation and is a common practice for deploying Lambda functions with external dependencies."",
    ""Import the necessary Lambda blueprint when creating the function."": ""Incorrect. Lambda blueprints are templates that provide a starting point for your function code, but they do not include custom libraries. Blueprints are useful for creating new functions with predefined code and settings, but they do not solve the problem of including external libraries."",
    ""Modify the function runtime to include the necessary library."": ""Incorrect. You cannot modify the AWS Lambda runtime environment directly to include additional libraries. Lambda runtimes are managed by AWS and are immutable. To include custom libraries, you must package them with your function code or use Lambda layers to add additional libraries.""
}
```"
A company needs to secure its existing website running behind an Elastic Load Balancer. The website's Amazon EC2 instances are CPU-constrained. What should be done to secure the website while not increasing the CPU load on the EC2 web servers? (Select TWO),"[""Configure an Elastic Load Balancer with SSL pass-through."", ""Configure SSL certificates on an Elastic Load Balancer."", ""Configure an Elastic Load Balancer with a Loadable Storage System."", ""Install SSL certificates on the EC2 instances."", ""Configure an Elastic Load Balancer with SSL termination.""]","[""Configure SSL certificates on an Elastic Load Balancer."", ""Configure an Elastic Load Balancer with SSL termination.""]","```json
{
    ""Configure an Elastic Load Balancer with SSL pass-through."": ""Incorrect. SSL pass-through means that the SSL connection is terminated at the EC2 instances, which requires the instances to handle the SSL decryption. This would increase the CPU load on the EC2 instances, which is not desirable since they are already CPU-constrained."",
    ""Configure SSL certificates on an Elastic Load Balancer."": ""Correct. By configuring SSL certificates on the Elastic Load Balancer, the SSL termination occurs at the load balancer. This offloads the SSL decryption process from the EC2 instances, reducing their CPU load. This is a common practice to secure websites without impacting the performance of backend instances."",
    ""Configure an Elastic Load Balancer with a Loadable Storage System."": ""Incorrect. This option does not relate to securing a website or handling SSL certificates. Loadable Storage System is not a relevant concept in the context of Elastic Load Balancers or securing web traffic."",
    ""Install SSL certificates on the EC2 instances."": ""Incorrect. Installing SSL certificates on the EC2 instances would require the instances to handle SSL decryption, increasing their CPU load. This contradicts the requirement to avoid increasing the CPU load on the EC2 web servers."",
    ""Configure an Elastic Load Balancer with SSL termination."": ""Correct. SSL termination at the Elastic Load Balancer means that the SSL decryption is handled by the load balancer, not the EC2 instances. This reduces the CPU load on the instances and secures the website, aligning with AWS best practices for load balancing and security.""
}
```"
A Developer is writing an imaging micro service on AWS Lambda. The service is dependent on several libraries that are not available in the Lambda runtime environment. Which strategy should the Developer follow to create the Lambda deployment package?,"[""Create a `ZIP` file with the source code and all dependent libraries."", ""Create a `ZIP` file with the source code and a script that installs the dependent libraries at runtime."", ""Create a `ZIP` file with the source code. Stage the dependent libraries on an Amazon S3 bucket indicated by the Lambda environment variable LD_LIBRARY_PATH."", ""Create a `ZIP` file with the source code and a buildspec.yaml file that installs the dependent libraries on AWS Lambda.""]","[""Create a `ZIP` file with the source code and all dependent libraries.""]","```json
{
    ""Create a `ZIP` file with the source code and all dependent libraries."": ""This is the correct choice. AWS Lambda requires that all code and dependencies be included in a deployment package. By creating a ZIP file with both the source code and all dependent libraries, the developer ensures that the Lambda function has everything it needs to execute without relying on external installations or configurations. This aligns with AWS best practices for deploying Lambda functions, which recommend packaging all dependencies together to avoid runtime errors and improve performance."",
    ""Create a `ZIP` file with the source code and a script that installs the dependent libraries at runtime."": ""This is incorrect because installing libraries at runtime can significantly increase the function's cold start time and lead to timeouts or performance issues. AWS Lambda functions are designed to be lightweight and should have all dependencies packaged together to ensure fast execution. Relying on runtime installations is not a best practice as it introduces unnecessary complexity and potential points of failure."",
    ""Create a `ZIP` file with the source code. Stage the dependent libraries on an Amazon S3 bucket indicated by the Lambda environment variable LD_LIBRARY_PATH."": ""This is incorrect because AWS Lambda does not support loading libraries from an S3 bucket at runtime. The LD_LIBRARY_PATH environment variable is not applicable in this context, as Lambda functions cannot dynamically load libraries from external sources like S3. All dependencies must be included in the deployment package to ensure they are available at execution time."",
    ""Create a `ZIP` file with the source code and a buildspec.yaml file that installs the dependent libraries on AWS Lambda."": ""This is incorrect because a buildspec.yaml file is used in AWS CodeBuild, not AWS Lambda. It is used to define the build process in CodeBuild, not to manage dependencies for a Lambda function. Lambda functions require all dependencies to be packaged together in a deployment package, and there is no mechanism to use a buildspec.yaml file within Lambda for dependency management.""
}
```"
A Developer is designing a fault-tolerant environment where client sessions will be saved. How can the Developer ensure that no sessions are lost if an Amazon EC2 instance fails?,"[""Use sticky sessions with an Elastic Load Balancer target group."", ""Use Amazon SQS to save session data."", ""Use Amazon DynamoDB to perform scalable session handling."", ""Use Elastic Load Balancer connection draining to stop sending requests to failing instances.""]","[""Use Amazon DynamoDB to perform scalable session handling.""]","```json
{
    ""Use sticky sessions with an Elastic Load Balancer target group."": ""Sticky sessions, also known as session affinity, bind a user's session to a specific EC2 instance. While this can help maintain session continuity, it does not ensure fault tolerance. If the bound EC2 instance fails, the session data is lost. This approach does not provide a solution for saving session data in a fault-tolerant manner, which is why it is incorrect for ensuring no sessions are lost."",
    ""Use Amazon SQS to save session data."": ""Amazon SQS is a message queuing service that allows decoupling of components in a distributed system. It is not designed for session data storage. SQS is used for message passing rather than storing session state, which requires persistent storage. Therefore, it is not suitable for saving session data to ensure fault tolerance, making this choice incorrect."",
    ""Use Amazon DynamoDB to perform scalable session handling."": ""Amazon DynamoDB is a fully managed NoSQL database service that provides fast and predictable performance with seamless scalability. It is ideal for storing session data because it offers high availability and durability, ensuring that session data is not lost even if an EC2 instance fails. This makes it the correct choice for fault-tolerant session handling. DynamoDB's ability to scale and its integration with other AWS services make it a best practice for session management in distributed applications."",
    ""Use Elastic Load Balancer connection draining to stop sending requests to failing instances."": ""Connection draining in Elastic Load Balancer allows existing requests to complete before terminating an instance, which helps in gracefully handling in-flight requests. However, it does not address the persistence of session data. If an instance fails, any session data stored in-memory on that instance would still be lost. Therefore, while connection draining improves request handling during instance termination, it does not ensure session data is fault-tolerant, making this choice incorrect for the given requirement.""
}
```"
"In a move toward using microservices, a company's Management team has asked all Development teams to build their services so that API requests depend only on that service's data store. One team is building a Payments service which has its own database; the service needs data that originates in the Accounts database. Both are using Amazon DynamoDB. What approach will result in the simplest, decoupled, and reliable method to get near-real time updates from the Accounts database?","[""Use Amazon Glue to perform frequent ETL updates from the Accounts database to the Payments database."", ""Use Amazon ElastiCache in Payments, with the cache updated by triggers in the Accounts database."", ""Use Amazon Kinesis Data Firehose to deliver all changes from the Accounts database to the Payments database."", ""Use Amazon DynamoDB Streams to deliver all changes from the Accounts database to the Payments database.""]","[""Use Amazon DynamoDB Streams to deliver all changes from the Accounts database to the Payments database.""]","```json
{
    ""Use Amazon Glue to perform frequent ETL updates from the Accounts database to the Payments database."": ""This choice is incorrect because Amazon Glue is primarily used for ETL (Extract, Transform, Load) operations which are batch-oriented and not suitable for near-real time data updates. The requirement is for a near-real time solution, and Glue's batch processing nature does not meet this requirement. Additionally, Glue introduces additional complexity and overhead that is unnecessary for this use case."",
    ""Use Amazon ElastiCache in Payments, with the cache updated by triggers in the Accounts database."": ""This choice is incorrect because Amazon ElastiCache is a caching service and is not designed to handle data synchronization between databases. Using triggers to update a cache is not a reliable method for ensuring data consistency and can lead to stale data if not managed properly. Furthermore, this approach adds unnecessary complexity and does not provide a decoupled architecture."",
    ""Use Amazon Kinesis Data Firehose to deliver all changes from the Accounts database to the Payments database."": ""This choice is incorrect because Amazon Kinesis Data Firehose is designed for streaming data to destinations like S3, Redshift, or Elasticsearch, not directly to another DynamoDB table. While it can handle streaming data, it is not the best fit for DynamoDB-to-DynamoDB data replication, and using it for this purpose would require additional infrastructure and complexity."",
    ""Use Amazon DynamoDB Streams to deliver all changes from the Accounts database to the Payments database."": ""This choice is correct because Amazon DynamoDB Streams captures changes to items in a DynamoDB table and can be used to trigger AWS Lambda functions to process these changes. This allows for near-real time updates and a decoupled architecture, as the Payments service can react to changes in the Accounts database without direct dependencies. This approach aligns with AWS best practices for building microservices that are loosely coupled and event-driven. DynamoDB Streams is specifically designed for this type of use case and provides a reliable and simple solution.""
}
```"
A company needs a fully-managed source control service that will work in AWS. The service must ensure that revision control synchronizes multiple distributed repositories by exchanging sets of changes peer-to-peer. All users need to work productively even when not connected to a network. Which source control service should be used?,"[""Subversion."", ""AWS CodeBuild."", ""AWS CodeCommit."", ""AWS CodeStar.""]","[""AWS CodeCommit.""]","```json
{
    ""Subversion."": ""Subversion is a centralized version control system, not a distributed one. It does not support peer-to-peer exchange of changes and requires a network connection to the central repository to work effectively. This makes it unsuitable for the requirement of working productively when not connected to a network."",
    ""AWS CodeBuild."": ""AWS CodeBuild is a fully managed continuous integration service that compiles source code, runs tests, and produces software packages ready to deploy. It is not a source control service and does not handle revision control or distributed repositories, making it incorrect for this requirement."",
    ""AWS CodeCommit."": ""AWS CodeCommit is a fully managed source control service that supports Git, a distributed version control system. Git allows for peer-to-peer exchange of changes and enables users to work offline, which aligns with the requirement of working productively without a network connection. This makes AWS CodeCommit the correct choice."",
    ""AWS CodeStar."": ""AWS CodeStar is a service for quickly developing, building, and deploying applications on AWS. It provides a unified user interface, but it is not a source control service itself. Instead, it integrates with AWS CodeCommit and other AWS services. Therefore, it does not directly meet the requirement for a source control service.""
}
```"
A Developer is writing a serverless application that requires that an AWS Lambda function be invoked every 10 minutes. What is an automated and serverless way to trigger the function?,"[""Deploy an Amazon EC2 instance based on Linux, and edit its `/etc/crontab` file by adding a command to periodically invoke the Lambda function."", ""Configure an environment variable named PERIOD for the Lambda function. Set the value to 600."", ""Create an Amazon CloudWatch Events rule that triggers on a regular schedule to invoke the Lambda function."", ""Create an Amazon SNS topic that has a subscription to the Lambda function with a 600-second timer.""]","[""Create an Amazon CloudWatch Events rule that triggers on a regular schedule to invoke the Lambda function.""]","```json
{
    ""Deploy an Amazon EC2 instance based on Linux, and edit its `/etc/crontab` file by adding a command to periodically invoke the Lambda function."": ""This choice is incorrect because it involves deploying an EC2 instance, which is not serverless. The question specifies a serverless solution, and using EC2 introduces unnecessary infrastructure management and costs. AWS Lambda is designed to be used in a serverless architecture, and using EC2 contradicts this approach."",
    ""Configure an environment variable named PERIOD for the Lambda function. Set the value to 600."": ""This choice is incorrect because setting an environment variable does not schedule or trigger a Lambda function. Environment variables are used to pass configuration information to the function, but they do not have any capability to invoke the function on a schedule."",
    ""Create an Amazon CloudWatch Events rule that triggers on a regular schedule to invoke the Lambda function."": ""This choice is correct because Amazon CloudWatch Events (now known as Amazon EventBridge) allows you to create rules that trigger based on a schedule using cron expressions or rate expressions. This is a serverless and automated way to invoke a Lambda function at regular intervals, such as every 10 minutes, aligning with AWS best practices for serverless applications."",
    ""Create an Amazon SNS topic that has a subscription to the Lambda function with a 600-second timer."": ""This choice is incorrect because Amazon SNS does not provide a built-in scheduling mechanism. SNS is used for pub/sub messaging and does not have a native feature to trigger events on a schedule. A timer would need to be implemented externally, which would not be serverless or automated in the context of SNS alone.""
}
```"
A company is building an application to track athlete performance using an Amazon DynamoDB table. Each item in the table is identified by a partition key (`user_id`) and a sort key (`sport_name`). The table design is shown below. (Note: Not all table attributes are shown) A Developer is asked to write a leaderboard application to display the top performers (`user_id`) based on the score for each `sport_name`. What process will allow the Developer to extract results MOST efficiently from the DynamoDB table?,"[""Use a DynamoDB query operation with the key attributes of `user_id` and `sport_name` and order the results based on the score attribute."", ""Create a global secondary index with a partition key of `sport_name` and a sort key of score, and get the results."", ""Use a DynamoDB scan operation to retrieve scores and `user_id` based on `sport_name`, and order the results based on the score attribute."", ""Create a local secondary index with a primary key of `sport_name` and a sort key of score and get the results based on the score attribute.""]","[""Create a global secondary index with a partition key of `sport_name` and a sort key of score, and get the results.""]","```json
{
    ""Use a DynamoDB query operation with the key attributes of `user_id` and `sport_name` and order the results based on the score attribute."": ""This choice is incorrect because a query operation on DynamoDB requires specifying the partition key and optionally a sort key. In this case, querying with `user_id` and `sport_name` would not allow efficient retrieval of top performers across all users for a specific sport. Additionally, DynamoDB does not support ordering results based on non-key attributes directly in a query operation."",
    ""Create a global secondary index with a partition key of `sport_name` and a sort key of score, and get the results."": ""This choice is correct because creating a global secondary index (GSI) with `sport_name` as the partition key and `score` as the sort key allows efficient querying of top performers for each sport. The GSI enables sorting by score within each sport, making it possible to retrieve the top scores efficiently. This approach aligns with AWS best practices for optimizing read operations in DynamoDB."",
    ""Use a DynamoDB scan operation to retrieve scores and `user_id` based on `sport_name`, and order the results based on the score attribute."": ""This choice is incorrect because a scan operation reads every item in the table, which is inefficient and costly, especially for large datasets. Additionally, ordering results based on a non-key attribute like `score` would require post-processing outside of DynamoDB, leading to increased latency and resource usage."",
    ""Create a local secondary index with a primary key of `sport_name` and a sort key of score and get the results based on the score attribute."": ""This choice is incorrect because a local secondary index (LSI) can only be created with the same partition key as the base table, which is `user_id` in this case. Since the requirement is to query based on `sport_name`, an LSI is not suitable. LSIs are also limited to 10GB per partition, which can be restrictive for large datasets.""
}
```"
A Developer is creating a mobile application that will not require users to log in. What is the MOST efficient method to grant users access to AWS resources?,"[""Use an identity provider to securely authenticate with the application."", ""Create an AWS Lambda function to create an IAM user when a user accesses the application."", ""Create credentials using AWS KMS and apply these credentials to users when using the application."", ""Use Amazon Cognito to associate unauthenticated users with an IAM role that has limited access to resources.""]","[""Use Amazon Cognito to associate unauthenticated users with an IAM role that has limited access to resources.""]","```json
{
    ""Use an identity provider to securely authenticate with the application."": ""This choice is incorrect because the question specifies that the application will not require users to log in. Using an identity provider implies some form of authentication, which contradicts the requirement of not requiring login. Identity providers are typically used when you need to authenticate users and manage their identities, which is not needed in this scenario."",
    ""Create an AWS Lambda function to create an IAM user when a user accesses the application."": ""This choice is incorrect because creating an IAM user for each application user is not scalable or efficient. IAM users are intended for managing access for individuals or services that need long-term access to AWS resources. Creating a new IAM user for each application user would lead to unnecessary complexity and potential security risks, as well as hitting service limits quickly."",
    ""Create credentials using AWS KMS and apply these credentials to users when using the application."": ""This choice is incorrect because AWS KMS is used for managing encryption keys, not for creating user credentials. Applying KMS credentials to users is a misunderstanding of KMS's purpose. KMS is designed for encrypting and decrypting data, not for user authentication or authorization."",
    ""Use Amazon Cognito to associate unauthenticated users with an IAM role that has limited access to resources."": ""This choice is correct because Amazon Cognito provides a feature called 'unauthenticated identities' which allows you to grant access to AWS resources without requiring users to log in. This is achieved by associating unauthenticated users with an IAM role that specifies what resources they can access. This approach is efficient and aligns with the requirement of not requiring user login while still controlling access to resources. It is a best practice for managing access in scenarios where user authentication is not needed.""
}
```"
An application running on Amazon EC2 instances must access objects within an Amaon S3 busket that are encrypted using server-side encryption using AWS KMS encryption keys (SSE-KMS). The application must have access to the customer master key (CMK) to decrypt the objects. Which combination of steps will grant the application access? (Select TWO),"[""Write an S3 bucket policy that grants the bucket access to the key."", ""Grant access to the key in the IAM EC2 role attached to the application's EC2 instances."", ""Write a key policy that enables IAM policies to grant access to the key."", ""Grant access to the key in the S3 bucket's ACL."", ""Create a Systems Manager parameter that exposes the KMS key to the EC2 instances.""]","[""Write an S3 bucket policy that grants the bucket access to the key."", ""Grant access to the key in the IAM EC2 role attached to the application's EC2 instances.""]","```json
{
    ""Write an S3 bucket policy that grants the bucket access to the key."": ""Incorrect. S3 bucket policies control access to the objects in the bucket, not to the KMS keys themselves. Access to KMS keys is managed through key policies and IAM policies, not bucket policies."",
    ""Grant access to the key in the IAM EC2 role attached to the application's EC2 instances."": ""Correct. Granting access to the KMS key in the IAM role associated with the EC2 instances allows the application running on those instances to decrypt the objects stored in S3. This is a common practice to ensure that the application has the necessary permissions to use the KMS key for decryption."",
    ""Write a key policy that enables IAM policies to grant access to the key."": ""Correct. A key policy is essential for granting permissions to use a KMS key. By enabling IAM policies to grant access, you allow IAM roles and users to use the key according to the permissions specified in their IAM policies. This is a best practice for managing KMS key permissions."",
    ""Grant access to the key in the S3 bucket's ACL."": ""Incorrect. S3 bucket ACLs control access to the bucket and its objects, not to KMS keys. KMS key permissions must be managed through key policies and IAM policies, not through S3 bucket ACLs."",
    ""Create a Systems Manager parameter that exposes the KMS key to the EC2 instances."": ""Incorrect. Systems Manager parameters are used for storing configuration data and secrets, not for granting access to KMS keys. Access to KMS keys should be managed through IAM roles and key policies, not by exposing keys through Systems Manager.""
}
```"
What does an Amazon SQS delay queue accomplish?,"[""Messages are hidden for a configurable amount of time when they are first added to the queue."", ""Messages are hidden for a configurable amount of time after they are consumed from the queue."", ""The consumer can poll the queue for a configurable amount of time before retrieving a message."", ""Message cannot be deleted for a configurable amount of time after they are consumed from the queue.""]","[""Messages are hidden for a configurable amount of time when they are first added to the queue.""]","```json
{
    ""Messages are hidden for a configurable amount of time when they are first added to the queue."": ""This choice is correct because an Amazon SQS delay queue allows you to postpone the delivery of new messages to consumers for a specified amount of time. This is useful in scenarios where you want to delay processing of messages, such as when you need to batch process messages or when you want to allow time for other processes to complete before processing the message. According to AWS documentation, this delay is configured at the queue level and applies to all messages added to the queue."",
    ""Messages are hidden for a configurable amount of time after they are consumed from the queue."": ""This choice is incorrect because Amazon SQS does not provide a feature to hide messages after they are consumed. Once a message is consumed and processed, it is typically deleted from the queue. The concept of hiding messages after consumption is not applicable to SQS. Instead, SQS provides a visibility timeout feature that temporarily hides a message after it is read from the queue, preventing other consumers from processing it until the timeout expires."",
    ""The consumer can poll the queue for a configurable amount of time before retrieving a message."": ""This choice is incorrect because it describes the long polling feature of SQS, which is unrelated to delay queues. Long polling allows consumers to wait for a specified duration for messages to arrive in the queue, reducing the number of empty responses and lowering costs. However, it does not involve delaying the visibility of messages when they are first added to the queue."",
    ""Message cannot be deleted for a configurable amount of time after they are consumed from the queue."": ""This choice is incorrect because SQS does not have a feature that prevents messages from being deleted after consumption. Once a message is processed, it is typically deleted by the consumer using the DeleteMessage API call. The delay queue feature does not affect the deletion process; it only delays the initial visibility of messages when they are added to the queue.""
}
```"
"A company has multiple Developers located across the globe who are updating code incrementally for a development project. When Developers upload code concurrently, internet connectivity is slow and it is taking a long time to upload code for deployment in AWS Elastic Beanstalk. Which step will result in minimized upload and deployment time with the LEAST amount of administrative effort?","[""Allow the Developers to upload the code to an Amazon S3 bucket, and deploy it directly to Elastic Beanstalk."", ""Allow the Developers to upload the code to a central FTP server to deploy the application to Elastic Beanstalk."", ""Create an AWS CodeCommit repository, allow the Developers to commit code to it, and then directly deploy the code to Elastic Beanstalk."", ""Create a code repository on an Amazon EC2 instance so that all Developers can update the code, and deploy the application from the instance to Elastic Beanstalk.""]","[""Create an AWS CodeCommit repository, allow the Developers to commit code to it, and then directly deploy the code to Elastic Beanstalk.""]","```json
{
    ""Allow the Developers to upload the code to an Amazon S3 bucket, and deploy it directly to Elastic Beanstalk."": ""This option is incorrect because while Amazon S3 can be used to store application versions for Elastic Beanstalk, it does not address the issue of concurrent code updates by multiple developers. S3 is primarily a storage service and does not provide version control or collaborative features needed for code development."",
    ""Allow the Developers to upload the code to a central FTP server to deploy the application to Elastic Beanstalk."": ""This option is incorrect because using an FTP server for code uploads is outdated and lacks the collaborative and version control features necessary for a team of developers. FTP does not integrate well with AWS services for deployment and does not minimize upload times effectively."",
    ""Create an AWS CodeCommit repository, allow the Developers to commit code to it, and then directly deploy the code to Elastic Beanstalk."": ""This option is correct because AWS CodeCommit is a fully managed source control service that makes it easy for teams to collaborate on code in a secure and scalable manner. It integrates well with AWS Elastic Beanstalk, allowing for streamlined deployments. CodeCommit supports Git, which is a widely used version control system that helps manage code changes efficiently, making it ideal for teams of developers working concurrently."",
    ""Create a code repository on an Amazon EC2 instance so that all Developers can update the code, and deploy the application from the instance to Elastic Beanstalk."": ""This option is incorrect because hosting a code repository on an EC2 instance requires additional administrative effort to manage the server, ensure security, and maintain uptime. It does not provide the scalability and integration benefits of a managed service like AWS CodeCommit, and it does not inherently solve the problem of slow internet connectivity or concurrent updates.""
}
```"
"A company recently migrated its web, application and NoSQL database tiers to AWS. The company is using Auto Scaling to scale the web and application tiers. More than 95 percent of the Amazon DynamoDB requests are repeated read requests. How can the DynamoDB NoSQL tier be scaled up to cache these repeated requests?","[""Amazon EMR."", ""Amazon DynamoDB Accelerator."", ""Amazon SQS."", ""Amazon CloudFront.""]","[""Amazon DynamoDB Accelerator.""]","```json
{
    ""Amazon EMR."": ""Amazon EMR (Elastic MapReduce) is a service for processing and analyzing large datasets using frameworks like Hadoop and Spark. It is not designed for caching repeated read requests from a database. Therefore, it is not suitable for scaling up the DynamoDB NoSQL tier to cache repeated requests."",
    ""Amazon DynamoDB Accelerator."": ""Amazon DynamoDB Accelerator (DAX) is a fully managed, highly available, in-memory cache for DynamoDB that can reduce response times from milliseconds to microseconds for read-heavy and bursty workloads. It is specifically designed to handle repeated read requests efficiently, making it the correct choice for caching repeated requests in the DynamoDB NoSQL tier. This aligns with AWS best practices for optimizing performance and cost-efficiency in read-heavy applications."",
    ""Amazon SQS."": ""Amazon SQS (Simple Queue Service) is a message queuing service that enables decoupling of application components. It is not designed for caching database requests or improving read performance. Therefore, it is not applicable for scaling the DynamoDB NoSQL tier to cache repeated read requests."",
    ""Amazon CloudFront."": ""Amazon CloudFront is a content delivery network (CDN) service that accelerates the delivery of static and dynamic web content. While it can cache content at edge locations, it is not designed to cache database requests or improve read performance for DynamoDB. Thus, it is not suitable for caching repeated read requests from DynamoDB.""
}
```"
"A Development team is working on a case management solution that allows medical claims to be processed and reviewed. Users log in to provide information related to their medical and financial situations. As part of the application, sensitive documents such as medical records, medical imaging, bank statements, and receipts are uploaded to Amazon S3. All documents must be securely transmitted and stored. All access to the documents must be recorded for auditing. What is the MOST secure approach?","[""Use S3 default encryption using Advanced Encryption Standard-256 (AES-256) on the destination bucket."", ""Use Amazon Cognito for authorization and authentication to ensure the security of the application and documents."", ""Use AWS Lambda to encrypt and decrypt objects as they are placed into the S3 bucket."", ""Use client-side encryption/decryption with Amazon S3 and AWS KMS.""]","[""Use client-side encryption/decryption with Amazon S3 and AWS KMS.""]","```json
{
    ""Use S3 default encryption using Advanced Encryption Standard-256 (AES-256) on the destination bucket."": ""This option is incorrect because while S3 default encryption ensures that data is encrypted at rest, it does not provide encryption during transmission. The question specifies that documents must be securely transmitted and stored, which requires encryption both in transit and at rest. Additionally, S3 server-side encryption does not address the requirement for client-side encryption, which is often considered more secure as it allows the client to manage encryption keys."",
    ""Use Amazon Cognito for authorization and authentication to ensure the security of the application and documents."": ""This option is incorrect because Amazon Cognito is primarily used for user authentication and authorization, not for encrypting data. While Cognito can help secure access to the application by managing user sign-up and sign-in, it does not directly address the requirement for encrypting documents uploaded to S3. The question focuses on the secure transmission and storage of documents, which requires encryption solutions."",
    ""Use AWS Lambda to encrypt and decrypt objects as they are placed into the S3 bucket."": ""This option is incorrect because using AWS Lambda for encryption and decryption introduces unnecessary complexity and potential latency. Lambda functions would need to be triggered for every upload and download, which could increase costs and processing time. Additionally, this approach does not inherently provide secure transmission of data. AWS provides more efficient and integrated solutions for encryption, such as client-side encryption with AWS KMS."",
    ""Use client-side encryption/decryption with Amazon S3 and AWS KMS."": ""This option is correct because client-side encryption ensures that data is encrypted before it is transmitted to S3, providing security both in transit and at rest. AWS KMS (Key Management Service) can be used to manage encryption keys securely, allowing clients to encrypt data before uploading it to S3. This approach meets the requirement for secure transmission and storage of sensitive documents. Client-side encryption is a best practice for handling sensitive data, as it gives the client control over the encryption process and keys.""
}
```"
A company has an internet-facing application that uses Web Identity Federation to obtain a temporary credential from AWS Security Token Service (AWS STS). The app then uses the token to access AWS services. Review the following response: Based on the response displayed what permissions are associated with the call from the application?,"[""Permissions associated with the role AROACLKWSDQRAOEXAMPLE:app1."", ""Permissions associated with the default role used when the AWS service was built."", ""Permission associated with the IAM principal that owns the AccessKeyID ASgeIAIOSFODNN7EXAMPLE."", ""Permissions associated with the account that owns the AWS service.""]","[""Permission associated with the IAM principal that owns the AccessKeyID ASgeIAIOSFODNN7EXAMPLE.""]","{""Permissions associated with the role AROACLKWSDQRAOEXAMPLE:app1."": ""This choice is incorrect because the question specifies that the application uses Web Identity Federation to obtain temporary credentials from AWS STS. In such a scenario, the permissions are associated with the IAM principal that owns the AccessKeyID, not a specific role like AROACLKWSDQRAOEXAMPLE:app1. Web Identity Federation allows users to assume a role, but the permissions are tied to the IAM principal that is authenticated."", ""Permissions associated with the default role used when the AWS service was built."": ""This choice is incorrect because AWS services do not have a 'default role' that is used when they are built. Permissions are explicitly defined by the IAM roles or policies associated with the service or application. In the context of Web Identity Federation, the permissions are tied to the IAM principal that owns the AccessKeyID."", ""Permission associated with the IAM principal that owns the AccessKeyID ASgeIAIOSFODNN7EXAMPLE."": ""This choice is correct because when using Web Identity Federation, the application exchanges the identity token for temporary AWS credentials from AWS STS. These temporary credentials are associated with the IAM principal that owns the AccessKeyID, which defines the permissions for accessing AWS services. This is a common practice to ensure secure and temporary access to AWS resources."", ""Permissions associated with the account that owns the AWS service."": ""This choice is incorrect because permissions are not associated with the AWS account itself but rather with IAM roles or users within the account. The account ownership does not determine the permissions for accessing AWS services. In the context of Web Identity Federation, permissions are tied to the IAM principal associated with the temporary credentials.""}"
"A Developer is using AWS CLI, but when running list commands on a large number of resources, it is timing out. What can be done to avoid this time-out?","[""Use pagination."", ""Use shorthand syntax."", ""Use parameter values."", ""Use quoting strings.""]","[""Use pagination.""]","{""Use pagination."": ""Pagination is the correct solution for handling timeouts when listing a large number of resources using AWS CLI. AWS CLI supports pagination, which breaks down the output into smaller, manageable chunks, preventing timeouts and making it easier to process large datasets. This is a best practice when dealing with AWS services that return large amounts of data, such as S3 or EC2. Refer to AWS CLI documentation on pagination for more details."", ""Use shorthand syntax."": ""Shorthand syntax is a way to provide input parameters in a compact format on the command line. It does not address the issue of timeouts when listing large numbers of resources. Therefore, it is not a solution to the problem described in the question."", ""Use parameter values."": ""Using parameter values involves specifying inputs for AWS CLI commands. While it is essential for executing commands, it does not solve the problem of timeouts due to large datasets. Parameter values do not affect the size of the output or the likelihood of a timeout."", ""Use quoting strings."": ""Quoting strings is a technique used in the command line to ensure that strings with spaces or special characters are interpreted correctly. It does not impact the performance or timeout behavior of AWS CLI commands when listing resources. Therefore, it is not relevant to solving the timeout issue.""}"
Where can PortMapping be defined when launching containers in Amazon ECS?,"[""Security groups."", ""Amazon Elastic Container Registry (Amzon ECR)."", ""Container agent."", ""Task definition.""]","[""Task definition.""]","{""Security groups."": ""Security groups in AWS are used to control inbound and outbound traffic to resources like EC2 instances, not for defining port mappings for containers. They act as virtual firewalls at the instance level, not at the container level. Therefore, they are not used to define port mappings in Amazon ECS."", ""Amazon Elastic Container Registry (Amzon ECR)."": ""Amazon ECR is a fully managed Docker container registry that makes it easy to store, manage, and deploy Docker container images. It does not handle the configuration of container runtime settings such as port mappings, which are defined in the task definition."", ""Container agent."": ""The Amazon ECS container agent runs on each infrastructure resource within an ECS cluster and is responsible for communicating with the ECS service. However, it does not define port mappings; it merely executes tasks as specified in the task definition."", ""Task definition."": ""Task definitions in Amazon ECS are used to specify the container configurations, including port mappings. They define how containers should be launched, including details such as the Docker image to use, CPU and memory requirements, and network settings like port mappings. This is the correct place to define port mappings for containers in ECS.""}"
"An organization is storing large files in Amazon S3, and is writing a web application to display meta-data about the files to end-users. Based on the metadata a user selects an object to download. The organization needs a mechanism to index the files and provide single-digit millisecond latency retrieval for the metadata. What AWS service should be used to accomplish this?","[""Amazon DynamoDB."", ""Amazon EC2."", ""AWS Lambda."", ""Amazon RDS.""]","[""Amazon DynamoDB.""]","```json
{
    ""Amazon DynamoDB."": ""Amazon DynamoDB is the correct choice because it is a fully managed NoSQL database service that provides single-digit millisecond latency, which is ideal for indexing and retrieving metadata quickly. DynamoDB is designed for high availability and scalability, making it suitable for applications that require fast access to data. It is commonly used for scenarios where quick lookups are necessary, such as indexing metadata for files stored in S3."",
    ""Amazon EC2."": ""Amazon EC2 is incorrect for this use case because it is a service that provides resizable compute capacity in the cloud, not a database service. While EC2 could theoretically host a database, it would not inherently provide the single-digit millisecond latency and ease of management that DynamoDB offers. Using EC2 for this purpose would require additional setup and maintenance of a database system, which is not optimal for this scenario."",
    ""AWS Lambda."": ""AWS Lambda is incorrect because it is a compute service that runs code in response to events and automatically manages the compute resources required by that code. While Lambda can be used to process metadata or trigger actions based on events, it is not designed for storing or retrieving data with low latency. It would not serve as an indexing mechanism for metadata retrieval."",
    ""Amazon RDS."": ""Amazon RDS is incorrect because it is a managed relational database service that provides scalability and ease of use for relational databases. While RDS can be used to store metadata, it typically does not provide the same level of performance in terms of single-digit millisecond latency as DynamoDB. RDS is better suited for applications that require complex queries and transactions, which are not necessary for simple metadata indexing.""
}
```"
"While developing an application that runs on Amazon EC2 in an Amazon VPC, a Developer identifies the need for centralized storage of application-level logs. Which AWS service can be used to securely store these logs?","[""Amazon EC2 VPC Flow Logs."", ""Amazon CloudWatch Logs."", ""Amazon CloudSearch."", ""AWS CloudTrail""]","[""Amazon CloudWatch Logs.""]","```json
{
    ""Amazon EC2 VPC Flow Logs."": ""Amazon EC2 VPC Flow Logs are used to capture information about the IP traffic going to and from network interfaces in a VPC. They are not designed for application-level logging. This choice is incorrect because it focuses on network traffic rather than application logs."",
    ""Amazon CloudWatch Logs."": ""Amazon CloudWatch Logs is the correct choice because it is designed to monitor, store, and access log files from Amazon EC2 instances, AWS CloudTrail, and other sources. It allows developers to centralize logs from multiple sources and provides features such as log retention, filtering, and analysis. This service is ideal for securely storing application-level logs, as it integrates well with other AWS services and provides robust security features."",
    ""Amazon CloudSearch."": ""Amazon CloudSearch is a managed search service that makes it easy to set up, manage, and scale a search solution for a website or application. It is not intended for log storage or analysis. This choice is incorrect because it is focused on search capabilities rather than log management."",
    ""AWS CloudTrail"": ""AWS CloudTrail is a service that enables governance, compliance, and operational and risk auditing of your AWS account. It records AWS API calls and related events for your account and delivers log files to an Amazon S3 bucket. While it provides logging of API activity, it is not designed for application-level logs. This choice is incorrect because it focuses on API call logging rather than application logs.""
}
```"
"A stock market monitoring application uses Amazon Kinesis for data ingestion. During simulated tests of peak data rates, the Kinesis stream cannot keep up with the incoming data. What step will allow Kinesis to accommodate the traffic during peak hours?","[""Install the Kinesis Producer Library (KPL) for ingesting data into the stream."", ""Reduce the data retention period to allow for more data ingestion using. DecreaseStreamRetentionPeriod."", ""Increase the shard count of the stream using UpdateShardCount."", ""Ingest multiple records into the stream in a single call using PutRecords.""]","[""Install the Kinesis Producer Library (KPL) for ingesting data into the stream.""]","```json
{
    ""Install the Kinesis Producer Library (KPL) for ingesting data into the stream."": ""This choice is incorrect. While the Kinesis Producer Library (KPL) can help optimize data ingestion by batching records and improving throughput, it does not directly address the issue of insufficient shard capacity. The problem described is that the stream cannot keep up with the data rate, which is typically resolved by increasing the number of shards."",
    ""Reduce the data retention period to allow for more data ingestion using. DecreaseStreamRetentionPeriod."": ""This choice is incorrect. Reducing the data retention period does not affect the ability of a Kinesis stream to ingest data. Data retention is related to how long data is stored in the stream, not the rate at which data can be ingested. The issue of handling peak data rates is related to the number of shards."",
    ""Increase the shard count of the stream using UpdateShardCount."": ""This choice is correct. Increasing the shard count is the appropriate solution for handling higher data ingestion rates. Each shard in a Kinesis stream has a fixed capacity for data ingestion and processing. By increasing the number of shards, you increase the overall capacity of the stream to handle more data during peak hours. This is a common practice to scale Kinesis streams to meet varying data rates."",
    ""Ingest multiple records into the stream in a single call using PutRecords."": ""This choice is incorrect. While using PutRecords can improve efficiency by reducing the number of API calls, it does not increase the capacity of the stream itself. The underlying issue of the stream not keeping up with data rates is related to shard capacity, which is not addressed by simply batching records.""
}
```"
A company has an AWS CloudFormation template that is stored as a single file. The template is able to launch and create a full infrastructure stack. Which best practice would increase the maintainability of the template?,"[""Use nested stacks for common template patterns."", ""Embed credentials to prevent typos."", ""Remove mappings to decrease the number of variables."", ""Use `AWS::Include` to reference publicly-hosted template files.""]","[""Use nested stacks for common template patterns.""]","```json
{
    ""Use nested stacks for common template patterns."": ""This choice is correct because using nested stacks in AWS CloudFormation is a best practice for improving the maintainability and organization of templates. Nested stacks allow you to break down a complex template into smaller, reusable components, making it easier to manage and update individual parts of the infrastructure. This approach also promotes reusability and reduces duplication of code, which aligns with AWS best practices for infrastructure as code. According to AWS documentation, nested stacks are particularly useful for managing common patterns and components that are used across multiple stacks."",
    ""Embed credentials to prevent typos."": ""This choice is incorrect because embedding credentials directly into CloudFormation templates is a security risk and goes against AWS best practices. AWS recommends using AWS Identity and Access Management (IAM) roles and policies to manage access and permissions securely. Hardcoding credentials can lead to accidental exposure and potential security breaches. Instead, sensitive information should be managed using AWS Secrets Manager or AWS Systems Manager Parameter Store."",
    ""Remove mappings to decrease the number of variables."": ""This choice is incorrect because removing mappings is not a recommended practice for improving maintainability. Mappings in CloudFormation templates provide a way to define conditional values that can be reused throughout the template. They help in managing configuration differences across environments or regions without hardcoding values. Removing mappings would reduce flexibility and make the template less adaptable to changes, which is contrary to the goal of maintainability."",
    ""Use `AWS::Include` to reference publicly-hosted template files."": ""This choice is incorrect because while `AWS::Include` can be used to include external template snippets, it is not the best practice for improving maintainability in the context of the given scenario. `AWS::Include` is useful for including reusable snippets, but it requires the external files to be publicly accessible, which can introduce security concerns. Additionally, it does not inherently improve the organization or modularity of a complex template as effectively as nested stacks do.""
}
```"
"An on-premises application makes repeated calls to store files to Amazon S3. As usage of the application has increased, `LimitExceeded` errors are being logged. What should be changed to fix this error?","[""Implement exponential backoffs in the application."", ""Load balance the application to multiple servers."", ""Move the application to Amazon EC2."", ""Add a one second delay to each API call.""]","[""Implement exponential backoffs in the application.""]","```json
{
    ""Implement exponential backoffs in the application."": ""This choice is correct because AWS recommends implementing exponential backoff for retrying requests that fail with certain error codes, such as 'LimitExceeded'. Exponential backoff helps to reduce the load on the service by spacing out retries, which can help mitigate throttling issues. This is a best practice for handling throttling errors when interacting with AWS services like S3, as documented in the AWS SDKs and API documentation."",
    ""Load balance the application to multiple servers."": ""This choice is incorrect because load balancing the application does not address the issue of 'LimitExceeded' errors when making API calls to S3. The error is related to the rate of API requests rather than the distribution of application load across servers. Load balancing would not reduce the frequency of API calls to S3, which is the underlying issue."",
    ""Move the application to Amazon EC2."": ""This choice is incorrect because moving the application to Amazon EC2 does not inherently solve the problem of 'LimitExceeded' errors. The error is related to the rate of S3 API requests, not the location of the application. While moving to EC2 might offer other benefits, it does not address the need to handle API request throttling."",
    ""Add a one second delay to each API call."": ""This choice is incorrect because adding a fixed delay to each API call is not an efficient or scalable solution. It does not adapt to varying levels of API request load and could unnecessarily slow down the application. Exponential backoff is a more effective strategy as it dynamically adjusts the delay based on the number of retries and the error response.""
}
```"
A company caches session information for a web application in an Amazon DynamoDB table. The company wants an automated way to delete old items from the table. What is the simplest way to do this?,"[""Write a script that deletes old records; schedule the scripts as a cron job on an Amazon EC2 instance."", ""Add an attribute with the expiration time; enable the Time To Live feature based on that attribute."", ""Each day, create a new table to hold session data; delete the previous day's table."", ""Add an attribute with the expiration time; name the attribute ItemExpiration.""]","[""Add an attribute with the expiration time; enable the Time To Live feature based on that attribute.""]","```json
{
    ""Write a script that deletes old records; schedule the scripts as a cron job on an Amazon EC2 instance."": ""This choice is incorrect because it involves managing infrastructure and writing custom code to handle the deletion of old records. This approach is not the simplest or most efficient way to manage item expiration in DynamoDB. It requires maintaining an EC2 instance, handling potential failures, and ensuring the script runs correctly, which adds operational overhead. AWS provides a built-in feature called Time To Live (TTL) that automatically deletes expired items, making this option unnecessary."",
    ""Add an attribute with the expiration time; enable the Time To Live feature based on that attribute."": ""This choice is correct because it leverages DynamoDB's built-in Time To Live (TTL) feature, which is specifically designed to automatically delete items after a specified expiration time. By adding an attribute to each item that specifies the expiration time and enabling TTL on that attribute, DynamoDB will automatically delete expired items without any additional infrastructure or code. This is a best practice for managing item expiration in DynamoDB, as it simplifies operations and reduces costs by automatically cleaning up old data."",
    ""Each day, create a new table to hold session data; delete the previous day's table."": ""This choice is incorrect because creating and deleting tables daily is not a practical or efficient solution for managing session data. It involves significant overhead in terms of table management and can lead to increased costs and complexity. Additionally, this approach does not leverage DynamoDB's capabilities for managing data expiration and is not aligned with AWS best practices for handling session data."",
    ""Add an attribute with the expiration time; name the attribute ItemExpiration."": ""This choice is incorrect because simply adding an attribute with an expiration time does not automatically delete items. While naming the attribute 'ItemExpiration' might imply its purpose, without enabling the Time To Live (TTL) feature on this attribute, DynamoDB will not automatically delete expired items. The key to using TTL is not just having an expiration attribute, but also enabling TTL on that attribute to automate the deletion process.""
}
```"
An application is expected to process many files. Each file takes four minutes to process each AWS Lambda invocation. The Lambda function does not return any important data. What is the fastest way to process all the files?,"[""First split the files to make them smaller, then process with synchronous RequestResponse Lambda invocations."", ""Make synchronous RequestResponse Lambda invocations and process the files one by one."", ""Make asynchronous Event Lambda invocations and process the files in parallel."", ""First join all the files, then process it all at once with an asynchronous Event Lambda invocation.""]","[""Make asynchronous Event Lambda invocations and process the files in parallel.""]","```json
{
    ""First split the files to make them smaller, then process with synchronous RequestResponse Lambda invocations."": ""This choice is incorrect because splitting the files to make them smaller and then processing them synchronously would add unnecessary complexity and delay. Synchronous RequestResponse invocations would require waiting for each invocation to complete before starting the next, which is inefficient when the goal is to process many files quickly. AWS Lambda is designed to handle parallel processing efficiently, and splitting files unnecessarily increases processing overhead."",
    ""Make synchronous RequestResponse Lambda invocations and process the files one by one."": ""This choice is incorrect because processing files one by one using synchronous RequestResponse invocations is not the fastest method. Synchronous invocations require waiting for each Lambda function to complete before starting the next, leading to a bottleneck. This approach does not leverage Lambda's ability to scale and handle multiple invocations in parallel, which is key to processing many files quickly."",
    ""Make asynchronous Event Lambda invocations and process the files in parallel."": ""This choice is correct because asynchronous Event Lambda invocations allow the application to process multiple files in parallel without waiting for each invocation to complete. AWS Lambda can scale to handle many concurrent executions, making this approach the fastest way to process many files. This method leverages Lambda's event-driven architecture and is a best practice for achieving high throughput in serverless applications. AWS documentation recommends using asynchronous invocations for tasks that can be parallelized and do not require immediate results."",
    ""First join all the files, then process it all at once with an asynchronous Event Lambda invocation."": ""This choice is incorrect because joining all files into one and processing them in a single Lambda invocation would exceed Lambda's execution time limit of 15 minutes if the total processing time exceeds that limit. Additionally, this approach does not take advantage of Lambda's ability to process tasks in parallel, which is crucial for optimizing speed when dealing with many files. This method would likely lead to inefficiencies and potential failures if the combined file size is too large.""
}
```"
The upload of a 15 GB object to Amazon S3 fails. The error message reads: `Your proposed upload exceeds the maximum allowed object size.`. What technique will allow the Developer to upload this object?,"[""Upload the object using the multi-part upload API."", ""Upload the object over an AWS Direct Connect connection."", ""Contact AWS Support to increase the object size limit."", ""Upload the object to another AWS region.""]","[""Upload the object using the multi-part upload API.""]","```json
{
    ""Upload the object using the multi-part upload API."": ""This choice is correct because Amazon S3 has a maximum object size limit of 5 GB for a single PUT operation. However, using the multi-part upload API, you can upload objects up to 5 TB. The multi-part upload API allows you to upload a single object as a set of parts, which can be uploaded independently and in parallel, making it suitable for large files. This is a best practice for handling large uploads in S3."",
    ""Upload the object over an AWS Direct Connect connection."": ""This choice is incorrect because AWS Direct Connect is a network service that provides a dedicated network connection from your premises to AWS. While it can improve the speed and reliability of data transfer, it does not change the maximum object size limit imposed by S3. The issue here is not related to network speed or reliability but to the object size limit for a single PUT operation."",
    ""Contact AWS Support to increase the object size limit."": ""This choice is incorrect because the maximum object size limit for a single PUT operation in S3 is a hard limit and cannot be increased by AWS Support. The correct approach to upload objects larger than 5 GB is to use the multi-part upload API, which is designed to handle such scenarios."",
    ""Upload the object to another AWS region."": ""This choice is incorrect because the maximum object size limit for a single PUT operation is consistent across all AWS regions. Changing the region will not affect this limit. The solution to uploading large objects is to use the multi-part upload API, which is applicable in all regions.""
}
```"
"AWS CodeBuild builds code for an application, creates the Docker image, pushes the image to Amazon Elastic Container Registry (Amazon ECR), and tags the image with a unique identifier. If the Developers already have AWS CLI configured on their workstations, how can the Docker images be pulled to the workstations?","[""Run the following: `docker pull REPOSITORY URI : TAG`."", ""Run the output of the following: `aws ecr get-login` and then run: `docker pull REPOSITORY URI : TAG`."", ""Run the following: `aws ecr get-login` and then run: `docker pull REPOSITORY URI : TAG`."", ""Run the output of the following: `aws ecr get-download-url-for-layer` and then run: `docker pull REPOSITORY URI : TAG`.""]","[""Run the output of the following: `aws ecr get-login` and then run: `docker pull REPOSITORY URI : TAG`.""]","```json
{
    ""Run the following: `docker pull REPOSITORY URI : TAG`."": ""This choice is incorrect because it assumes that the Docker client on the workstation is already authenticated with the Amazon ECR repository. Amazon ECR requires authentication using an authorization token, which is not handled by this command alone. Without authentication, the `docker pull` command will fail due to lack of permissions."",
    ""Run the output of the following: `aws ecr get-login` and then run: `docker pull REPOSITORY URI : TAG`."": ""This choice is correct because the `aws ecr get-login` command generates a Docker login command that includes an authentication token required to access the Amazon ECR repository. Running this command authenticates the Docker client with ECR, allowing the subsequent `docker pull` command to succeed in pulling the image. This process is necessary due to the private nature of ECR repositories, which require authentication to access."",
    ""Run the following: `aws ecr get-login` and then run: `docker pull REPOSITORY URI : TAG`."": ""This choice is incorrect because it suggests running `aws ecr get-login` directly, which would only print the login command to the console. Without executing the output of `aws ecr get-login`, the Docker client will not be authenticated, and the `docker pull` command will fail due to lack of permissions."",
    ""Run the output of the following: `aws ecr get-download-url-for-layer` and then run: `docker pull REPOSITORY URI : TAG`."": ""This choice is incorrect because `aws ecr get-download-url-for-layer` is not used for authenticating Docker clients or pulling images. This command is intended for retrieving download URLs for specific image layers, which is not necessary for pulling an entire image using Docker. This reflects a misunderstanding of how Docker images are typically pulled from ECR.""
}
```"
"A web application is designed to allow new users to create accounts using their email addresses. The application will store attributes for each user, and is expecting millions of user to sign up. What should the Developer implement to achieve the design goals?","[""Amazon Cognito user pools."", ""AWS Mobile Hub user data storage."", ""Amazon Cognito Sync."", ""AWS Mobile Hub cloud logic.""]","[""Amazon Cognito user pools.""]","```json
{
    ""Amazon Cognito user pools."": ""Amazon Cognito user pools are designed for user sign-up and sign-in functionality, making them ideal for handling authentication in web and mobile applications. They can scale to handle millions of users, which aligns with the application's requirement to support a large number of sign-ups. Cognito user pools also manage user attributes and provide built-in security features such as multi-factor authentication and password recovery, adhering to AWS best practices for secure user management."",
    ""AWS Mobile Hub user data storage."": ""AWS Mobile Hub was a service that provided a console to help configure and integrate various AWS services for mobile applications. However, it is not specifically designed for user authentication or managing user accounts. Instead, it focuses on backend services like storage and analytics. Furthermore, AWS Mobile Hub has been deprecated, making it an unsuitable choice for new applications."",
    ""Amazon Cognito Sync."": ""Amazon Cognito Sync was a service used to synchronize user data across devices. It is not intended for user authentication or account creation, which are the primary requirements of the scenario. Cognito Sync has been replaced by AWS AppSync and other services, making it an outdated choice for managing user accounts."",
    ""AWS Mobile Hub cloud logic."": ""AWS Mobile Hub cloud logic allowed developers to create and deploy backend logic for mobile applications. However, it is not related to user authentication or account management. Since AWS Mobile Hub has been deprecated, relying on it for new applications is not recommended. The correct service for user authentication and account management is Amazon Cognito user pools.""
}
```"
"A company needs a new REST API that can return information about the contents of an Amazon S3 bucket, such as a count of the objects stored in it. The company has decided that the new API should be written as a microservice using AWS Lambda and Amazon API Gateway. How should the Developer ensure that the microservice has the necessary access to the Amazon S3 bucket, while adhering to security best practices?","[""Create an IAM user that has permissions to access the Amazon S3 bucket, and store the IAM user credentials in the Lambda function source code."", ""Create an IAM role that has permissions to access the Amazon S3 bucket and assign it to the Lambda function as its execution role."", ""Create an Amazon S3 bucket policy that specifies the Lambda service as its principal and assign it to the Amazon S3 bucket."", ""Create an IAM role, attach the AmazonS3FullAccess managed policy to it, and assign the role to the Lambda function as its execution role.""]","[""Create an IAM role that has permissions to access the Amazon S3 bucket and assign it to the Lambda function as its execution role.""]","```json
{
    ""Create an IAM user that has permissions to access the Amazon S3 bucket, and store the IAM user credentials in the Lambda function source code."": ""This choice is incorrect because storing IAM user credentials in the Lambda function source code is not a security best practice. Hardcoding credentials in source code can lead to security vulnerabilities if the code is exposed. AWS best practices recommend using IAM roles for granting permissions to AWS services, as they provide temporary credentials and reduce the risk of credential leakage."",
    ""Create an IAM role that has permissions to access the Amazon S3 bucket and assign it to the Lambda function as its execution role."": ""This choice is correct because assigning an IAM role to a Lambda function as its execution role is a security best practice. IAM roles provide temporary credentials and allow AWS services to securely access other AWS resources. By granting the necessary permissions to the IAM role, the Lambda function can access the S3 bucket without hardcoding credentials. This approach adheres to the principle of least privilege and minimizes security risks."",
    ""Create an Amazon S3 bucket policy that specifies the Lambda service as its principal and assign it to the Amazon S3 bucket."": ""This choice is incorrect because while bucket policies can grant access to S3 buckets, they are not the best way to grant a specific Lambda function access to an S3 bucket. Bucket policies are generally used to grant access to external accounts or services, not for granting permissions to specific AWS services like Lambda. Using an IAM role attached to the Lambda function is a more secure and appropriate method."",
    ""Create an IAM role, attach the AmazonS3FullAccess managed policy to it, and assign the role to the Lambda function as its execution role."": ""This choice is partially correct but not ideal due to the use of the AmazonS3FullAccess managed policy. While attaching an IAM role to the Lambda function is correct, using the AmazonS3FullAccess policy grants more permissions than necessary, violating the principle of least privilege. It's better to create a custom policy that grants only the necessary permissions to access the specific S3 bucket.""
}
```"
"An organization is using Amazon CloudFront to ensure that its users experience low-latency access to its web application. The organization has identified a need to encrypt all traffic between users and CloudFront, and all traffic between CloudFront and the web application. How can these requirements be met? (Choose TWO)","[""Use AWS KMS to encrypt traffic between CloudFront and the web application."", ""Set the Origin Protocol Policy to `HTTPS Only`."", ""Set the Origin's HTTP Port to 443."", ""Set the Viewer Protocol Policy to `HTTPS Only` or `Redirect HTTP to HTTPS`."", ""Enable the CloudFront option Restrict Viewer Access.""]","[""Set the Origin Protocol Policy to `HTTPS Only`."", ""Set the Viewer Protocol Policy to `HTTPS Only` or `Redirect HTTP to HTTPS`.""]","```json
{
    ""Use AWS KMS to encrypt traffic between CloudFront and the web application."": ""This choice is incorrect because AWS KMS is primarily used for managing encryption keys and not for encrypting traffic between services. To encrypt traffic, you should use HTTPS, which is handled by setting the appropriate protocol policies in CloudFront."",
    ""Set the Origin Protocol Policy to `HTTPS Only`."": ""This choice is correct because setting the Origin Protocol Policy to 'HTTPS Only' ensures that CloudFront communicates with the origin server over HTTPS, encrypting the data in transit between CloudFront and the web application. This is a best practice for securing data and is supported by AWS documentation."",
    ""Set the Origin's HTTP Port to 443."": ""This choice is incorrect because setting the HTTP port to 443 is not a valid configuration. Port 443 is used for HTTPS traffic, not HTTP. The correct approach is to set the protocol policy to HTTPS, not to configure ports directly in this context."",
    ""Set the Viewer Protocol Policy to `HTTPS Only` or `Redirect HTTP to HTTPS`."": ""This choice is correct because setting the Viewer Protocol Policy to 'HTTPS Only' or 'Redirect HTTP to HTTPS' ensures that all traffic between the users and CloudFront is encrypted using HTTPS. This aligns with best practices for securing user data in transit and is recommended in AWS documentation."",
    ""Enable the CloudFront option Restrict Viewer Access."": ""This choice is incorrect because 'Restrict Viewer Access' is used to control access to content using signed URLs or signed cookies, not for encrypting traffic. It does not fulfill the requirement of encrypting traffic between users and CloudFront.""
}
```"
"An application is using Amazon DynamoDB as its data store, and should be able to read 100 items per second as strongly consistent reads. Each item is 5 KB in size. To what value should the table's provisioned read throughput be set?","[""50 read capacity units."", ""100 read capacity units."", ""200 read capacity units."", ""500 read capacity units.""]","[""200 read capacity units.""]","```json
{
    ""50 read capacity units."": ""This choice is incorrect because each strongly consistent read of an item up to 4 KB in size requires 1 read capacity unit. Since each item is 5 KB, it requires 2 read capacity units per strongly consistent read. Therefore, to read 100 items per second, you need 200 read capacity units, not 50."",
    ""100 read capacity units."": ""This choice is incorrect because it assumes that each item requires only 1 read capacity unit. However, since each item is 5 KB, it exceeds the 4 KB threshold for a single read capacity unit. Therefore, each item requires 2 read capacity units, resulting in a total of 200 read capacity units needed for 100 items per second."",
    ""200 read capacity units."": ""This choice is correct because each strongly consistent read of a 5 KB item requires 2 read capacity units (1 unit for every 4 KB or part thereof). Therefore, to read 100 items per second, you need 100 x 2 = 200 read capacity units. This aligns with AWS best practices for calculating read capacity for strongly consistent reads."",
    ""500 read capacity units."": ""This choice is incorrect because it overestimates the required read capacity units. Each 5 KB item requires 2 read capacity units for a strongly consistent read, so for 100 items per second, 200 read capacity units are sufficient. Allocating 500 read capacity units would be unnecessary and could lead to increased costs without any performance benefit.""
}
```"
"An application uses Lambda functions to extract metadata from files uploaded to an S3 bucket; the metadata is stored in Amazon DynamoDB. The application starts behaving unexpectedly, and the Developer wants to examine the logs of the Lambda function code for errors. Based on this system configuration, where would the Developer find the logs?","[""Amazon S3."", ""AWS CloudTrail."", ""Amazon CloudWatch."", ""Amazon DynamoDB""]","[""Amazon CloudWatch.""]","```json
{
    ""Amazon S3."": ""Incorrect. Amazon S3 is used for storing objects and files, not for logging. While the files are uploaded to S3, the logs of the Lambda function execution are not stored here. S3 is primarily used for storage, not for logging purposes."",
    ""AWS CloudTrail."": ""Incorrect. AWS CloudTrail is used for auditing and logging API calls made within an AWS account. It provides a history of AWS API calls for account activity, but it does not capture the logs of Lambda function executions. CloudTrail is more about tracking changes and actions in the AWS environment rather than application-specific logs."",
    ""Amazon CloudWatch."": ""Correct. Amazon CloudWatch is the service used to monitor and log AWS resources and applications. Lambda function logs are automatically sent to CloudWatch Logs, where developers can view and search them for troubleshooting and monitoring purposes. This is the best practice for accessing logs related to Lambda function executions."",
    ""Amazon DynamoDB"": ""Incorrect. Amazon DynamoDB is a NoSQL database service used to store and retrieve data, such as the metadata extracted by the Lambda function in this scenario. It does not store logs of Lambda function executions. DynamoDB is used for data storage, not for logging.""
}
```"
A Developer is creating a Lambda function that will generate and export a file. The function requires 100 MB of temporary storage for temporary files while executing. These files will not be needed after the function is complete. How can the Developer MOST efficiently handle the temporary files?,"[""Store the files in EBS and delete the files at the end of the Lambda function."", ""Copy the files to EFS and delete the files at the end of the Lambda function."", ""Store the files in the /tmp directory and delete the files at the end of the Lambda function."", ""Copy the files to an S3 bucket with a lifecycle policy to delete the files.""]","[""Store the files in the /tmp directory and delete the files at the end of the Lambda function.""]","```json
{
    ""Store the files in EBS and delete the files at the end of the Lambda function."": ""This choice is incorrect because Amazon EBS (Elastic Block Store) is not suitable for temporary storage in AWS Lambda. EBS volumes are designed for use with EC2 instances and are not directly accessible from Lambda functions. Using EBS would require additional setup and management, making it inefficient for temporary storage needs in a Lambda function."",
    ""Copy the files to EFS and delete the files at the end of the Lambda function."": ""This choice is incorrect because while Amazon EFS (Elastic File System) can be used with AWS Lambda for persistent storage, it is not the most efficient solution for temporary storage. EFS is more suited for scenarios where data needs to be shared across multiple instances or Lambda functions. For temporary storage that is only needed during the execution of a single function, using EFS would introduce unnecessary complexity and cost."",
    ""Store the files in the /tmp directory and delete the files at the end of the Lambda function."": ""This choice is correct because AWS Lambda provides a /tmp directory with up to 512 MB of storage that is ideal for temporary files. This storage is ephemeral and is automatically cleaned up after the function execution completes. Using the /tmp directory is a best practice for handling temporary files in Lambda functions as it is simple, cost-effective, and requires no additional setup."",
    ""Copy the files to an S3 bucket with a lifecycle policy to delete the files."": ""This choice is incorrect because while Amazon S3 is a durable storage service, using it for temporary files that are only needed during the execution of a Lambda function is inefficient. S3 is better suited for persistent storage needs. Additionally, setting up a lifecycle policy to delete files adds unnecessary complexity for temporary storage that can be handled more efficiently with the /tmp directory.""
}
```"
"A Developer has developed a web application and wants to deploy it quickly on a Tomcat server on AWS. The Developer wants to avoid having to manage the underlying infrastructure. What is the easiest way to deploy the application, based on these requirements?","[""AWS CloudFormation."", ""AWS Elastic Beanstalk."", ""Amazon S3."", ""AWS CodePipeline""]","[""AWS Elastic Beanstalk.""]","```json
{
    ""AWS CloudFormation."": ""AWS CloudFormation is a service used to define and provision infrastructure as code. While it can be used to deploy applications, it requires managing and defining the infrastructure in templates, which does not align with the requirement to avoid managing the underlying infrastructure. Therefore, it is not the easiest way to deploy a web application on a Tomcat server without managing infrastructure."",
    ""AWS Elastic Beanstalk."": ""AWS Elastic Beanstalk is the correct choice because it is a Platform as a Service (PaaS) that allows developers to deploy applications quickly without managing the underlying infrastructure. It supports Java applications on Tomcat servers and automatically handles the deployment, from capacity provisioning, load balancing, and auto-scaling to application health monitoring. This aligns perfectly with the requirement to deploy quickly and avoid infrastructure management."",
    ""Amazon S3."": ""Amazon S3 is a storage service used for storing and retrieving any amount of data at any time. It is not designed for deploying and running web applications, especially those requiring a server environment like Tomcat. Therefore, it does not meet the requirement of deploying a web application on a Tomcat server."",
    ""AWS CodePipeline"": ""AWS CodePipeline is a continuous integration and continuous delivery (CI/CD) service for fast and reliable application and infrastructure updates. While it can be part of a deployment process, it is not a standalone service for deploying applications on a server like Tomcat. It requires integration with other services to deploy applications, which adds complexity and does not align with the requirement to avoid managing infrastructure.""
}
```"
An application runs on multiple EC2 instances behind an ELB. Where is the session data best written so that it can be served reliably across multiple requests?,"[""Write data to Amazon ElastiCache."", ""Write data to Amazon Elastic Block Store."", ""Write data to Amazon EC2 Instance Store."", ""Write data to the root filesystem.""]","[""Write data to Amazon ElastiCache.""]","```json
{
    ""Write data to Amazon ElastiCache."": ""This choice is correct because Amazon ElastiCache is a managed in-memory data store service that supports both Redis and Memcached. It is ideal for storing session data because it provides fast, low-latency access to data, which is crucial for maintaining session state across multiple requests and instances. ElastiCache can be accessed by all instances behind the ELB, ensuring session data is consistent and available across the application. This aligns with AWS best practices for handling session state in a distributed environment."",
    ""Write data to Amazon Elastic Block Store."": ""This choice is incorrect because Amazon EBS is a block storage service designed for use with EC2 instances. While it provides persistent storage, it is not suitable for session data in a distributed application because it is attached to a single EC2 instance at a time. This means that session data would not be accessible across multiple instances, which is necessary for applications behind an ELB."",
    ""Write data to Amazon EC2 Instance Store."": ""This choice is incorrect because the instance store is ephemeral storage that is physically attached to the host computer. Data stored here is lost if the instance is stopped or terminated, making it unreliable for session data that needs to persist across requests and instances. Additionally, instance store data is not shared across multiple instances, which is a requirement for applications behind an ELB."",
    ""Write data to the root filesystem."": ""This choice is incorrect because writing session data to the root filesystem of an EC2 instance would mean the data is only available to that specific instance. In a load-balanced environment, requests can be routed to any instance, so session data needs to be accessible to all instances. Writing to the root filesystem does not provide the necessary persistence or availability across multiple instances.""
}
```"
A company is migrating from a monolithic architecture to a microservices-based architecture. The Developers need to refactor the application so that the many microservices can asynchronously communicate with each other without impacting performance. Use of which managed AWS services will enable asynchronous message passing? (Choose TWO),"[""Amazon SQS."", ""Amazon Cognito."", ""Amazon Kinesis."", ""Amazon SNS."", ""Amazon ElastiCache.""]","[""Amazon SQS."", ""Amazon SNS.""]","```json
{
    ""Amazon SQS."": ""Amazon Simple Queue Service (SQS) is a fully managed message queuing service that enables asynchronous communication between microservices. It decouples the components of a cloud application, allowing each component to operate independently, which is essential for microservices architectures. SQS is designed to handle high throughput and offers features like message retention, dead-letter queues, and message ordering, making it ideal for asynchronous message passing."",
    ""Amazon Cognito."": ""Amazon Cognito is a service that provides user sign-up, sign-in, and access control. It is not related to asynchronous message passing between microservices. Cognito is used for authentication and authorization, not for message queuing or communication between services."",
    ""Amazon Kinesis."": ""Amazon Kinesis is a platform for real-time data streaming and is used for collecting, processing, and analyzing streaming data in real-time. While it can be used for data streaming scenarios, it is not primarily designed for asynchronous message passing between microservices. Kinesis is more suited for real-time data analytics rather than decoupling microservices."",
    ""Amazon SNS."": ""Amazon Simple Notification Service (SNS) is a fully managed pub/sub messaging service that enables asynchronous communication between microservices. SNS can push messages to multiple subscribers, making it suitable for scenarios where multiple microservices need to be notified of an event. It is often used in conjunction with SQS to fan out messages to multiple queues, enhancing the decoupling of microservices."",
    ""Amazon ElastiCache."": ""Amazon ElastiCache is a web service that makes it easy to deploy, operate, and scale an in-memory cache in the cloud. It is used to improve the performance of web applications by retrieving data from in-memory caches, rather than relying entirely on slower disk-based databases. ElastiCache is not designed for asynchronous message passing between microservices.""
}
```"
"According to best practice, how should access keys be managed in AWS? (Choose TWO)","[""Use the same access key in all applications for consistency."", ""Delete all access keys for the account root user."", ""Leave unused access keys in the account for tracking purposes."", ""Embed and encrypt access keys in code for continuous deployment."", ""Use Amazon IAM roles instead of access keys where possible.""]","[""Delete all access keys for the account root user."", ""Use Amazon IAM roles instead of access keys where possible.""]","```json
{
    ""Use the same access key in all applications for consistency."": ""This is incorrect because using the same access key across multiple applications increases the risk of exposure and potential misuse. AWS best practices recommend using IAM roles and policies to manage access, which allows for more granular control and reduces the need for hardcoding access keys."",
    ""Delete all access keys for the account root user."": ""This is correct because AWS best practices strongly advise against using the root account for everyday tasks. The root account should have its access keys deleted to prevent unauthorized access. Instead, use IAM users and roles for accessing AWS services."",
    ""Leave unused access keys in the account for tracking purposes."": ""This is incorrect because unused access keys pose a security risk. AWS best practices recommend regularly rotating and deleting unused access keys to minimize the risk of them being compromised."",
    ""Embed and encrypt access keys in code for continuous deployment."": ""This is incorrect because embedding access keys in code is a security risk, even if they are encrypted. AWS recommends using IAM roles and environment variables to manage credentials, which are more secure and manageable."",
    ""Use Amazon IAM roles instead of access keys where possible."": ""This is correct because IAM roles provide temporary security credentials and are a more secure way to grant permissions to applications and services. They eliminate the need to manage long-term access keys and reduce the risk of credential exposure.""
}
```"
An application running on an Amazon Linux EC2 instance needs to manage the AWS infrastructure. How can the EC2 instance be configured to make AWS API calls securely?,"[""Sign the AWS CLI command using the signature version 4 process."", ""Run the `aws configure` AWS CLI command and specify the access key id and secret access key."", ""Specify a role for the EC2 instance with the necessary privileges."", ""Pass the access key id and secret access key as parameters for each AWS CLI command.""]","[""Specify a role for the EC2 instance with the necessary privileges.""]","```json
{
    ""Sign the AWS CLI command using the signature version 4 process."": ""This choice is incorrect because while Signature Version 4 is a method to sign AWS API requests, it is not a method for securely managing credentials on an EC2 instance. Signature Version 4 is used to authenticate requests to AWS services, but it does not address the secure storage or management of credentials on an EC2 instance."",
    ""Run the `aws configure` AWS CLI command and specify the access key id and secret access key."": ""This choice is incorrect because storing access keys directly on an EC2 instance is not a best practice due to security risks. If the instance is compromised, the credentials can be exposed. AWS recommends using IAM roles for EC2 instances to provide temporary credentials, which are more secure and automatically rotated."",
    ""Specify a role for the EC2 instance with the necessary privileges."": ""This choice is correct because assigning an IAM role to an EC2 instance allows it to securely access AWS services without the need to store long-term credentials on the instance. IAM roles provide temporary security credentials that are automatically rotated, reducing the risk of credential exposure. This is a best practice for managing AWS credentials on EC2 instances."",
    ""Pass the access key id and secret access key as parameters for each AWS CLI command."": ""This choice is incorrect because passing access keys as parameters can expose them in command history or logs, which is a security risk. AWS best practices recommend using IAM roles for EC2 instances to avoid the need to handle credentials directly.""
}
```"
"An application needs to use the IP address of the client in its processing. The application has been moved into AWS and has been placed behind an Application Load Balancer (ALB). However, all the client IP addresses now appear to be the same. The application must maintain the ability to scale horizontally. Based on this scenario, what is the MOST cost-effective solution to this problem?","[""Remove the application from the ALB. Delete the ALB and change Amazon Route 53 to direct traffic to the instance running the application."", ""Remove the application from the ALB. Create a Classic Load Balancer in its place. Direct traffic to the application using the HTTP protocol."", ""Alter the application code to inspect the X-Forwarded-For header. Ensure that the code can work properly if a list of IP addresses is passed in the header."", ""Alter the application code to inspect a custom header. Alter the client code to pass the IP address in the custom header.""]","[""Alter the application code to inspect the X-Forwarded-For header. Ensure that the code can work properly if a list of IP addresses is passed in the header.""]","```json
{
    ""Remove the application from the ALB. Delete the ALB and change Amazon Route 53 to direct traffic to the instance running the application."": ""This choice is incorrect because removing the ALB and directing traffic directly to the instance would eliminate the benefits of load balancing, such as distributing traffic across multiple instances for high availability and fault tolerance. Additionally, it would not allow the application to scale horizontally, which is a requirement in the scenario."",
    ""Remove the application from the ALB. Create a Classic Load Balancer in its place. Direct traffic to the application using the HTTP protocol."": ""This choice is incorrect because a Classic Load Balancer does not inherently solve the problem of capturing the client's IP address. Both ALB and Classic Load Balancer can pass the client's IP address using the X-Forwarded-For header. Additionally, Classic Load Balancer is considered legacy and does not support many of the advanced features of ALB, such as path-based routing and host-based routing."",
    ""Alter the application code to inspect the X-Forwarded-For header. Ensure that the code can work properly if a list of IP addresses is passed in the header."": ""This choice is correct because the X-Forwarded-For header is a standard way for proxies and load balancers to forward the client's IP address to the application. This solution allows the application to remain behind the ALB, maintaining the ability to scale horizontally and benefit from load balancing features. AWS documentation recommends inspecting the X-Forwarded-For header to obtain the client's IP address when using an ALB."",
    ""Alter the application code to inspect a custom header. Alter the client code to pass the IP address in the custom header."": ""This choice is incorrect because it requires changes to both the application and client code, which is not cost-effective or practical. It also introduces complexity and potential security issues, as clients would need to be trusted to send the correct IP address. Using the X-Forwarded-For header is a standard practice that avoids these issues.""
}
```"
A development team is using AWS Elastic Beanstalk to deploy a two-tier application that consists of a load-balanced web tier and an Amazon RDS database tier in production. The team would like to separate the RDS instance from the Elastic Beanstalk. How can this be accomplished?,"[""Use the Elastic Beanstalk CLI to disassociate the database."", ""Use the AWS CLI to disassociate the database."", ""Change the deployment policy to disassociate the database."", ""Recreate a new Elastic Beanstalk environment without Amazon RDS.""]","[""Recreate a new Elastic Beanstalk environment without Amazon RDS.""]","```json
{
    ""Use the Elastic Beanstalk CLI to disassociate the database."": ""This choice is incorrect because the Elastic Beanstalk CLI does not provide a direct command to disassociate an RDS instance from an Elastic Beanstalk environment. Elastic Beanstalk environments that include an RDS instance are tightly coupled, and the recommended approach is to create the RDS instance separately and manage it independently."",
    ""Use the AWS CLI to disassociate the database."": ""This choice is incorrect because while the AWS CLI can be used to manage AWS resources, there is no specific command to disassociate an RDS instance from an Elastic Beanstalk environment. The RDS instance needs to be created and managed separately from the Elastic Beanstalk environment to achieve this separation."",
    ""Change the deployment policy to disassociate the database."": ""This choice is incorrect because deployment policies in Elastic Beanstalk do not control the association or disassociation of RDS instances. Deployment policies are related to how application versions are deployed, not how resources like RDS are managed."",
    ""Recreate a new Elastic Beanstalk environment without Amazon RDS."": ""This choice is correct because to separate the RDS instance from the Elastic Beanstalk environment, you need to create the RDS instance independently of the Elastic Beanstalk environment. This involves setting up a new Elastic Beanstalk environment without including an RDS instance and then connecting to an independently managed RDS instance. This approach follows AWS best practices for managing database resources separately for better control and flexibility.""
}
```"
"A company is using AWS CodePipeline to deliver one of its applications. The delivery pipeline is triggered by changes to the master branch of an AWS CodeCommit repository and uses AWS CodeBuild to implement the test and build stages of the process and AWS CodeDeploy to deploy the application. The pipeline has been operating successfully for several months and there have been no modifications. Following a recent change to the application's source code, AWS CodeDeploy has not deployed the updates application as expected. What are the possible causes? (Choose TWO)","[""The change was not made in the master branch of the AWS CodeCommit repository."", ""One of the earlier stages in the pipeline failed and the pipeline has terminated."", ""One of the Amazon EC2 instances in the company's AWS CodePipeline cluster is inactive."", ""The AWS CodePipeline is incorrectly configured and is not executing AWS CodeDeploy."", ""AWS CodePipeline does not have permissions to access AWS CodeCommit.""]","[""The change was not made in the master branch of the AWS CodeCommit repository."", ""One of the earlier stages in the pipeline failed and the pipeline has terminated.""]","```json
{
    ""The change was not made in the master branch of the AWS CodeCommit repository."": ""This choice is correct. AWS CodePipeline is configured to trigger on changes to the master branch. If the change was made to a different branch, the pipeline would not be triggered, and thus AWS CodeDeploy would not deploy the updates. This is a common configuration in CI/CD pipelines to ensure that only approved changes are deployed."",
    ""One of the earlier stages in the pipeline failed and the pipeline has terminated."": ""This choice is correct. AWS CodePipeline processes stages sequentially, and if any stage fails, the pipeline stops executing further stages. If the test or build stage failed, the deployment stage would not be reached, explaining why AWS CodeDeploy did not deploy the application."",
    ""One of the Amazon EC2 instances in the company's AWS CodePipeline cluster is inactive."": ""This choice is incorrect. AWS CodePipeline itself does not use EC2 instances directly. It is a managed service that orchestrates other AWS services like CodeBuild and CodeDeploy. The state of EC2 instances would not directly affect the execution of the pipeline unless they are part of the deployment target environment, which is not implied in the question."",
    ""The AWS CodePipeline is incorrectly configured and is not executing AWS CodeDeploy."": ""This choice is incorrect. The pipeline has been operating successfully for several months without modifications, indicating that the configuration is likely correct. An incorrect configuration would have caused issues earlier, not just after a recent code change."",
    ""AWS CodePipeline does not have permissions to access AWS CodeCommit."": ""This choice is incorrect. The pipeline has been functioning for months, which implies that the necessary permissions were already in place. A permissions issue would have prevented the pipeline from working from the start, not just after a recent code change.""
}
```"
"A social media company is using Amazon Cognito in order to synchronize profiles across different mobile devices, to enable end users to have a seamless experience. Which of the following configurations can be used to silently notify users whenever an update is available on all other devices?","[""Modify the user pool to include all the devices which keep them in sync."", ""Use the SyncCallback interface to receive notifications on the application."", ""Use an Amazon Cognito stream to analyze the data and push the notifications."", ""Use the push synchronization feature with the appropriate IAM role.""]","[""Use the push synchronization feature with the appropriate IAM role.""]","```json
{
    ""Modify the user pool to include all the devices which keep them in sync."": ""This choice is incorrect because Amazon Cognito User Pools are primarily used for user authentication and management, not for device synchronization. User Pools do not inherently manage or synchronize data across devices. They are designed to handle user sign-up, sign-in, and access control, not to keep devices in sync."",
    ""Use the SyncCallback interface to receive notifications on the application."": ""This choice is incorrect because the SyncCallback interface is not a feature of Amazon Cognito. While callbacks can be used in programming to handle asynchronous operations, Amazon Cognito does not provide a SyncCallback interface for device synchronization notifications. This option seems to be based on a misunderstanding of available Cognito features."",
    ""Use an Amazon Cognito stream to analyze the data and push the notifications."": ""This choice is incorrect because Amazon Cognito Streams are used to stream user identity data to Amazon Kinesis for analysis, not for sending notifications to devices. While Cognito Streams can be used to analyze data changes, they do not directly provide a mechanism to notify devices of updates."",
    ""Use the push synchronization feature with the appropriate IAM role."": ""This choice is correct because Amazon Cognito provides a push synchronization feature that uses Amazon SNS to send silent push notifications to all devices associated with a user identity whenever data stored in the Cognito Sync store is updated. This feature requires an appropriate IAM role to grant permissions for sending notifications. This is the recommended approach for synchronizing data across devices in real-time.""
}
```"
"An on-premises application is implemented using a Linux, Apache, MySQL and PHP (LAMP) stack. The Developer wants to run this application in AWS. Which of the following sets of AWS services can be used to run this stack?","[""Amazon API Gateway, Amazon S3."", ""AWS Lambda, Amazon DynamoDB."", ""Amazon EC2, Amazon Aurora."", ""Amazon Cognito, Amazon RDS."", ""Amazon ECS, Amazon EBS.""]","[""Amazon EC2, Amazon Aurora.""]","```json
{
    ""Amazon API Gateway, Amazon S3."": ""This choice is incorrect because Amazon API Gateway is used for creating and managing APIs, and Amazon S3 is used for object storage. Neither service is suitable for running a LAMP stack, which requires compute resources and a relational database. API Gateway and S3 do not provide the necessary environment to run PHP applications or host a MySQL database."",
    ""AWS Lambda, Amazon DynamoDB."": ""This choice is incorrect because AWS Lambda is a serverless compute service that runs code in response to events and is not suitable for hosting a traditional LAMP stack application, which typically requires a persistent server environment. Additionally, Amazon DynamoDB is a NoSQL database, whereas the LAMP stack requires a relational database like MySQL. Therefore, this combination does not meet the requirements for running a LAMP stack."",
    ""Amazon EC2, Amazon Aurora."": ""This choice is correct because Amazon EC2 provides the necessary compute resources to host the Linux, Apache, and PHP components of the LAMP stack. Amazon Aurora is a MySQL-compatible relational database service that can replace the MySQL component of the LAMP stack, offering improved performance and scalability. This combination aligns with AWS best practices for running traditional applications in the cloud by using scalable and managed services."",
    ""Amazon Cognito, Amazon RDS."": ""This choice is incorrect because Amazon Cognito is used for user authentication and authorization, not for running applications. While Amazon RDS can host a MySQL database, Cognito does not provide the compute resources needed for the Linux, Apache, and PHP components of the LAMP stack. Therefore, this combination is not suitable for running a complete LAMP stack application."",
    ""Amazon ECS, Amazon EBS."": ""This choice is incorrect because Amazon ECS is a container orchestration service that requires applications to be containerized, which is not a requirement for a traditional LAMP stack. While Amazon EBS provides block storage that could be used with EC2 instances, ECS and EBS alone do not provide a relational database service like MySQL, which is necessary for a LAMP stack. Therefore, this combination does not fully support the requirements of a LAMP stack.""
}
```"
"An application displays a status dashboard. The status is updated by 1 KB messages from an SQS queue. Although the status changes infrequently, the Developer must minimize the time between the message arrival in the queue and the dashboard update. What technique provides the shortest delay in updating the dashboard?","[""Retrieve the messages from the queue using long polling every 20 seconds."", ""Reduce the size of the messages by compressing them before sending."", ""Retrieve the messages from the queue using short polling every 10 seconds."", ""Reduce the size of each message payload by sending it in two parts.""]","[""Retrieve the messages from the queue using long polling every 20 seconds.""]","```json
{
    ""Retrieve the messages from the queue using long polling every 20 seconds."": ""This choice is correct because long polling reduces the number of empty responses by allowing the ReceiveMessage call to wait until a message is available in the queue. This minimizes the delay between message arrival and retrieval, which is ideal for applications that need to update a dashboard promptly. Long polling is a best practice for reducing costs and improving performance when retrieving messages from SQS, as it reduces the number of API requests and the associated costs."",
    ""Reduce the size of the messages by compressing them before sending."": ""This choice is incorrect because the size of the messages is not the primary factor affecting the delay in updating the dashboard. The messages are already small (1 KB), and compressing them would not significantly impact the transmission time or the delay in processing them. The key factor in minimizing delay is how quickly messages are retrieved from the queue, not their size."",
    ""Retrieve the messages from the queue using short polling every 10 seconds."": ""This choice is incorrect because short polling does not wait for messages to arrive in the queue and may return empty responses if no messages are available at the time of polling. This can lead to increased latency and higher costs due to more frequent API calls. Long polling is more efficient and reduces latency by waiting for messages to become available."",
    ""Reduce the size of each message payload by sending it in two parts."": ""This choice is incorrect because splitting the message into two parts would not reduce the delay in updating the dashboard. In fact, it could increase complexity and processing time, as the application would need to handle and reassemble the message parts. The focus should be on optimizing message retrieval from the queue, not altering the message size or structure.""
}
```"
An on-premises legacy application is caching data files locally and writing shared images to local disks. What is necessary to allow for horizontal scaling when migrating the application to AWS?,"[""Modify the application to have both shared images and caching data written to Amazon EBS."", ""Modify the application to read and write cache data on Amazon S3, and also store shared images on S3."", ""Modify the application to use Amazon S3 for serving shared images; cache data can then be written to local disks."", ""Modify the application to read and write cache data on Amazon S3, while continuing to write shared images to local disks.""]","[""Modify the application to read and write cache data on Amazon S3, and also store shared images on S3.""]","```json
{
    ""Modify the application to have both shared images and caching data written to Amazon EBS."": ""This choice is incorrect because Amazon EBS is a block storage service that is tied to a single EC2 instance, which does not inherently support horizontal scaling across multiple instances. For horizontal scaling, a shared storage solution like Amazon S3 is preferred, as it allows multiple instances to access the same data concurrently."",
    ""Modify the application to read and write cache data on Amazon S3, and also store shared images on S3."": ""This choice is correct because Amazon S3 is a highly scalable object storage service that allows multiple instances to read and write data concurrently, making it ideal for both caching data and storing shared images. This supports horizontal scaling as any number of instances can access the data stored in S3. Additionally, S3 provides high durability and availability, which are important for both cache and shared images."",
    ""Modify the application to use Amazon S3 for serving shared images; cache data can then be written to local disks."": ""This choice is incorrect because while using Amazon S3 for shared images is appropriate, writing cache data to local disks does not support horizontal scaling. Local disks are tied to individual instances, which means that the cache data would not be accessible to other instances, limiting the ability to scale horizontally."",
    ""Modify the application to read and write cache data on Amazon S3, while continuing to write shared images to local disks."": ""This choice is incorrect because although using Amazon S3 for cache data supports horizontal scaling, writing shared images to local disks does not. Local disks are not shared across instances, which means that shared images would not be accessible to other instances, thereby hindering horizontal scaling.""
}
```"
A Developer must trigger an AWS Lambda function based on the item lifecycle activity in an Amazon DynamoDB table. How can the Developer create the solution?,"[""A Developer must trigger an AWS Lambda function based on the item lifecycle activity in an Amazon DynamoDB table. How can the Developer create the solution?"", ""Enable a DynamoDB stream that publishes an SNS message. Trigger the Lambda function asynchronously from the SNS message."", ""Enable a DynamoDB stream, and trigger the Lambda function synchronously from the stream."", ""Enable a DynamoDB stream, and trigger the Lambda function asynchronously from the stream.""]","[""Enable a DynamoDB stream, and trigger the Lambda function synchronously from the stream.""]","```json
{
    ""A Developer must trigger an AWS Lambda function based on the item lifecycle activity in an Amazon DynamoDB table. How can the Developer create the solution?"": ""This is not a valid answer choice. It seems to be a repetition of the question rather than an answer option."",
    ""Enable a DynamoDB stream that publishes an SNS message. Trigger the Lambda function asynchronously from the SNS message."": ""This choice is incorrect because it introduces unnecessary complexity. While it is possible to use SNS to trigger a Lambda function, it is not the most direct or efficient method for responding to changes in a DynamoDB table. DynamoDB Streams can directly trigger a Lambda function without the need for SNS, making the process more straightforward and efficient."",
    ""Enable a DynamoDB stream, and trigger the Lambda function synchronously from the stream."": ""This choice is incorrect because it incorrectly describes the nature of the trigger. DynamoDB Streams trigger Lambda functions asynchronously, not synchronously. The Lambda function processes the stream records asynchronously as they become available. The description of synchronous triggering does not align with how DynamoDB Streams and Lambda integration work."",
    ""Enable a DynamoDB stream, and trigger the Lambda function asynchronously from the stream."": ""This choice is correct because it accurately describes the integration between DynamoDB Streams and AWS Lambda. When a DynamoDB table is configured with a stream, changes to the table (such as inserts, updates, and deletes) are captured in the stream. AWS Lambda can be configured to process these stream records asynchronously, allowing developers to respond to changes in the table in real-time. This is a common pattern for event-driven architectures in AWS, and it is supported by AWS best practices and documentation.""
}
```"
"After installing the AWS CLI, a Developer tries to run the command aws configure but receives the following error: `Error: aws: command not found`. What is the most likely cause of this error?","[""The `aws` executable is not in the PATH environment variable."", ""Access to the `aws` executable has been denied to the installer."", ""Incorrect AWS credentials were provided."", ""The `aws` script does not have an executable file mode.""]","[""The `aws` executable is not in the PATH environment variable.""]","{""The `aws` executable is not in the PATH environment variable."": ""This is the correct answer. The error 'aws: command not found' typically indicates that the system cannot locate the 'aws' command because it is not in the PATH environment variable. The PATH variable tells the operating system where to look for executable files. If the AWS CLI was installed but not added to the PATH, the system won't be able to find and execute it. This is a common issue when installing command-line tools and is not specific to AWS CLI."", ""Access to the `aws` executable has been denied to the installer."": ""This choice is incorrect. If access to the 'aws' executable was denied during installation, the installation process would likely fail with a different error message related to permissions, rather than 'command not found'. The error message in the question specifically indicates that the system cannot locate the command, not that it lacks permission to execute it."", ""Incorrect AWS credentials were provided."": ""This choice is incorrect. Providing incorrect AWS credentials would result in authentication errors when trying to execute AWS CLI commands that require AWS account access, not a 'command not found' error. The error in the question occurs before any AWS credentials are used, as it relates to the system's inability to find the command."", ""The `aws` script does not have an executable file mode."": ""This choice is incorrect. If the 'aws' script did not have executable permissions, the error would be related to permissions, such as 'Permission denied', rather than 'command not found'. The 'command not found' error indicates that the system cannot locate the file, not that it lacks permission to execute it.""}"
"The Developer for a retail company must integrate a fraud detection solution into the order processing solution. The fraud detection solution takes between ten and thirty minutes to verify an order. At peak, the web site can receive one hundred orders per minute. What is the most scalable method to add the fraud detection solution to the order processing pipeline?","[""Add all new orders to an Amazon SQS queue. Configure a fleet of 10 EC2 instances spanning multiple AZs with the fraud detection solution installed on them to pull orders from this queue. Update the order with a pass or fails status."", ""Add all new orders to an SQS queue. Configure an Auto Scaling group that uses the queue depth metric as its unit of scale to launch a dynamically-sized fleet of EC2 instances spanning multiple AZs with the fraud detection solution installed on them to pull orders from this queue. Update the order with a pass or fails status."", ""Add all new orders to an Amazon Kinesis Stream. Subscribe a Lambda function to automatically read batches of records from the Kinesis Stream. The Lambda function includes the fraud detection software and will update the order with a pass or fail status."", ""Write all new orders to Amazon DynamoDB. Configure DynamoDB Streams to include all new orders. Subscribe a Lambda function to automatically read batches of records from the Kinesis Stream. The Lambda function includes the fraud detection software and will update the order with a pass or fail status.""]","[""Add all new orders to an SQS queue. Configure an Auto Scaling group that uses the queue depth metric as its unit of scale to launch a dynamically-sized fleet of EC2 instances spanning multiple AZs with the fraud detection solution installed on them to pull orders from this queue. Update the order with a pass or fails status.""]","```json
{
    ""Add all new orders to an Amazon SQS queue. Configure a fleet of 10 EC2 instances spanning multiple AZs with the fraud detection solution installed on them to pull orders from this queue. Update the order with a pass or fails status."": ""This choice is incorrect because it uses a fixed number of EC2 instances, which may not be sufficient to handle peak loads of 100 orders per minute, especially when each order takes 10 to 30 minutes to process. This approach lacks scalability as it does not dynamically adjust the number of instances based on the workload, potentially leading to delays in order processing during peak times."",
    ""Add all new orders to an SQS queue. Configure an Auto Scaling group that uses the queue depth metric as its unit of scale to launch a dynamically-sized fleet of EC2 instances spanning multiple AZs with the fraud detection solution installed on them to pull orders from this queue. Update the order with a pass or fails status."": ""This choice is correct because it leverages Amazon SQS for decoupling and buffering the order processing workload and uses an Auto Scaling group to dynamically adjust the number of EC2 instances based on the queue depth. This ensures that the system can handle varying loads efficiently, scaling out during peak times and scaling in when the load decreases, which is a best practice for building scalable and cost-effective applications on AWS."",
    ""Add all new orders to an Amazon Kinesis Stream. Subscribe a Lambda function to automatically read batches of records from the Kinesis Stream. The Lambda function includes the fraud detection software and will update the order with a pass or fail status."": ""This choice is incorrect because AWS Lambda has a maximum execution time of 15 minutes, which is insufficient for processing orders that take between 10 and 30 minutes. Additionally, Kinesis Streams are more suited for real-time data processing rather than handling long-running tasks like fraud detection that require significant processing time."",
    ""Write all new orders to Amazon DynamoDB. Configure DynamoDB Streams to include all new orders. Subscribe a Lambda function to automatically read batches of records from the Kinesis Stream. The Lambda function includes the fraud detection software and will update the order with a pass or fail status."": ""This choice is incorrect because, similar to the previous option, it relies on AWS Lambda, which cannot handle tasks that exceed 15 minutes in execution time. Furthermore, using DynamoDB and DynamoDB Streams introduces unnecessary complexity for a task that is better suited for a message queue like SQS, which is designed for decoupling and handling asynchronous workloads.""
}
```"
"When a Developer tries to run an AWS CodeBuild project, it raises an error because the length of all environment variables exceeds the limit for the combined maximum of characters. What is the recommended solution?","[""Add the export `LC_ALL=\""en_US.utf8\""` command to the pre_build section to ensure POSIX localization."", ""Use Amazon Cognito to store key-value pairs for large numbers of environment variables."", ""Update the settings for the build project to use an Amazon S3 bucket for large numbers of environment variables."", ""Use AWS Systems Manager Parameter Store to store large numbers of environment variables.""]","[""Use AWS Systems Manager Parameter Store to store large numbers of environment variables.""]","```json
{
    ""Add the export `LC_ALL=\""en_US.utf8\""` command to the pre_build section to ensure POSIX localization."": ""This choice is incorrect because setting the locale with LC_ALL is unrelated to managing environment variables or their size. Localization settings are used to define language and regional settings for software environments, not to manage or store environment variables."",
    ""Use Amazon Cognito to store key-value pairs for large numbers of environment variables."": ""This choice is incorrect because Amazon Cognito is primarily used for user authentication and authorization, not for storing environment variables. Cognito is designed to manage user pools and identity pools, which are unrelated to the problem of environment variable size limits in CodeBuild."",
    ""Update the settings for the build project to use an Amazon S3 bucket for large numbers of environment variables."": ""This choice is incorrect because while Amazon S3 can store large amounts of data, it is not designed for securely managing environment variables. Using S3 for this purpose would require additional logic to fetch and parse the data, and it does not provide the same level of security and integration as AWS Systems Manager Parameter Store."",
    ""Use AWS Systems Manager Parameter Store to store large numbers of environment variables."": ""This choice is correct because AWS Systems Manager Parameter Store is specifically designed to store configuration data and secrets, including environment variables, securely and at scale. It allows developers to manage configuration data centrally and retrieve it during build processes, which is a best practice for managing environment variables that exceed size limits. Parameter Store integrates well with AWS services like CodeBuild, providing a secure and scalable solution.""
}
```"
A set of APIs are exposed to customers using the Amazon API Gateway. These APIs have caching enabled on the API Gateway. Customers have asked for an option to invalidate this cache for each of the APIs. What action can be taken to allow API customers to invalidate the API Cache?,"[""Ask customers to use AWS credentials to call the InvalidateCache API."", ""Ask customers to invoke an AWS API endpoint which invalidates the cache."", ""Ask customers to pass an HTTP header called `Cache-Control:max-age=0`."", ""Ask customers to add a query string parameter called `INVALIDATE_CACHE` when making an API call.""]","[""Ask customers to pass an HTTP header called `Cache-Control:max-age=0`.""]","```json
{
    ""Ask customers to use AWS credentials to call the InvalidateCache API."": ""This option is incorrect because there is no specific 'InvalidateCache' API in AWS API Gateway that customers can call directly using AWS credentials. Cache invalidation is typically managed by the API Gateway settings or by passing specific headers or parameters in the request."",
    ""Ask customers to invoke an AWS API endpoint which invalidates the cache."": ""This option is incorrect because AWS API Gateway does not provide a direct API endpoint for customers to invalidate the cache. Cache invalidation is not typically exposed as a direct API call for end-users due to security and management concerns."",
    ""Ask customers to pass an HTTP header called `Cache-Control:max-age=0`."": ""This option is correct because using the 'Cache-Control:max-age=0' HTTP header is a standard way to indicate that the cache should be bypassed or invalidated. When this header is included in a request, it tells the API Gateway to fetch a fresh response from the backend and not to use the cached response. This approach aligns with HTTP caching standards and is a common practice for cache control."",
    ""Ask customers to add a query string parameter called `INVALIDATE_CACHE` when making an API call."": ""This option is incorrect because simply adding a query string parameter like 'INVALIDATE_CACHE' does not automatically invalidate the cache in API Gateway. Cache invalidation through query parameters would require custom logic on the server-side to interpret this parameter and act accordingly, which is not a built-in feature of API Gateway.""
}
```"
A Developer has been asked to build a real-time dashboard web application to visualize the key prefixes and storage size of objects in Amazon S3 buckets. Amazon DynamoDB will be used to store the Amazon S3 metadata. What is the optimal and MOST cost-effective design to ensure that the real-time dashboard is kept up to date with the state of the objects in the Amazon S3 buckets?,"[""Use an Amazon CloudWatch event backed by an AWS Lambda function. Issue an Amazon S3 API call to get a list of all Amazon S3 objects and persist the metadata within DynamoDB. Have the web application poll the DynamoDBtable to reflect this change."", ""Use Amazon S3 Event Notification backed by a Lambda function to persist the metadata into DynamoDB. Have the web application poll the DynamoDB table to reflect this change."", ""Run a cron job within an Amazon EC2 instance to list all objects within Amazon S3 and persist the metadata into DynamoDB. Have the web application poll the DynamoDB table to reflect this change."", ""Create a new Amazon EMR cluster to get all the metadata about Amazon S3 objects; persist the metadata into DynamoDB. Have the web application poll the DynamoDB table to reflect this change.""]","[""Use Amazon S3 Event Notification backed by a Lambda function to persist the metadata into DynamoDB. Have the web application poll the DynamoDB table to reflect this change.""]","```json
{
    ""Use an Amazon CloudWatch event backed by an AWS Lambda function. Issue an Amazon S3 API call to get a list of all Amazon S3 objects and persist the metadata within DynamoDB. Have the web application poll the DynamoDBtable to reflect this change."": ""This option is incorrect because using CloudWatch events to trigger a Lambda function to list all S3 objects is not efficient or cost-effective for real-time updates. Listing all objects in an S3 bucket frequently can be costly and does not provide real-time updates as it relies on periodic polling rather than event-driven architecture."",
    ""Use Amazon S3 Event Notification backed by a Lambda function to persist the metadata into DynamoDB. Have the web application poll the DynamoDB table to reflect this change."": ""This option is correct because S3 Event Notifications can automatically trigger a Lambda function whenever an object is created, deleted, or modified in the S3 bucket. This provides a real-time, event-driven solution that is both efficient and cost-effective. The Lambda function can then update the DynamoDB table with the new metadata, ensuring the dashboard is up to date. This approach leverages AWS's serverless capabilities and is aligned with best practices for real-time data processing."",
    ""Run a cron job within an Amazon EC2 instance to list all objects within Amazon S3 and persist the metadata into DynamoDB. Have the web application poll the DynamoDB table to reflect this change."": ""This option is incorrect because running a cron job on an EC2 instance to list all S3 objects is not cost-effective or efficient for real-time updates. It involves maintaining EC2 instances, which incurs additional costs and operational overhead. Additionally, it does not provide real-time updates as it relies on scheduled polling."",
    ""Create a new Amazon EMR cluster to get all the metadata about Amazon S3 objects; persist the metadata into DynamoDB. Have the web application poll the DynamoDB table to reflect this change."": ""This option is incorrect because using an EMR cluster to process S3 metadata is overkill for this use case. EMR is designed for big data processing and analytics, which is unnecessary for simply updating metadata in real-time. It is not cost-effective and introduces unnecessary complexity for a task that can be handled by a serverless architecture using S3 Event Notifications and Lambda.""
}
```"
A Developer must repeatedly and consistently deploy a serverless RESTful API on AWS. Which techniques will work? (Choose TWO),"[""Define a Swagger file. Use AWS Elastic Beanstalk to deploy the Swagger file."", ""Define a Swagger file. Use AWS CodeDeploy to deploy the Swagger file."", ""Deploy a SAM template with an inline Swagger definition."", ""Define a Swagger file. Deploy a SAM template that references the Swagger file."", ""Define an inline Swagger definition in a Lambda function. Invoke the Lambda function.""]","[""Deploy a SAM template with an inline Swagger definition."", ""Define a Swagger file. Deploy a SAM template that references the Swagger file.""]","```json
{
    ""Define a Swagger file. Use AWS Elastic Beanstalk to deploy the Swagger file."": ""Incorrect. AWS Elastic Beanstalk is primarily used for deploying web applications and services, not specifically for deploying Swagger files or serverless APIs. Elastic Beanstalk does not natively support Swagger file deployment for APIs, which is typically handled by API Gateway or SAM."",
    ""Define a Swagger file. Use AWS CodeDeploy to deploy the Swagger file."": ""Incorrect. AWS CodeDeploy is a service for automating code deployments to EC2 instances, on-premises servers, or Lambda functions. It is not used for deploying Swagger files or serverless APIs directly. Swagger files are typically used with API Gateway or SAM for API deployment."",
    ""Deploy a SAM template with an inline Swagger definition."": ""Correct. AWS SAM (Serverless Application Model) is designed to simplify the deployment of serverless applications, including APIs. By defining an inline Swagger definition within a SAM template, developers can deploy a RESTful API using AWS API Gateway and AWS Lambda. This approach leverages SAM's capabilities to manage serverless resources efficiently."",
    ""Define a Swagger file. Deploy a SAM template that references the Swagger file."": ""Correct. This approach involves creating a SAM template that references an external Swagger file. AWS SAM supports this method, allowing developers to define API Gateway resources using Swagger or OpenAPI definitions. This is a common practice for deploying RESTful APIs in a serverless architecture, providing a clear separation between API definition and deployment logic."",
    ""Define an inline Swagger definition in a Lambda function. Invoke the Lambda function."": ""Incorrect. While Lambda functions can be invoked directly, they are not used to deploy Swagger definitions. Swagger (or OpenAPI) definitions are typically used with API Gateway to define RESTful APIs. Lambda functions are the compute layer, and API Gateway is the service that manages API endpoints and integrates with Swagger definitions.""
}
```"
"An existing serverless application processes uploaded image files. The process currently uses a single Lambda function that takes an image file, performs the processing, and stores the file in Amazon S3. Users of the application now require thumbnail generation of the images. Users want to avoid any impact to the time it takes to perform the image uploads. How can thumbnail generation be added to the application, meeting user requirements while minimizing changes to existing code?","[""Change the existing Lambda function handling the uploads to create thumbnails at the time of upload. Have the function store both the image and thumbnail in Amazon S3."", ""Create a second Lambda function that handles thumbnail generation and storage. Change the existing Lambda function to invoke it asynchronously."", ""Create an S3 event notification with a Lambda function destination. Create a new Lambda function to generate and store thumbnails."", ""Create an S3 event notification to an SQS Queue. Create a scheduled Lambda function that processes the queue, and generates and stores thumbnails.""]","[""Create an S3 event notification with a Lambda function destination. Create a new Lambda function to generate and store thumbnails.""]","```json
{
    ""Change the existing Lambda function handling the uploads to create thumbnails at the time of upload. Have the function store both the image and thumbnail in Amazon S3."": ""This choice is incorrect because it would increase the execution time of the existing Lambda function, potentially impacting the time it takes to perform image uploads. This goes against the user requirement to avoid any impact on upload time. Best practices suggest keeping Lambda functions focused on a single task to maintain efficiency and scalability."",
    ""Create a second Lambda function that handles thumbnail generation and storage. Change the existing Lambda function to invoke it asynchronously."": ""This choice is incorrect because while it separates the concerns of processing and thumbnail generation, it still requires changes to the existing Lambda function to invoke another function asynchronously. This could introduce complexity and potential issues in the existing code, which the requirement aims to minimize."",
    ""Create an S3 event notification with a Lambda function destination. Create a new Lambda function to generate and store thumbnails."": ""This choice is correct because it leverages S3 event notifications to trigger a new Lambda function whenever an image is uploaded. This approach decouples the thumbnail generation process from the initial image upload, ensuring no impact on upload time and minimizing changes to the existing code. It follows AWS best practices for event-driven architectures, where S3 can trigger Lambda functions directly."",
    ""Create an S3 event notification to an SQS Queue. Create a scheduled Lambda function that processes the queue, and generates and stores thumbnails."": ""This choice is incorrect because it introduces unnecessary complexity by adding an SQS queue and a scheduled Lambda function. While it decouples the thumbnail generation process, it does not provide real-time processing like direct S3 event notifications to Lambda. This could delay thumbnail generation and is not as efficient as the direct S3 to Lambda integration.""
}
```"
"A company is using Amazon API Gateway to manage access to a set of microservices implemented as AWS Lambda functions. Following a bug report, the company makes a minor breaking change to one of the APIs. In order to avoid impacting existing clients when the new API is deployed, the company wants to allow clients six months to migrate from v1 to v2. Which approach should the Developer use to handle this change?","[""Update the underlying Lambda function and provide clients with the new Lambda invocation URL."", ""Use API Gateway to automatically propagate the change to clients, specifying 180 days in the phased deployment parameter."", ""Use API Gateway to deploy a new stage named v2 to the API and provide users with its URL."", ""Update the underlying Lambda function, create an Amazon CloudFront distribution with the updated Lambda function as its origin.""]","[""Use API Gateway to deploy a new stage named v2 to the API and provide users with its URL.""]","```json
{
    ""Update the underlying Lambda function and provide clients with the new Lambda invocation URL."": ""This choice is incorrect because directly providing clients with the Lambda invocation URL bypasses API Gateway, which manages access and provides features like throttling, caching, and monitoring. It also does not address versioning or allow for a phased migration from v1 to v2."",
    ""Use API Gateway to automatically propagate the change to clients, specifying 180 days in the phased deployment parameter."": ""This choice is incorrect because API Gateway does not have a feature to automatically propagate changes to clients with a phased deployment parameter. API Gateway stages and versioning need to be managed explicitly by the developer."",
    ""Use API Gateway to deploy a new stage named v2 to the API and provide users with its URL."": ""This choice is correct because using API Gateway stages is a best practice for managing different versions of an API. By deploying a new stage named v2, the company can provide a separate URL for the new version, allowing clients to migrate at their own pace over the six-month period. This approach leverages API Gateway's versioning capabilities."",
    ""Update the underlying Lambda function, create an Amazon CloudFront distribution with the updated Lambda function as its origin."": ""This choice is incorrect because using CloudFront with Lambda as the origin is not a standard practice for API versioning and does not address the need to manage API versions through API Gateway. CloudFront is typically used for content delivery and caching, not for API version management.""
}
```"
A company developed a set of APIs that are being served through the Amazon API Gateway. The API calls need to be authenticated based on OpenID identity providers such as Amazon or Facebook. The APIs should allow access based on a custom authorization model. Which is the simplest and MOST secure design to use to build an authentication and authorization model for the APIs?,"[""Use Amazon Cognito user pools and a custom authorizer to authenticate and authorize users based on JSON Web Tokens."", ""Build a OpenID token broker with Amazon and Facebook. Users will authenticate with these identify providers and pass the JSON Web Token to the API to authenticate each API call."", ""Store user credentials in Amazon DynamoDB and have the application retrieve temporary credentials from AWS STS. Make API calls by passing user credentials to the APIs for authentication and authorization."", ""Use Amazon RDS to store user credentials and pass them to the APIs for authentications and authorization.""]","[""Use Amazon Cognito user pools and a custom authorizer to authenticate and authorize users based on JSON Web Tokens.""]","```json
{
    ""Use Amazon Cognito user pools and a custom authorizer to authenticate and authorize users based on JSON Web Tokens."": ""This choice is correct because Amazon Cognito user pools provide a secure and scalable solution for managing user authentication and authorization. Cognito supports OpenID Connect and can integrate with identity providers like Amazon and Facebook. It issues JSON Web Tokens (JWTs) that can be used to authenticate API requests. A custom authorizer in API Gateway can then be used to validate these JWTs and implement a custom authorization model. This approach leverages managed services, reducing the need for custom code and infrastructure, and follows AWS best practices for security and scalability."",
    ""Build a OpenID token broker with Amazon and Facebook. Users will authenticate with these identify providers and pass the JSON Web Token to the API to authenticate each API call."": ""This choice is incorrect because building a custom OpenID token broker adds unnecessary complexity and potential security risks. Amazon Cognito already provides built-in support for federated identity providers, including Amazon and Facebook, and can handle the issuance and validation of JWTs. Using Cognito simplifies the architecture, reduces the need for custom code, and leverages AWS's managed security features."",
    ""Store user credentials in Amazon DynamoDB and have the application retrieve temporary credentials from AWS STS. Make API calls by passing user credentials to the APIs for authentication and authorization."": ""This choice is incorrect because storing user credentials in DynamoDB and using AWS STS for temporary credentials is not a suitable approach for API authentication and authorization. This method does not leverage OpenID Connect or JWTs, which are key requirements in the question. It also introduces security risks by handling user credentials directly, which is not recommended. Cognito provides a more secure and appropriate solution for managing user authentication and authorization."",
    ""Use Amazon RDS to store user credentials and pass them to the APIs for authentications and authorization."": ""This choice is incorrect because using Amazon RDS to store user credentials is not aligned with best practices for API authentication and authorization. RDS is a relational database service and is not designed for handling authentication flows or managing user credentials securely. This approach would require custom authentication logic and does not leverage OpenID Connect or JWTs. Amazon Cognito is a more appropriate service for managing user identities and integrating with identity providers.""
}
```"
Where should an Elastic Beanstalk configuration file named healthcheckur1.config be placed in the application source bundle?,"[""In the root of the application."", ""In the bin folder."", ""In healthcheckur1.config.ebextension under root."", ""In the .ebextensions folder.""]","[""In the .ebextensions folder.""]","```json
{
    ""In the root of the application."": ""This choice is incorrect. Elastic Beanstalk configuration files must be placed in the .ebextensions folder within the application source bundle. Placing the file in the root of the application will not be recognized by Elastic Beanstalk for configuration purposes. AWS Elastic Beanstalk expects configuration files to be located in the .ebextensions directory to process them correctly during environment creation and updates."",
    ""In the bin folder."": ""This choice is incorrect. The bin folder is typically used for executable files or scripts, not for configuration files related to Elastic Beanstalk. Configuration files for Elastic Beanstalk must be placed in the .ebextensions folder to be recognized and applied. Placing the configuration file in the bin folder will result in it being ignored by Elastic Beanstalk."",
    ""In healthcheckur1.config.ebextension under root."": ""This choice is incorrect. There is no valid directory or naming convention called 'healthcheckur1.config.ebextension' recognized by Elastic Beanstalk. Configuration files must be placed in the .ebextensions folder and should have a .config extension. The naming and placement do not follow the AWS best practices for Elastic Beanstalk configuration files."",
    ""In the .ebextensions folder."": ""This choice is correct. According to AWS best practices, Elastic Beanstalk configuration files should be placed in the .ebextensions folder within the application source bundle. This folder is specifically designated for configuration files that define environment variables, software packages, and other resources needed by the environment. Placing the configuration file here ensures that Elastic Beanstalk processes it correctly during environment creation and updates. This is supported by AWS documentation, which specifies the use of the .ebextensions directory for such purposes.""
}
```"
"A Developer has implemented a Lambda function that needs to add new customers to an RDS database that is expected to run hundreds of times per hour. The Lambda function is configured to use 512MB of RAM and is based on the following pseudo code. After testing the Lambda function, the Developer notices that the Lambda execution time is much longer than expected. What should the Developer do to improve performance?","[""Increase the amount of RAM allocated to the Lambda function, which will increase the number of threads the Lambda can use."", ""Increase the size of the RDS database to allow for an increased number of database connections each hour."", ""Move the database connection and close statement out of the handler. Place the connection in the global space."", ""Replace RDS wit Amazon DynamoDB to implement control over the number of writes per second.""]","[""Move the database connection and close statement out of the handler. Place the connection in the global space.""]","```json
{
    ""Increase the amount of RAM allocated to the Lambda function, which will increase the number of threads the Lambda can use."": ""Incorrect. Increasing the RAM for a Lambda function does increase CPU power, but it does not directly address the issue of database connection latency. The problem is likely due to the overhead of establishing a new database connection on each invocation, which is not solved by simply increasing RAM."",
    ""Increase the size of the RDS database to allow for an increased number of database connections each hour."": ""Incorrect. Increasing the size of the RDS database may improve performance in terms of handling more concurrent connections or queries, but it does not address the specific issue of connection latency caused by opening and closing connections on each Lambda invocation."",
    ""Move the database connection and close statement out of the handler. Place the connection in the global space."": ""Correct. By moving the database connection out of the handler and into the global space, the connection can be reused across multiple invocations of the Lambda function, reducing the overhead of establishing a new connection each time. This is a best practice for improving performance in AWS Lambda functions that interact with databases."",
    ""Replace RDS wit Amazon DynamoDB to implement control over the number of writes per second."": ""Incorrect. While DynamoDB is a scalable NoSQL database that can handle a high number of writes per second, replacing RDS with DynamoDB is a significant architectural change and may not be suitable if the application requires relational database features. Additionally, the issue at hand is related to connection management rather than write throughput.""
}
```"
A static website is hosted in an Amazon S3 bucket. Several HTML pages on the site use JavaScript to download images from another Amazon S3 bucket. These images are not displayed when users browse the site. What is the possible cause for the issue?,"[""The referenced Amazon S3 bucket is in another region."", ""The images must be stored in the same Amazon S3 bucket."", ""Port 80 must be opened on the security group in which the Amazon S3 bucket is located."", ""Cross Origin Resource Sharing must be enabled on the Amazon S3 bucket.""]","[""Cross Origin Resource Sharing must be enabled on the Amazon S3 bucket.""]","```json
{
    ""The referenced Amazon S3 bucket is in another region."": ""This choice is incorrect because the region of the S3 bucket does not affect the ability to access its contents via JavaScript. S3 buckets can be accessed globally as long as the correct permissions are set, regardless of the region they are in. The issue described is related to browser security policies, not the geographical location of the bucket."",
    ""The images must be stored in the same Amazon S3 bucket."": ""This choice is incorrect because there is no requirement for images to be stored in the same S3 bucket as the static website for them to be accessed. Cross-Origin Resource Sharing (CORS) can be configured to allow resources to be shared between different buckets, which is a common practice for separating static assets and media files."",
    ""Port 80 must be opened on the security group in which the Amazon S3 bucket is located."": ""This choice is incorrect because Amazon S3 does not use security groups. Security groups are used with Amazon EC2 instances, not S3 buckets. S3 is a managed service that handles its own network security, and access is controlled via bucket policies and CORS configurations, not by opening ports."",
    ""Cross Origin Resource Sharing must be enabled on the Amazon S3 bucket."": ""This choice is correct because Cross-Origin Resource Sharing (CORS) is a security feature implemented by web browsers to prevent JavaScript from making requests to a different domain than the one that served the web page. To allow the static website to access images from another S3 bucket, CORS must be configured on the bucket containing the images to allow requests from the domain of the static website. This is a common requirement when hosting static websites that need to access resources from different origins.""
}
```"
Amazon S3 has the following structure: `S3://BUCKET/FOLDERNAME/FILENAME.zip`. Which S3 best practice would optimize performance with thousands of PUT request each second to a single bucket?,"[""Prefix folder names with user id; for example, `s3://BUCKET/2013-FOLDERNAME/FILENAME.zip`."", ""Prefix file names with timestamps; for example, `s3://BUCKET/FOLDERNAME/2013-26-05-15-00-00-FILENAME.zip`."", ""Prefix file names with random hex hashes; for example, `s3://BUCKET/FOLDERNAME/23a6-FILENAME.zip`."", ""Prefix folder names with random hex hashes; for example, `s3://BUCKET/23a6-FOLDERNAME/FILENAME.zip`.""]","[""Prefix folder names with random hex hashes; for example, `s3://BUCKET/23a6-FOLDERNAME/FILENAME.zip`.""]","```json
{
    ""Prefix folder names with user id; for example, `s3://BUCKET/2013-FOLDERNAME/FILENAME.zip`."": ""This choice is incorrect because prefixing with user id does not ensure a random distribution of keys across partitions. S3 performance is optimized when keys are distributed evenly across partitions, and user ids may lead to hot partitions if many requests are concentrated on a few user ids."",
    ""Prefix file names with timestamps; for example, `s3://BUCKET/FOLDERNAME/2013-26-05-15-00-00-FILENAME.zip`."": ""This choice is incorrect because timestamps can lead to sequential key names. Sequential keys can result in 'hot spots' where a single partition is overwhelmed with requests, reducing overall performance. S3 performance is improved with non-sequential key names."",
    ""Prefix file names with random hex hashes; for example, `s3://BUCKET/FOLDERNAME/23a6-FILENAME.zip`."": ""This choice is partially correct because using random hex hashes can help distribute keys across partitions. However, prefixing at the file name level may not be as effective as prefixing at the folder level when organizing large datasets."",
    ""Prefix folder names with random hex hashes; for example, `s3://BUCKET/23a6-FOLDERNAME/FILENAME.zip`."": ""This choice is correct because prefixing folder names with random hex hashes helps distribute keys evenly across multiple partitions in S3. This distribution is crucial for optimizing performance with high request rates, as it prevents any single partition from becoming a bottleneck. AWS best practices recommend randomizing key prefixes to improve performance in high-throughput scenarios.""
}
```"
